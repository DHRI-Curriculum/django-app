frontmatter:
  abstract: Digital technologies have made vast amounts of text available to researchers,
    and this same technological moment has provided us with the capacity to analyze
    that text faster than humanly possible. The first step in that analysis is to
    transform texts designed for human consumption into a form a computer can analyze.
    Using Python and the Natural Language ToolKit (commonly called NLTK), this workshop
    introduces strategies to turn qualitative texts into quantitative objects. Through
    that process, we will present a variety of strategies for simple analysis of text-based
    data.
  contributors:
  - collaboration:
      current: true
      role: Au
      workshop: text-analysis
    first_name: Rafael
    last_name: Davis Portela
    url: https://github.com/rafadavis
  - collaboration:
      current: false
      role: Au
      workshop: text-analysis
    first_name: Michelle
    last_name: McSweeney
    url: https://github.com/michellejm
  - collaboration:
      current: false
      role: Au
      workshop: text-analysis
    first_name: Rachel
    last_name: Rakov
    url: https://github.com/rachelrakov
  - collaboration:
      current: false
      role: Au
      workshop: text-analysis
    first_name: Kalle
    last_name: Westerling
    url: https://github.com/kallewesterling
  - collaboration:
      current: false
      role: Au
      workshop: text-analysis
    first_name: Patrick
    last_name: Smyth
    url: https://github.com/smythp
  - collaboration:
      current: false
      role: Au
      workshop: text-analysis
    first_name: Hannah
    last_name: Aizenman
    url: https://github.com/story645
  - collaboration:
      current: false
      role: Au
      workshop: text-analysis
    first_name: Kelsey
    last_name: Chatlosh
    url: https://github.com/kchatlosh
  - collaboration:
      current: false
      role: Re
      workshop: text-analysis
    first_name: Filipa
    last_name: Calado
    url: https://github.com/gofilipa
  - collaboration:
      current: true
      role: Ed
      workshop: text-analysis
    first_name: Lisa
    last_name: Rhody
    url: https://github.com/lmrhody
  - collaboration:
      current: true
      role: Ed
      workshop: text-analysis
    first_name: Kalle
    last_name: Westerling
    url: https://github.com/kallewesterling
  estimated_time: '10'
  ethical_considerations:
  - <p>In working with massive amounts of text, it is natural to lose the original
    context. We must be aware of that and be careful when analizing it.</p>
  - <p>It is important to constantly question our assumptions and the indexes we are
    using. Numbers and graphs do not tell the story, our analysis does. We must be
    careful not to draw hasty and simplistic conclusions for things that are complex.
    Just because we found out that author A uses more unique words than author B,
    does it mean that A is a better writer than B?</p>
  learning_objectives:
  - <p>How to prepare texts for computational analysis, including strategies for transforming
    texts into numbers</p>
  - <p>How to use NLTK methods such as <code>concordance</code> and <code>similar</code></p>
  - <p>How to clean and standardize your data, including powerful tools such as stemmers
    and lemmatizers</p>
  - <p>Compare frequency distribution of words in a text to quantify the narrative
    arc</p>
  - <p>Understand stop words and how to remove them when needed.</p>
  - <p>Utilize Part-of-Speech tagging to gather insights about a text</p>
  - <p>Transform any document that you have (or have access to) in a .txt format into
    a text that can be analyzed computationally</p>
  - <p>How to tokenize your data and put it in nltk compatible format.</p>
  projects:
  - annotation: '[Short list of academic Text & Data mining projects](https://libguides.bc.edu/textdatamining/projects)'
    title: Short list of academic Text & Data mining projects
    url: https://libguides.bc.edu/textdatamining/projects
  - annotation: '[Building a Simple Chatbot from Scratch in Python](https://github.com/parulnith/Building-a-Simple-Chatbot-in-Python-using-NLTK)'
    title: Building a Simple Chatbot from Scratch in Python
    url: https://github.com/parulnith/Building-a-Simple-Chatbot-in-Python-using-NLTK
  - annotation: '[Classifying personality type by social media posts](https://github.com/TGDivy/MBTI-Personality-Classifier)'
    title: Classifying personality type by social media posts
    url: https://github.com/TGDivy/MBTI-Personality-Classifier
  readings:
  - annotation: "[A Beginner\u2019s Tutorial to Jupyter Notebooks](https://towardsdatascience.com/a-beginners-tutorial-to-jupyter-notebooks-1b2f8705888a)"
    title: "A Beginner\u2019s Tutorial to Jupyter Notebooks"
    url: https://towardsdatascience.com/a-beginners-tutorial-to-jupyter-notebooks-1b2f8705888a
  - annotation: '[What is text analysis](https://www.scribbr.com/methodology/textual-analysis/)'
    title: What is text analysis
    url: https://www.scribbr.com/methodology/textual-analysis
  workshop: text-analysis
lessons:
- challenge: ''
  keywords:
  - NLTK
  order: 1
  questions:
  - answers:
      correct:
      - Part-of-Speech (POS) tagging can help identifying verbs, adjectives and nouns
        in a text.
      incorrect:
      - A text is not data in itself, but can produce data if converted into numbers.
      - A corpus is any collection of texts, independently of being related to each
        other or not.
    question: 'Check all sentences below that are correct:'
  solution: ''
  text: "<p>When we think of \"data,\" we often think of numbers, things that can\
    \ be summarized, statisticized, and graphed. Rarely when I ask people \"what is\
    \ data?\" do they respond \"<em>Moby Dick</em>.\" And yet, more and more, text\
    \ is data. Whether it is <em>Moby Dick</em>, or every romance novel written since\
    \ 1750, or today's newspaper or twitter feed, we are able to transform written\
    \ (and spoken) language into data that can be quantified and visualized. That\
    \ has been done for a while, but now we can do it in a much larger scale, in a\
    \ much faster way.</p>\n<h2>\n<a aria-hidden=\"true\" class=\"anchor\" href=\"\
    #corpora\" id=\"user-content-corpora\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Corpora</h2>\n<p>The first step in gathering insights\
    \ from texts is to create a <strong>corpus</strong>. A corpus is a collection\
    \ of texts that are somehow related to each other. For example, the <a href=\"\
    https://corpus.byu.edu/coca/\" rel=\"nofollow\" target=\"_blank\">Corpus of Contemporary\
    \ American English</a>, <a href=\"http://www.trumptwitterarchive.com/\" rel=\"\
    nofollow\" target=\"_blank\">Donald Trump's Tweets</a>, <a href=\"https://byts.commons.gc.cuny.edu/\"\
    \ rel=\"nofollow\" target=\"_blank\">text messages</a> sent by bilingual young\
    \ adults, <a href=\"https://chroniclingamerica.loc.gov/newspapers/\" rel=\"nofollow\"\
    \ target=\"_blank\">digitized newspapers</a>, or <a href=\"https://www.gutenberg.org/\"\
    \ rel=\"nofollow\" target=\"_blank\">books</a> in the public domain are all corpora.\
    \ There are infinitely many corpora, and, sometimes, you will want to make your\
    \ own\u2014that is, one that best fits your research question.</p>\n<p>The route\
    \ you take from here will depend on your research question. Let's say, for example,\
    \ that you want to examine gender differences in writing style. Based on previous\
    \ linguistic research, you hypothesize that male-identified authors use more definitives\
    \ than female-identified. So you collect two corpora\u2014one written by men,\
    \ one written by women\u2014and you count the number of <em>the</em>s, <em>this</em>s,\
    \ and <em>that</em>s compared to the number of <em>a</em>s, <em>an</em>s, and\
    \ <em>one</em>s. Maybe you find a difference, maybe you don't. We can already\
    \ see that this is a relatively crude way of going about answering this question,\
    \ but it is a start.</p>\n<p>Keep in mind that, oftentimes our analysis of gender\
    \ assumes pre-existing gender roles that reproduce gender as a binary system.\
    \ Some digital humanists have pointed out that, if gender is binary, then the\
    \ relation between male and female will likely be one of opposition. As <a href=\"\
    https://dhdebates.gc.cuny.edu/read/untitled-f2acf72c-a469-49d8-be35-67f9ac1e3a60/section/5d9c1b63-7b60-42dd-8cda-bde837f638f4#ch01\"\
    \ rel=\"nofollow\" target=\"_blank\">Laura Mandell</a> says, the categories of\
    \ \"male\" and \"female\" are socially constructed, and quantitative analysis\
    \ practitioners should avoid jumping to conclusions about \"male\" and \"female\"\
    \ styles of thinking and writing \"as if the M/F terms were simple pointers to\
    \ an unproblematic reality, transparently referential and not discursively constituted.\"\
    </p>\n<p>There has been some research about how the <a href=\"http://science.sciencemag.org/content/sci/331/6014/176.full.pdf\"\
    \ rel=\"nofollow\" target=\"_blank\">linguistic complexity of written language</a>\
    \ in long-form pieces (i.e., books, articles, letters, etc.) has decreased over\
    \ time. Simply put, people today use shorter sentences with fewer embedded clauses\
    \ and complex tense constructions than people did in the past. (Note that this\
    \ is not necessarily a bad or good thing.) Based on this research, we want to\
    \ know if short-form platforms are emblematic of the change (we predict that they\
    \ are based on our own experience with short-form platforms like email and Twitter).\
    \ One way to do this would be to use Part-of-Speech tagging. Part-of-Speech (POS)\
    \ tagging is a way to identify the category of words in a given text.</p>\n<p>For\
    \ example, the sentence:</p>\n<blockquote>\n<p>I like the red bicycle.</p>\n</blockquote>\n\
    <p>has one pronoun, one verb, one determiner, one adjective, and one noun.</p>\n\
    <blockquote>\n<p>(<strong>I</strong> : Pronoun), (<strong>like</strong> : Verb),\
    \ (<strong>the</strong> : Determiner), (<strong>red</strong> : Adjective), (<strong>bicycle</strong>\
    \ : Noun)</p>\n</blockquote>\n<p>NLTK uses the <a href=\"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\"\
    \ rel=\"nofollow\" target=\"_blank\">Penn Tree Bank Tag Set</a>. This is a very\
    \ detailed tag list that goes far beyond just nouns, verbs, and adjectives, but\
    \ gives insight into different types of nouns, prepositions, and verbs as well.\
    \ Virtually all POS taggers will create a list of (word, POS) pairs. If newspaper\
    \ articles have a higher ratio of function words (prepositions, auxiliaries, determiners,\
    \ etc.) to semantic words (nouns, verbs, adjectives), than tweets, then we have\
    \ one piece of evidence supporting our hypothesis. It's important to note here\
    \ that we must use either ratios or otherwise normalized data (in the sense that\
    \ raw numbers will not work). Because of the way that language works (function\
    \ words are often repeated, for example), a sample of 100 words will have more\
    \ unique words than a sample of 1,000. Therefore, to compare different data types\
    \ (articles vs. tweets), this fact should be taken into account.</p>\n<h2>\n<a\
    \ aria-hidden=\"true\" class=\"anchor\" href=\"#a-note-about-languages\" id=\"\
    user-content-a-note-about-languages\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>A Note About Languages</h2>\n<p>Even though in this\
    \ workshop we will use the English language in the examples, NLTK does support\
    \ many other languages, due to amazing contributions from the Python Text Analysis\
    \ community. The support, however, varies according to the desired task. Not all\
    \ functions and tools will be available for all the supported languages. The good\
    \ news is that the available tools keep growing in quantity and quality.</p>\n\
    <p>If you are planning to work with other languages than English, you will have\
    \ to figure out what tools are available and how to use them. Unfortunately, it\
    \ is not something that can be fully explained in a general workshop like this.\
    \ Some times it is as easy as changing <code>stopwords.words(\"English\")</code>\
    \ (a command we will teach you later) to <code>stopwords.words(\"Spanish\")</code>.\
    \ Ocasionally, it will be harder than that. A search engine (Google, DuckDuckGo...)\
    \ will be your best friend here.</p>"
  title: Text as Data
  workshop: text-analysis
- challenge: ''
  keywords:
  - Machine Learning
  - Text Normalization
  order: 2
  questions:
  - answers:
      correct:
      - In any type of data analysis, we usually want to cleanse the data in order
        to prepare it for the analysis. In text analysis, this process is called "text
        normalization" and can involve tasks such as removing undesired words and
        punctuation.
      incorrect:
      - Stop words are useless for text analysis, therefore the first step in any
        project is to remove them from the text.
      - Textual alterations can potentially change the original intended meaning.
        Therefore, we must always strive to work with the data exactly as it is in
        the source.
    question: 'Check all sentences below that are correct:'
  solution: ''
  text: "<p>Generally, however, our questions are more about topics rather than writing\
    \ style. So, once we have a corpus\u2014whether that is one text or millions\u2014\
    we usually want to clean and normalize it. There are three terms we are going\
    \ to need:</p>\n<ul>\n<li>\n<strong>Text normalization</strong> is the process\
    \ of taking a list of words and transforming it into a more uniform sequence.\
    \ Usually, this involves removing punctuation, making the words all the same case,\
    \ removing <em>stop words</em>, and either <em>stemming</em> or <em>lemmatizing</em>\
    \ the words. It can also include expanding abbreviations or matching misspellings\
    \ (but these are advanced practices that we will not cover).</li>\n</ul>\n<p>You\
    \ probably know what removing punctuation and capitalization refer to, but the\
    \ other terms may be new:</p>\n<ul>\n<li>\n<p><strong>Stop words</strong> are\
    \ words that appear frequently in a language, often adding grammatical structure,\
    \ but little semantic content. There is no official list of stop words for any\
    \ language, though there are some common, all-purpose lists built in to NLTK.\
    \ However, different tasks require different lists. The purpose of removing stop\
    \ words is to remove words that are so common that their meaning is diminished\
    \ across a large number of texts.</p>\n</li>\n<li>\n<p><strong>Stemming and lemmatizing</strong>\
    \ both of these processes try to consolidate words like \"laughs\" and \"laughing\"\
    \ to\_\_\"laugh\" since they all mean essentially the same thing, they are just\
    \ inflected differently. So again, in an attempt to reduce the number of words,\
    \ and get a realistic understanding of the meaning of a text, these words are\
    \ collapsed. Stemming does this by cutting off the end (very fast), lemmatizing\
    \ does this by looking up the dictionary form (very slow).</p>\n</li>\n</ul>\n\
    <p>Language is messy, and created for and by people, not computers. There is a\
    \ lot of grammatical information in a sentence that a computer cannot use. For\
    \ example, I could say to you:</p>\n<blockquote>\n<p>The house is burning.</p>\n\
    </blockquote>\n<p>and you would understand me. You would also understand if I\
    \ say</p>\n<blockquote>\n<p>house burn.</p>\n</blockquote>\n<p>The first has more\
    \ information about tense, and which house in particular, but the sentiment is\
    \ the same either way.</p>\n<p>In going from the first sentence to the normalized\
    \ words, we removed the stop words (<em>the</em> and <em>is</em>), and removed\
    \ punctuation and case, and lemmatized what was left (<em>burning</em> becomes\
    \ <em>burn</em>\u2014though we might have stemmed this, its impossible to tell\
    \ from the example). This results in what is essentially a \"bag of words,\" or\
    \ a corpus of words without any structure. Because normalizing your text reduces\
    \ the number of words (and therefore the number of dimensions in your data), and\
    \ keeps only the words that contribute meaning to the document, this cleaning\
    \ is usually desirable.</p>\n<p>This is a very important topic in Machine Learning\
    \ tutorials. For the time being, we just need to know that there is \"clean\"\
    \ and \"dirty\" versions of text data. Sometimes our questions are about the clean\
    \ data, but sometimes our questions are in the \"dirt.\"</p>\n<h2>\n<a aria-hidden=\"\
    true\" class=\"anchor\" href=\"#a-note-on-ethics\" id=\"user-content-a-note-on-ethics\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>A Note on\
    \ Ethics</h2>\n<p>The act of cleaning/normalizing subscribes text to predetermined\
    \ categories of meaning, forcing meaning into existing \"boxes,\" so to speak.\
    \ This doesn't mean that we should avoid cleaning or normalizing text, but that\
    \ we should be aware of how some textual reductions have the potential to affect\
    \ meaning. How does quantification reinforce differences or stratifications within\
    \ our data? We have to be careful about the kinds of questions we are asking,\
    \ and how we might be reproducing some of our assumptions in our inquiry.</p>\n\
    <p>To read more about ethics and text analysis, see Lauren Klein's \"<a href=\"\
    https://arcade.stanford.edu/blogs/distant-reading-after-moretti\" rel=\"nofollow\"\
    \ target=\"_blank\">Distant Reading After Moretti</a>,\" where she questions,\
    \ \"Instead of first asking what can be modeled\u2014what phenomena we can track\
    \ at scale\u2014we might instead ask: what might be hidden in this corpus?\u201D\
    </p>"
  title: Cleaning and Normalizing
  workshop: text-analysis
- challenge: ''
  keywords:
  - Corpus
  - Jupyter Notebook
  - Library
  - Matplotlib
  order: 3
  questions: []
  solution: ''
  text: "<p>In the following sections, we are going to learn how to work with the\
    \ NLTK Corpus and go through a series of methods that come built-in to NLTK that\
    \ allow us to turn our words into numbers and visualizations.</p>\n<p>All of the\
    \ code for this section is in a <a href=\"https://raw.githubusercontent.com/DHRI-Curriculum/text-analysis/v2.0/files/TextAnalysisWalkthrough.ipynb\"\
    \ rel=\"nofollow\" target=\"_blank\">Jupyter Notebook file</a>. You should download\
    \ it and save it on your desired folder. Here, we are assuming you are saving\
    \ in in the <code>files</code> folder inside the <code>text-analysis</code> folder.</p>\n\
    <p>In this file you will find all of the workshop commands and the expected outputs.\
    \ If you ever feel stuck or can't seem to be able to advance in the workshop,\
    \ you can open this file and see how we did it. Feel free to open the file right\
    \ now (click on it), take a peek at its contents if you want, and close it.</p>\n\
    <p>To do it, open your Jupyter Notebook, and navigate to the <code>files</code>\
    \ folder inside the <code>text-analysis</code> folder. Click on the <code>TextAnalysisWalkthrough.ipynb</code>\
    \ file. Once you are done, just close the tab.</p>\n<p><a href=\"/static/website/images/lessons/text-analysis/TextAnalysisWalkthrough.png\"\
    \ rel=\"noopener noreferrer\" target=\"_blank\"><img alt=\"image of the Jupyter\
    \ Notebook for this workshop\" class=\"img-fluid d-block my-4\" src=\"/static/website/images/lessons/text-analysis/TextAnalysisWalkthrough.png\"\
    \ style=\"max-width:100%;\"/></a></p>\n<p>For the best possible experience, we\
    \ suggest/encourage you to:</p>\n<ul>\n<li>Create an <code>.ipynb</code> file\
    \ and follow the workshop typing all the code yourself.</li>\n<li>Avoid copying/pasting\
    \ the code. Much of learning has to do with you typing yourself.</li>\n<li>Only\
    \ check the <code>TextAnalysisWalkthrough.ipynb</code> file if you get lost or\
    \ if you are not able to get the right output. Before opening it, put some time\
    \ trying to figure out by yourself why it isn't working. A big part of coding\
    \ is learning to identify what we are doing wrong.</li>\n<li>I would also caution\
    \ you against working with both files open at the same time. It is easy to get\
    \ confused and start modifying the wrong one.</li>\n</ul>\n<p>But those are only\
    \ suggestions. Maybe they will work for you, maybe they won't, so feel free to\
    \ do as it best suit you. You are in charge here!</p>\n<h2>\n<a aria-hidden=\"\
    true\" class=\"anchor\" href=\"#creating-a-jupyter-notebook-file\" id=\"user-content-creating-a-jupyter-notebook-file\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Creating\
    \ a Jupyter Notebook File</h2>\n<p>Now you will create your Jupyter notebook file,\
    \ in which you will run the workshop. Return to the Jupyter Home Tab in your Browser\
    \ (or, if you closed it completely, launch the Jupyter Notebook again), and start\
    \ a New Python Notebook using the <code>New</code> button in the upper right corner.</p>\n\
    <p>Even though Jupyter Notebook doesn't force you to do so, it is very important\
    \ to name your file, or you will end up later with a bunch of untitled files and\
    \ you will have no idea what they are about. In the top left, click in the word\
    \ <code>Untitled</code> and give your file a name such as \"intro_nltk\".</p>\n\
    <p><a href=\"/static/website/images/lessons/text-analysis/change_title.jpg\" rel=\"\
    noopener noreferrer\" target=\"_blank\"><img alt=\"image of a new jupyter notebook\
    \ file\" class=\"img-fluid d-block my-4\" src=\"/static/website/images/lessons/text-analysis/change_title.jpg\"\
    \ style=\"max-width:100%;\"/></a></p>\n<p>In the first blank cell, type the following\
    \ to import the NLTK library:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">nltk</span></pre></div>\n\
    <p><strong>Libraries</strong> are sets of instructions that Python can use to\
    \ perform specialized functions. The Natural Language ToolKit (<code>nltk</code>)\
    \ is one such library. As the name suggests, its focus is on language processing.</p>\n\
    <p>We will also need the <code>matplotlib</code> library later on, so import it\
    \ now:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"\
    pl-k\">import</span> <span class=\"pl-s1\">matplotlib</span></pre></div>\n<p><code>matplotlib</code>\
    \ is a library for making graphs. In the middle of this tutorial, we are going\
    \ to make a dispersion plot of words in our texts.</p>\n<p>Finally, because of\
    \ a quirk of Jupyter notebooks, we need to specify that <code>matplotlib</code>\
    \ should display its graphs in the notebook (as opposed to in a separate window),\
    \ so we type this command (this is technically a Jupyter command, not Python):</p>\n\
    <div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">%</span><span\
    \ class=\"pl-s1\">matplotlib</span> <span class=\"pl-s1\">inline</span></pre></div>\n\
    <p>All three of these commands can be written in the same cell and run all at\
    \ once (<kbd>shift</kbd> + <kbd>enter</kbd>) or in different cells.</p>\n<p><a\
    \ href=\"/static/website/images/lessons/text-analysis/imports.png\" rel=\"noopener\
    \ noreferrer\" target=\"_blank\"><img alt=\"Image showing that the three lines\
    \ given above can be written in a single cell in the Jupyter notebook, one after\
    \ another\" class=\"img-fluid d-block my-4\" src=\"/static/website/images/lessons/text-analysis/imports.png\"\
    \ style=\"max-width:100%;\"/></a></p>\n<p>If you don't see an error when you run\
    \ the notebook\u2014that is, if there is no output\u2014you can move on to the\
    \ next step. It is not rare in programming that when you do things right, the\
    \ result will be nothing happening. This is what we like to call a <em>silent\
    \ success</em>.</p>\n<p>Next, we need to load all of the NLTK corpora into our\
    \ program. Even though we downloaded them to our computer, we need to tell Python\
    \ we want to use them.</p>\n<div class=\"highlight highlight-source-python\"><pre><span\
    \ class=\"pl-k\">from</span> <span class=\"pl-s1\">nltk</span>.<span class=\"\
    pl-s1\">book</span> <span class=\"pl-k\">import</span> <span class=\"pl-c1\">*</span></pre></div>\n\
    <p>The pre-loaded NLTK texts should appear again. These are preformatted data\
    \ sets. We will still have to do some minor processing, but having the data in\
    \ this format saves us a few steps. At the end of this tutorial, we will make\
    \ our own corpus. This is a special type of python object specific to NLTK (it\
    \ isn't a string, list, or dictionary). Sometimes it will behave like a string,\
    \ and sometimes like a list of words.</p>\n<p><a href=\"/static/website/images/lessons/text-analysis/nltkbook.png\"\
    \ rel=\"noopener noreferrer\" target=\"_blank\"><img alt='Image showing a second\
    \ cell with the output of \"from nltk.book import *\"' class=\"img-fluid d-block\
    \ my-4\" src=\"/static/website/images/lessons/text-analysis/nltkbook.png\" style=\"\
    max-width:100%;\"/></a></p>\n<p>Notice that each of the texts already have a variable\
    \ name. <em>Moby Dick</em> is <code>text1</code>, <em>Sense and Sensibility</em>\
    \ is <code>text2</code>, and so on. When we want to work with those books, we\
    \ will call them by their variable name, as you'll see soon.</p>\n<p>If you got\
    \ any error messages, check the code and make sure you typed everything correctly.\
    \ Even spaces before words matter!</p>\n<p>If you are sure you are running the\
    \ code correctly, you probably have an installing issue, so you might have to\
    \ go back to the instalation instructions to figure it out.</p>"
  title: Using the NLTK Corpus
  workshop: text-analysis
- challenge: ''
  keywords:
  - Concordance
  - Phatic Language
  order: 4
  questions:
  - answers:
      correct:
      - Using the <code>concordance</code> method with a specific word, such as "whale",
        returns the words that surround "whale" in different sentences, helping us
        to get a glimpse of the contexts in which the word "whale" shows up.
      incorrect:
      - The similar method brings a list of words that are similiar in writing, but
        not necessarily in meaning, like "whale" and "while".
    question: 'Check all sentences below that are correct:'
  solution: ''
  text: "<p>Let's start by analyzing <em>Moby Dick</em>, which is <code>text1</code>\
    \ for NLTK.</p>\n<p>The first function we will look at is <code>concordance</code>.\
    \ \"Concordance\" in this context means the characters on either side of the word.\
    \ Our text is behaving like one giant string, so concordance will just count the\
    \ number of characters on either side. By default, this is 25 characters on either\
    \ side of our target word (including spaces), but <a href=\"http://www.nltk.org/_modules/nltk/text.html#Text.concordance\"\
    \ rel=\"nofollow\" target=\"_blank\">you can change that if you want</a>.</p>\n\
    <p>In the Jupyter Notebook, type:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-s1\">text1</span>.<span class=\"pl-en\">concordance</span>(<span\
    \ class=\"pl-s\">\"whale\"</span>)</pre></div>\n<p>The output shows us the 25\
    \ characters on either side of the word \"whale\" in <em>Moby Dick</em>. Let's\
    \ try this with another word, \"love.\" Just replace the word \"whale\" with \"\
    love,\" and we get the contexts in which Melville uses \"love\" in <em>Moby Dick</em>.\
    \ <code>concordance</code> is used (behind the scenes) for several other functions,\
    \ including <code>similar</code> and <code>common_contexts</code>.</p>\n<p>Let's\
    \ now see which words appear in similar contexts as the word \"love.\" NLTK has\
    \ a built-in function for this as well: <code>similar</code>.</p>\n<div class=\"\
    highlight highlight-source-python\"><pre><span class=\"pl-s1\">text1</span>.<span\
    \ class=\"pl-en\">similar</span>(<span class=\"pl-s\">\"love\"</span>)</pre></div>\n\
    <p>Behind the scenes, Python found all the contexts where the word \"love\" appears.\
    \ It also finds similar environments, and then what words were common among the\
    \ similar contexts. This gives a sense of what other words appear in similar contexts.\
    \ This is somewhat interesting in itself, but more interesting if we compare it\
    \ to something else. Let's take a look at another text. What about <em>Sense and\
    \ Sensibility</em> (<code>text2</code>)? Let's see what words are similar to \"\
    love\" in Jane Austen's writing. In the next cell, type:</p>\n<div class=\"highlight\
    \ highlight-source-python\"><pre><span class=\"pl-s1\">text2</span>.<span class=\"\
    pl-en\">similar</span>(<span class=\"pl-s\">\"love\"</span>)</pre></div>\n<p>We\
    \ can compare the two and see immediately that Melville and Austen use the word\
    \ \"love\" differently.</p>\n<h2>\n<a aria-hidden=\"true\" class=\"anchor\" href=\"\
    #investigating-lol\" id=\"user-content-investigating-lol\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Investigating \"lol\"</h2>\n\
    <p>Let's expand from novels for a minute and take a look at the NLTK Chat Corpus.\
    \ In chats, text messages, and other digital communication platforms, \"lol\"\
    \ is exceedingly common. We know it doesn't simply mean \"laughing out loud\"\u2014\
    maybe the <code>similar</code> function can provide some insight into what it\
    \ does mean.</p>\n<div class=\"highlight highlight-source-python\"><pre><span\
    \ class=\"pl-s1\">text5</span>.<span class=\"pl-en\">similar</span>(<span class=\"\
    pl-s\">\"lol\"</span>)</pre></div>\n<p>The resulting list is a lot of greetings,\
    \ indicating that \"lol\" probably has more of a <a href=\"https://www.oxfordreference.com/view/10.1093/oi/authority.20110803100321840\"\
    \ rel=\"nofollow\" target=\"_blank\">phatic function</a>. Phatic language is language\
    \ primarily for communicating social closeness. Phatic words stand in contrast\
    \ to semantic words, which contribute meaning to the utterance.</p>\n<p>If you\
    \ are interested in this type of analysis, take a look at the <code>common_contexts</code>\
    \ function in the <a href=\"https://www.nltk.org/book/\" rel=\"nofollow\" target=\"\
    _blank\">NLTK book</a> or in the <a href=\"https://www.nltk.org/\" rel=\"nofollow\"\
    \ target=\"_blank\">NLTK docs</a>.</p>"
  title: Searching for Words
  workshop: text-analysis
- challenge: '<p>Try this with <code>text2</code>, <em>Sense and Sensibility</em>,
    <a href="#downloading-the-corpus">as we saw here</a>. Some relevant words are
    "marriage," "love," "home," "mother," "husband," "sister," and "wife." Pick a
    few to compare. You can compare an unlimited number, but it''s easier to read
    a few at a time. (Note that the comma in our writing here is <em>inside</em> the
    quotation mark, because that is how proper English grammar works. However, in
    Python, you would have to put commas <em>outside</em> of the quotation marks to
    create a <em>list</em>.)</p>

    <p>NLTK has many more functions built-in, but some of the most powerful functions
    are related to cleaning, part-of-speech tagging, and other stages in the text
    analysis pipeline (where the pipeline refers to the process of loading, cleaning,
    and analyzing text).</p>'
  keywords: []
  order: 5
  questions:
  - answers:
      correct:
      - You can get a visual representation of ocurrences of a word with the <code>dispersion_plot</code>
        method.
      - The <code>dispersion_plot</code> method allows you to input a list of strings,
        as long as you split them with commas.
      - Contrary to grammar rule, in a list of strings, the commas must come outside
        of the quotation marks.
      incorrect: []
    question: 'Check all sentences below that are correct:'
  solution: '<div class="highlight highlight-source-python"><pre><span class="pl-s1">text2</span>.<span
    class="pl-en">dispersion_plot</span>([<span class="pl-s">"love"</span>, <span
    class="pl-s">"marriage"</span>])</pre></div>

    <div class="highlight highlight-source-python"><pre><span class="pl-s1">text2</span>.<span
    class="pl-en">dispersion_plot</span>([<span class="pl-s">"husband"</span>, <span
    class="pl-s">"wife"</span>])</pre></div>'
  text: "<p>In many ways, <code>concordance</code> and <code>similar</code> are heightened\
    \ word searches that tell us something about what is happening near the target\
    \ words. Another metric we can use is to visualize where the words appear in the\
    \ text. In the case of <em>Moby Dick</em>, we want to compare where \"whale\"\
    \ and \"monster\" appear throughout the text. In this case, the text is functioning\
    \ as a list of words, and will make a mark where each word appears, offset from\
    \ the first word. We will <em>pass</em> this <em>function</em> a <em>list</em>\
    \ of <em>strings</em> to plot. In the next cell, type:</p>\n<div class=\"highlight\
    \ highlight-source-python\"><pre><span class=\"pl-s1\">text1</span>.<span class=\"\
    pl-en\">dispersion_plot</span>([<span class=\"pl-s\">\"whale\"</span>, <span class=\"\
    pl-s\">\"monster\"</span>])</pre></div>\n<p>A graph should appear with a tick\
    \ mark everywhere that \"whale\" appears and everywhere that \"monster\" appears.\
    \ Knowing the story, we can interpret this graph and align it to what we know\
    \ of how the narrative progresses, helping us develop a visual of the story \u2014\
    \ where the whale goes from being a whale to being a monster to being a whale\
    \ again. If we did not know the story, this could give us hints of the narrative\
    \ arc.</p>"
  title: Positioning Words
  workshop: text-analysis
- challenge: ''
  keywords: []
  order: 6
  questions:
  - answers:
      correct:
      - The <code>lower()</code> method returns the lowercase form of all of the alphabetical
        characters in a string.
      - The <code>append()</code> method adds an item to the end of the list.
      incorrect:
      - '"Whale", "WHALE", and "whale" are all different tokens of the same type.'
      - The <code>isalpha()</code> method transforms integers in alphabetical strings.
    question: 'Check all sentences below that are correct:'
  solution: ''
  text: "<p>We will now turn our attention away from the NLTK library and work with\
    \ our text using the <em>built-in Python functions</em>, the ones that come included\
    \ with the Python language, rather than the NLTK library. (This difference is\
    \ relevant because built-in python functions will work with any list of strings,\
    \ while some of the functions that are specific to the NLTK library will require\
    \ you to make your text \"nltk ready\". Don't worry about that now, we will show\
    \ you how to do it later in this workshop).</p>\n<p>First, let's find out how\
    \ many times a given word appears in the corpus. In this case (and all cases going\
    \ forward), our text will be treated as a list of words. Therefore, we will use\
    \ the <code>count</code> function. We could just as easily do this with a text\
    \ editor, but performing this in Python allows us to save it to a variable and\
    \ then utilize this statistic in other calculations (for example, if we want to\
    \ know what percentage of words in a corpus are 'lol', we would need a count of\
    \ the 'lol's). In the next cell, type:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-s1\">text1</span>.<span class=\"pl-en\">count</span>(<span\
    \ class=\"pl-s\">\"whale\"</span>)</pre></div>\n<p>We see that \"whale\" occurs\
    \ 906 times, but that seems a little low. Let's check the same thing, but now\
    \ for \"Whale\" and \"WHALE\":</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-s1\">text1</span>.<span class=\"pl-en\">count</span>(<span\
    \ class=\"pl-s\">\"Whale\"</span>)</pre></div>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-s1\">text1</span>.<span class=\"pl-en\">count</span>(<span\
    \ class=\"pl-s\">\"WHALE\"</span>)</pre></div>\n<p>What is clear here is that\
    \ the <code>count</code> method is case-sensitive.\n\"Whale\" with a capital \"\
    W\" appears 282 times, and \"WHALE\" another 38 times. Depending on the type of\
    \ analysis, this distinction can be a problem, and we might want \"whale\", \"\
    Whale\" and \"WHALE\" to be collapsed into one single word. We will deal with\
    \ that in a moment. For the time being, we will accept that we have three different\
    \ entries for \"whale.\"</p>\n<p>This gets at a distinction between <strong>type</strong>\
    \ and <strong>token</strong>. \"Whale\" and \"whale\" are different types (as\
    \ of now) because they do not match identically. Every instance of \"whale\" in\
    \ the corpus is another <strong>token</strong>\u2014it is an instance of the type,\
    \ \"whale.\" Therefore, there are 906 tokens of \"whale\" in our corpus, 282 tokens\
    \ of \"Whale\" and 38 tokens of \"WHALE\".</p>\n<p>But that's not what we want.\
    \ Let's fix this by making all of the words lowercase. We will make a new list\
    \ of words, and call it <code>text1_tokens</code>. We will fill this list with\
    \ all the words in <code>text1</code>, but in their lowercase form. Python has\
    \ a built-in function, <code>lower()</code> that takes all letters and makes them\
    \ lowercase. In this same step, we are going to do a kind of tricky move, and\
    \ only keep the words that are alphabetical and pass over anything that is punctuation\
    \ or numbers. There is a built-in function, <code>isalpha()</code>, that will\
    \ allow us to save only those words that are made of letters. If <code>isalpha()</code>\
    \ is true, we'll make the word lowercase, and keep the word. If not, we'll pass\
    \ over it and move to the next one.</p>\n<p>Type the following code into a new\
    \ cell in your notebook. Pay special attention to the indentation, which must\
    \ appear as below. (Note that in Jupyter Notebook, indentation usually comes automatically.\
    \ If not, make sure to type the <kbd>space</kbd> key 4 times)</p>\n<div class=\"\
    highlight highlight-source-python\"><pre><span class=\"pl-s1\">text1_tokens</span>\
    \ <span class=\"pl-c1\">=</span> []\n<span class=\"pl-k\">for</span> <span class=\"\
    pl-s1\">t</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">text1</span>:\n\
    \_\_\_\_<span class=\"pl-k\">if</span> <span class=\"pl-s1\">t</span>.<span class=\"\
    pl-en\">isalpha</span>():\n\_\_\_\_\_\_\_\_<span class=\"pl-s1\">t</span> <span\
    \ class=\"pl-c1\">=</span> <span class=\"pl-s1\">t</span>.<span class=\"pl-en\"\
    >lower</span>()\n\_\_\_\_\_\_\_\_<span class=\"pl-s1\">text1_tokens</span>.<span\
    \ class=\"pl-en\">append</span>(<span class=\"pl-s1\">t</span>)</pre></div>\n\
    <p>If everything went right, you should get no output. Remember the \"silent success?\"\
    </p>\n<p>Another way to perform the same action more tersely is to use what's\
    \ called a <a href=\"https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions\"\
    \ rel=\"nofollow\" target=\"_blank\">list comprehension</a>. A list comprehension\
    \ is a shorter, faster way to write a for-loop. It is syntactically a little more\
    \ difficult to read (for a human), but, in this case, it's much faster to process.</p>\n\
    <p>Don't worry too much about understanding the syntax of list comprehensions\
    \ right now, just try to recognize on it the elements you've seen in the for loop.\
    \ For every example, we will show both the for loop and list comprehension options\
    \ so you can slowly get used to the latter.</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-s1\">text1_tokens</span> <span class=\"pl-c1\">=</span>\
    \ [<span class=\"pl-s1\">t</span>.<span class=\"pl-en\">lower</span>() <span class=\"\
    pl-k\">for</span> <span class=\"pl-s1\">t</span> <span class=\"pl-c1\">in</span>\
    \ <span class=\"pl-s1\">text1</span> <span class=\"pl-k\">if</span> <span class=\"\
    pl-s1\">t</span>.<span class=\"pl-en\">isalpha</span>()]</pre></div>\n<h2>\n<a\
    \ aria-hidden=\"true\" class=\"anchor\" href=\"#take-a-breath\" id=\"user-content-take-a-breath\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Take a Breath</h2>\n\
    <p>Let's take a breath, because this was a difficulty spike. For loops are weird\
    \ and not super intuitive. It usually takes some time for us to get used to them.</p>\n\
    <p>I suggest going back to the loop above, review it, try to understand why all\
    \ indentations are where they are.</p>\n<p>Feel like you understand it? Try deleting\
    \ it and writing the loop yourself without looking at this guide.</p>\n<p>You\
    \ can also copy the whole loop to a new jupyter notebook cell and play around\
    \ with it. What happens when you change the order of the commands? How about the\
    \ indentation? Don't be afraid to break it.</p>\n<p>If you feel like you are done\
    \ playing with the loop, time to move to the next section to see the results.</p>\n\
    <h2>\n<a aria-hidden=\"true\" class=\"anchor\" href=\"#token\" id=\"user-content-token\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Token</h2>\n\
    <p>Do you remember the glossary terms from this section?</p>\n<ul>\n<li><a href=\"\
    /glossary/term/token/\">Token</a></li>\n<li><a href=\"/glossary/term/tokenizing/\"\
    >Tokenizing</a></li>\n<li><a href=\"/glossary/term/type/\">Type</a></li>\n</ul>"
  title: Types vs. Tokens
  workshop: text-analysis
- challenge: ''
  keywords: []
  order: 7
  questions: []
  solution: ''
  text: "<p>Great! Now <code>text1_tokens</code> is a list of all of the tokens in\
    \ our corpus, with the punctuation removed, and all the words in lowercase. Let's\
    \ check it:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"\
    pl-s1\">text1_tokens</span>.<span class=\"pl-en\">count</span>(<span class=\"\
    pl-s\">\"whale\"</span>)</pre></div>\n<p>And now we have 1226 tokens for \"whale\"\
    , which is the exact some of the counts we did before. To double check, count\
    \ \"Whale\" and \"WHALE\" again and you should see no results for them.</p>\n\
    <p>Now we want to know how many words there are in our corpus\u2014that is, how\
    \ many tokens in total. Therefore, we want to ask, \"What is the length of that\
    \ list of words?\" Python has a built-in <code>len</code> function that allows\
    \ you to find out the length of many types. Pass it a list, and it will tell you\
    \ how many items are in the list. Pass it a string, and it will tell you how many\
    \ characters are in the string. Pass it a dictionary, and it will tell you how\
    \ many items are in the dictionary. In the next cell, type:</p>\n<div class=\"\
    highlight highlight-source-python\"><pre><span class=\"pl-en\">len</span>(<span\
    \ class=\"pl-s1\">text1_tokens</span>)</pre></div>\n<p>Just for comparison, check\
    \ out how many words were in <code>text1</code>\u2014before we removed the punctuation\
    \ and the numbers.</p>\n<div class=\"highlight highlight-source-python\"><pre><span\
    \ class=\"pl-en\">len</span>(<span class=\"pl-s1\">text1</span>)</pre></div>\n\
    <p>We see there are over 218,000 words in <em>Moby Dick</em> (including metadata).\
    \ But this is the number of words total\u2014we want to know the number of unique\
    \ words. That is, we want to know how many <em>types</em>, not just how many tokens.</p>\n\
    <p>In order to get unique words, rather than just all words in general, we will\
    \ make a <strong>set</strong> from the list. A <code>set</code> in Python works\
    \ just like it would <a href=\"https://en.wikipedia.org/wiki/Set_(mathematics)\"\
    \ rel=\"nofollow\" target=\"_blank\">in math</a>, it's all the unique values,\
    \ with any duplicate items removed.</p>\n<p>So let's find out the length of our\
    \ set. just like in math, we can also nest our functions. So, rather than saying\
    \ <code>x = set(text1_tokens)</code> and then finding the length of \"x\", we\
    \ can do it all in one step.</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-en\">len</span>(<span class=\"pl-en\">set</span>(<span\
    \ class=\"pl-s1\">text1_tokens</span>))</pre></div>"
  title: Length and Unique Words
  workshop: text-analysis
- challenge: '<p>Let''s compare the lexical density of <em>Moby Dick</em> with <em>Sense
    and Sensibility</em>. Make sure to:</p>

    <ol>

    <li>Make all the words lowercase and remove punctuation.</li>

    <li>Make a slice of the first 10,000 words.</li>

    <li>Calculate lexical density by dividing the length of the set of the slice by
    the length of the slice.</li>

    </ol>

    <p>Remember to be aware of the ethical implications for the conclusions that we
    might draw about our data. What assumptions might we be reifying about these writers?</p>'
  keywords: []
  order: 8
  questions:
  - answers:
      correct:
      - The <code>len</code> method returns the length of the input, which can mean
        different things depending on its type. If it is a string, it will return
        the number of characters; if it is a list or dictionary, it will return the
        number of items.
      - Comparing the lexical density between texts of different sizes can give a
        problematic result. A possible solution is to use list slice and compare parts
        of both texts of a similar size.
      incorrect:
      - The lexical density measures the number of unique words per total word, and
        it is an objective measure of writing quality.
    question: 'Check all sentences below that are correct:'
  solution: '<div class="highlight highlight-source-python"><pre><span class="pl-s1">text2_tokens</span>
    <span class="pl-c1">=</span> []

    <span class="pl-k">for</span> <span class="pl-s1">t</span> <span class="pl-c1">in</span>
    <span class="pl-s1">text2</span>:

    &nbsp;&nbsp;&nbsp;&nbsp;<span class="pl-k">if</span> <span class="pl-s1">t</span>.<span
    class="pl-en">isalpha</span>():

    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="pl-s1">t</span> <span
    class="pl-c1">=</span> <span class="pl-s1">t</span>.<span class="pl-en">lower</span>()

    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="pl-s1">text2_tokens</span>.<span
    class="pl-en">append</span>(<span class="pl-s1">t</span>)

    <span class="pl-s1">text2_slice</span> <span class="pl-c1">=</span> <span class="pl-s1">text2_tokens</span>[<span
    class="pl-c1">0</span>:<span class="pl-c1">10000</span>]

    <span class="pl-en">len</span>(<span class="pl-en">set</span>(<span class="pl-s1">text2_slice</span>))
    <span class="pl-c1">/</span> <span class="pl-en">len</span>(<span class="pl-s1">text2_slice</span>)</pre></div>'
  text: "<p>Now we can calculate the <strong>lexical density</strong>, the number\
    \ of unique words per total words. <a href=\"https://pdfs.semanticscholar.org/c2a8/56959d7f5880c98ccd4cfeb4b4f5b7133ec7.pdf\"\
    \ rel=\"nofollow\" target=\"_blank\">Statistical studies</a> have shown that lexical\
    \ density is a good metric to approximate lexical diversity\u2014the range of\
    \ vocabulary an author uses. For our first pass at lexical density, we will simply\
    \ divide the number of unique words by the total number of words:</p>\n<div class=\"\
    highlight highlight-source-python\"><pre><span class=\"pl-en\">len</span>(<span\
    \ class=\"pl-en\">set</span>(<span class=\"pl-s1\">text1_tokens</span>)) <span\
    \ class=\"pl-c1\">/</span> <span class=\"pl-en\">len</span>(<span class=\"pl-s1\"\
    >text1_tokens</span>)</pre></div>\n<p>If we want to use this metric to compare\
    \ texts, we immediately notice a problem. Lexical density is dependent upon the\
    \ length of a text and therefore is strictly a comparative measure. It is possible\
    \ to compare 100 words from one text to 100 words from another, but because language\
    \ is finite and repetitive, it is not possible to compare 100 words from one to\
    \ 200 words from another. Even with these restrictions, lexical density is a useful\
    \ metric in grade level estimations, <a href=\"http://www.mdpi.com/2226-471X/2/3/7\"\
    \ rel=\"nofollow\" target=\"_blank\">vocabulary use</a> and genre classification,\
    \ and a reasonable proxy for lexical diversity.</p>\n<p>Let's take this constraint\
    \ into account by working with only the first 10,000 words of our text. First\
    \ we need to slice our list, returning the words in position 0 to position 9,999\
    \ (we'll actually write it as \"up to, but not including\" 10,000).</p>\n<div\
    \ class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">text1_slice</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">text1_tokens</span>[<span\
    \ class=\"pl-c1\">0</span>:<span class=\"pl-c1\">10000</span>]</pre></div>\n<p>Now\
    \ we can do the same calculation we did above:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-en\">len</span>(<span class=\"pl-en\">set</span>(<span\
    \ class=\"pl-s1\">text1_slice</span>)) <span class=\"pl-c1\">/</span> <span class=\"\
    pl-en\">len</span>(<span class=\"pl-s1\">text1_slice</span>)</pre></div>\n<p>This\
    \ is a much higher number, though the number itself is arbitrary. When comparing\
    \ different texts, this step is essential to get an accurate measure.</p>"
  title: Lexical Density
  workshop: text-analysis
- challenge: ''
  keywords:
  - Stop Words
  order: 9
  questions:
  - answers:
      correct:
      - Stop words are words that usually don't contribute with much semantic content,
        like prepositions, determiners, etc.
      - List comprehensions are faster ways of iterating and creating lists when compared
        with for loops.
      incorrect:
      - 'To use stop words we need to import them from the nltk corpus, using the
        following code: <code>import stopwords from nltk.corpus</code>'
    question: 'Check all sentences below that are correct:'
  solution: ''
  text: "<p>Thus far, we have been asking questions that take stop words and grammatical\
    \ features into account. For the most part, we want to exclude these features\
    \ since they don't actually contribute very much semantic content to our models.\
    \ Therefore, we will:</p>\n<ol>\n<li>Remove capitalization and punctuation (we've\
    \ already done this).</li>\n<li>Remove stop words.</li>\n<li>Lemmatize (or stem)\
    \ our words, i.e. \"jumping\" and \"jumps\" become \"jump.\"</li>\n</ol>\n<p>We\
    \ already completed step one, and are now working with our <code>text1_tokens</code>.\
    \ Remember, this variable, <code>text1_tokens</code>, contains a list of strings\
    \ that we will work with. We want to remove the stop words from that list. The\
    \ NLTK library comes with fairly comprehensive lists of stop words for many languages.\
    \ Stop words are function words that contribute very little semantic meaning and\
    \ most often have grammatical functions. Usually, these are function words such\
    \ as determiners, prepositions, auxiliaries, and others.</p>\n<p>To use NLTK's\
    \ stop words, we need to import the list of words from the corpus. (We could have\
    \ done this at the beginning of our program, and in more fully developed code,\
    \ we would put it up there, but this works, too.) In the next cell, type:</p>\n\
    <div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span>\
    \ <span class=\"pl-s1\">nltk</span>.<span class=\"pl-s1\">corpus</span> <span\
    \ class=\"pl-k\">import</span> <span class=\"pl-s1\">stopwords</span></pre></div>\n\
    <p>We need to specify the English list, and save it into its own variable that\
    \ we can use in the next step:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-s1\">stops</span> <span class=\"pl-c1\">=</span> <span\
    \ class=\"pl-s1\">stopwords</span>.<span class=\"pl-en\">words</span>(<span class=\"\
    pl-s\">'english'</span>)</pre></div>\n<p>Now let's take a look at those words:</p>\n\
    <div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">print</span>(<span\
    \ class=\"pl-s1\">stops</span>)</pre></div>\n<p>Now we want to go through all\
    \ of the words in our text, and if that word is in the stop words list, remove\
    \ it from our list. Otherwise, we want it to skip it. (The code below is slow,\
    \ so it may take some time to process). The way we can write this in Python is:</p>\n\
    <div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">text1_stops</span>\
    \ <span class=\"pl-c1\">=</span> []\n<span class=\"pl-k\">for</span> <span class=\"\
    pl-s1\">t</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">text1_tokens</span>:\n\
    \_\_\_\_<span class=\"pl-k\">if</span> <span class=\"pl-s1\">t</span> <span class=\"\
    pl-c1\">not</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">stops</span>:\n\
    \_\_\_\_\_\_\_\_<span class=\"pl-s1\">text1_stops</span>.<span class=\"pl-en\"\
    >append</span>(<span class=\"pl-s1\">t</span>)</pre></div>\n<p>A faster option,\
    \ if you are feeling bold, would be using list comprehension:</p>\n<div class=\"\
    highlight highlight-source-python\"><pre><span class=\"pl-s1\">text1_stops</span>\
    \ <span class=\"pl-c1\">=</span> [<span class=\"pl-s1\">t</span> <span class=\"\
    pl-k\">for</span> <span class=\"pl-s1\">t</span> <span class=\"pl-c1\">in</span>\
    \ <span class=\"pl-s1\">text1_tokens</span> <span class=\"pl-k\">if</span> <span\
    \ class=\"pl-s1\">t</span> <span class=\"pl-c1\">not</span> <span class=\"pl-c1\"\
    >in</span> <span class=\"pl-s1\">stops</span>]</pre></div>\n<p>To check the result:</p>\n\
    <div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">print</span>(<span\
    \ class=\"pl-s1\">text1_stops</span>[:<span class=\"pl-c1\">30</span>])</pre></div>\n\
    <h2>\n<a aria-hidden=\"true\" class=\"anchor\" href=\"#verifying-list-contents\"\
    \ id=\"user-content-verifying-list-contents\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Verifying List Contents</h2>\n<p>Now that we\
    \ removed our stop words, let's see how many words are left in our list:</p>\n\
    <div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">len</span>(<span\
    \ class=\"pl-s1\">text1_stops</span>)</pre></div>\n<p>You should get a much lower\
    \ number.</p>\n<p>For reference, let's also check how many unique words there\
    \ are. We will do this by making a set of words. Sets are the same in Python as\
    \ they are in math, they are all of the unique words rather than all the words.\
    \ So, if \"whale\" appears 200 times in the list of words, it will only appear\
    \ once in the set.</p>\n<div class=\"highlight highlight-source-python\"><pre><span\
    \ class=\"pl-en\">len</span>(<span class=\"pl-en\">set</span>(<span class=\"pl-s1\"\
    >text1_stops</span>))</pre></div>"
  title: 'Data Cleaning: Removing Stop Words'
  workshop: text-analysis
- challenge: ''
  keywords:
  - Lemmatization
  - Lexical Density
  order: 10
  questions:
  - answers:
      correct:
      - Stemming and lemmatizing are different forms of reducing word variations to
        their roots.
      - <code>sorted(set(list_of_strings))</code> returns the unique items of <code>list_of_strings</code>
        in alphabetical order.
      incorrect: []
    question: 'Check all sentences below that are correct:'
  solution: ''
  text: "<p>Now that we've removed the stop words from our corpus, the next step is\
    \ to stem or lemmatize the remaining words. This means that we will strip off\
    \ the grammatical structure from the words. For example, <code>cats \u2B62 cat</code>,\
    \ and <code>walked \u2B62 walk</code>. If that was all we had to do, we could\
    \ stem the corpus and achieve the correct result, because stemming (as the name\
    \ implies) really just means cutting off affixes to find the root (or the stem).\
    \ Very quickly, however, this gets complicated, such as in the case of <code>men\
    \ \u2B62 man</code> and <code>sang \u2B62 sing</code>. Lemmatization deals with\
    \ this by looking up the word in a reference and finding the appropriate root\
    \ (though note that this still is not entirely accurate). Lemmatization, therefore,\
    \ takes a relatively long time, since each word must be looked up in a reference.\
    \ NLTK comes with pre-built stemmers and lemmatizers.</p>\n<p>We will use the\
    \ WordNet Lemmatizer from the NLTK Stem library, so let's import that now:</p>\n\
    <div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span>\
    \ <span class=\"pl-s1\">nltk</span>.<span class=\"pl-s1\">stem</span> <span class=\"\
    pl-k\">import</span> <span class=\"pl-v\">WordNetLemmatizer</span></pre></div>\n\
    <p>Because of the way that it is written \"under the hood,\" an instance of the\
    \ lemmatizer needs to be called. We know this from reading <a href=\"https://www.nltk.org/\"\
    \ rel=\"nofollow\" target=\"_blank\">the docs</a>.</p>\n<div class=\"highlight\
    \ highlight-source-python\"><pre><span class=\"pl-s1\">wordnet_lemmatizer</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-v\">WordNetLemmatizer</span>()</pre></div>\n\
    <p>Let's quickly see what lemmatizing does.</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-s1\">wordnet_lemmatizer</span>.<span class=\"pl-en\">lemmatize</span>(<span\
    \ class=\"pl-s\">\"children\"</span>)</pre></div>\n<p>Now try this one:</p>\n\
    <div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">wordnet_lemmatizer</span>.<span\
    \ class=\"pl-en\">lemmatize</span>(<span class=\"pl-s\">\"better\"</span>)</pre></div>\n\
    <p>It didn't work, but...</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-s1\">wordnet_lemmatizer</span>.<span class=\"pl-en\">lemmatize</span>(<span\
    \ class=\"pl-s\">\"better\"</span>, <span class=\"pl-s1\">pos</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-s\">'a'</span>)</pre></div>\n<p>... sometimes\
    \ we can get better results if we define a specific part of speech(pos). \"a\"\
    \ is for \"adjective\", as we learned <a href=\"http://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html\"\
    \ rel=\"nofollow\" target=\"_blank\">here</a>.</p>\n<p>Now we will lemmatize the\
    \ words in the list.</p>\n<div class=\"highlight highlight-source-python\"><pre><span\
    \ class=\"pl-s1\">text1_clean</span> <span class=\"pl-c1\">=</span> []\n<span\
    \ class=\"pl-k\">for</span> <span class=\"pl-s1\">t</span> <span class=\"pl-c1\"\
    >in</span> <span class=\"pl-s1\">text1_stops</span>:\n\_\_\_\_<span class=\"pl-s1\"\
    >t_lem</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">wordnet_lemmatizer</span>.<span\
    \ class=\"pl-en\">lemmatize</span>(<span class=\"pl-s1\">t</span>)\n\_\_\_\_<span\
    \ class=\"pl-s1\">text1_clean</span>.<span class=\"pl-en\">append</span>(<span\
    \ class=\"pl-s1\">t_lem</span>)</pre></div>\n<p>And again, there is a faster version\
    \ for you to use once you feel comfortable with list comprehensions:</p>\n<div\
    \ class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">text1_clean</span>\
    \ <span class=\"pl-c1\">=</span> [<span class=\"pl-s1\">wordnet_lemmatizer</span>.<span\
    \ class=\"pl-en\">lemmatize</span>(<span class=\"pl-s1\">t</span>) <span class=\"\
    pl-k\">for</span> <span class=\"pl-s1\">t</span> <span class=\"pl-c1\">in</span>\
    \ <span class=\"pl-s1\">text1_stops</span>]</pre></div>\n<h2>\n<a aria-hidden=\"\
    true\" class=\"anchor\" href=\"#verifying-clean-list-contents\" id=\"user-content-verifying-clean-list-contents\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Verifying\
    \ Clean List Contents</h2>\n<p>Let's check now to see the length of our final,\
    \ cleaned version of the data, and then check the unique set of words. Notice\
    \ how we will use the <code>print</code> function this time. Jupyter Notebook\
    \ does print commands without the <code>print</code> function, but it will only\
    \ print one thing per cell (the last command), and we wanted to print two different\
    \ things:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"\
    pl-en\">print</span>(<span class=\"pl-en\">len</span>(<span class=\"pl-s1\">text1_clean</span>))\n\
    <span class=\"pl-en\">print</span>(<span class=\"pl-en\">len</span>(<span class=\"\
    pl-en\">set</span>(<span class=\"pl-s1\">text1_clean</span>)))</pre></div>\n<p>If\
    \ everything went right, you should have the same length as before, but a smaller\
    \ number of unique words. That makes sense since we did not remove any word, we\
    \ only changed some of them.</p>\n<p>Now if we were to calculate lexical density,\
    \ we would be looking at how many word stems with semantic content are represented\
    \ in <em>Moby Dick</em>, which is a different question than the one in our first\
    \ analysis of lexical density.</p>\n<p>Why don't you try that by yourself? Try\
    \ to remember how to calculate lexical density without looking back first. It\
    \ is ok if you have forgotten.</p>\n<p>Now let's have a look at the words Melville\
    \ uses in <em>Moby Dick</em>. We'd like to look at all of the <em>types</em>,\
    \ but not necessarily all of the <em>tokens.</em> We will order this set so that\
    \ it is in an order we can handle. In the next cell, type:</p>\n<div class=\"\
    highlight highlight-source-python\"><pre><span class=\"pl-en\">sorted</span>(<span\
    \ class=\"pl-en\">set</span>(<span class=\"pl-s1\">text1_clean</span>))[:<span\
    \ class=\"pl-c1\">30</span>]</pre></div>\n<p><code>sorted</code> combined with\
    \ <code>set</code> should give us a list of all the unique words in <em>Moby Dick</em>\
    \ in alphabetical order, but we only want to see the first ones. Notice how there\
    \ are some words we wouldn't have expected, such as 'abandon', 'abandoned', 'abandonedly',\
    \ and 'abandonment'. This process is far from perfect, but it is useful. However,\
    \ depending on your goal, a different process, like <em>stemming</em> might be\
    \ better.</p>"
  title: 'Data Cleaning: Lemmatizing Words'
  workshop: text-analysis
- challenge: ''
  keywords:
  - Stemming
  order: 11
  questions:
  - answers:
      correct:
      - Both Stemming and Lemmatizing are far from perfect, so they must be used with
        caution.
      - There is no obvious best choice between Stemmers and Lemmatizers, so the best
        way to go is experimenting and seeing what results better fit your goals.
      incorrect: []
    question: 'Check all sentences below that are correct:'
  solution: ''
  text: "<p>The code to implement this and view the output is below:</p>\n<div class=\"\
    highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span\
    \ class=\"pl-s1\">nltk</span>.<span class=\"pl-s1\">stem</span> <span class=\"\
    pl-k\">import</span> <span class=\"pl-v\">PorterStemmer</span>\n<span class=\"\
    pl-s1\">porter_stemmer</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\"\
    >PorterStemmer</span>()</pre></div>\n<p>The Porter is the most common Stemmer.\
    \ Let's see what stemming does to words and compare it with lemmatizers:</p>\n\
    <div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">print</span>(<span\
    \ class=\"pl-s1\">porter_stemmer</span>.<span class=\"pl-en\">stem</span>(<span\
    \ class=\"pl-s\">'berry'</span>))\n<span class=\"pl-en\">print</span>(<span class=\"\
    pl-s1\">porter_stemmer</span>.<span class=\"pl-en\">stem</span>(<span class=\"\
    pl-s\">'berries'</span>))\n<span class=\"pl-en\">print</span>(<span class=\"pl-s1\"\
    >wordnet_lemmatizer</span>.<span class=\"pl-en\">lemmatize</span>(<span class=\"\
    pl-s\">'berry'</span>))\n<span class=\"pl-en\">print</span>(<span class=\"pl-s1\"\
    >wordnet_lemmatizer</span>.<span class=\"pl-en\">lemmatize</span>(<span class=\"\
    pl-s\">'berries'</span>))</pre></div>\n<p>Stemmer doesn't look so good, right?\
    \ But how about checking how stemmer handles some of the words that our lemmatized\
    \ \"failed\" us?</p>\n<div class=\"highlight highlight-source-python\"><pre><span\
    \ class=\"pl-en\">print</span>(<span class=\"pl-s1\">porter_stemmer</span>.<span\
    \ class=\"pl-en\">stem</span>(<span class=\"pl-s\">'abandon'</span>))\n<span class=\"\
    pl-en\">print</span>(<span class=\"pl-s1\">porter_stemmer</span>.<span class=\"\
    pl-en\">stem</span>(<span class=\"pl-s\">'abandoned'</span>))\n<span class=\"\
    pl-en\">print</span>(<span class=\"pl-s1\">porter_stemmer</span>.<span class=\"\
    pl-en\">stem</span>(<span class=\"pl-s\">'abandonedly'</span>))\n<span class=\"\
    pl-en\">print</span>(<span class=\"pl-s1\">porter_stemmer</span>.<span class=\"\
    pl-en\">stem</span>(<span class=\"pl-s\">'abandonment'</span>))</pre></div>\n\
    <p>Still not perfect, but a bit better. So the question is, how to choose between\
    \ stemming and lemmatizing? As many things in text analysis, that depends. The\
    \ best way to go is experimenting, seeing the results and chosing the one that\
    \ better fits your goals.</p>\n<p>As a general rule, stemming is faster while\
    \ lemmatizing is more accurate (but not always, as we just saw). For academics,\
    \ usually the choice goes for the latter.</p>\n<p>Anyway, let's stem our text\
    \ with the Porter Stemmer:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-s1\">t1_porter</span> <span class=\"pl-c1\">=</span> []\n\
    <span class=\"pl-k\">for</span> <span class=\"pl-s1\">t</span> <span class=\"\
    pl-c1\">in</span> <span class=\"pl-s1\">text1_clean</span>:\n\_\_\_\_<span class=\"\
    pl-s1\">t_stemmed</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\"\
    >porter_stemmer</span>.<span class=\"pl-en\">stem</span>(<span class=\"pl-s1\"\
    >t</span>)\n\_\_\_\_<span class=\"pl-s1\">t1_porter</span>.<span class=\"pl-en\"\
    >append</span>(<span class=\"pl-s1\">t_stemmed</span>)</pre></div>\n<p>Or, if\
    \ we want a faster way:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-s1\">t1_porter</span> <span class=\"pl-c1\">=</span> [<span\
    \ class=\"pl-s1\">porter_stemmer</span>.<span class=\"pl-en\">stem</span>(<span\
    \ class=\"pl-s1\">t</span>) <span class=\"pl-k\">for</span> <span class=\"pl-s1\"\
    >t</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">text1_clean</span>]</pre></div>\n\
    <p>And let's check the results:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-en\">print</span>(<span class=\"pl-en\">len</span>(<span\
    \ class=\"pl-en\">set</span>(<span class=\"pl-s1\">t1_porter</span>)))\n<span\
    \ class=\"pl-en\">print</span>(<span class=\"pl-en\">sorted</span>(<span class=\"\
    pl-en\">set</span>(<span class=\"pl-s1\">t1_porter</span>))[:<span class=\"pl-c1\"\
    >30</span>])</pre></div>\n<p>A very different list of words is produced. This\
    \ list is shorter than the list produced by the lemmatizer, but is also less accurate,\
    \ and some of the words will completely change their meaning (like 'berry' becoming\
    \ 'berri').</p>"
  title: 'Data Cleaning: Stemming Words'
  workshop: text-analysis
- challenge: '<ol>

    <li>

    <p>Try to get the same result of the loop above (the one with "my_list"), but
    this time with a list comprehension. Save this other list as "my_list2".</p>

    </li>

    <li>

    <p>Compare both lists to see if they are identical.</p>

    </li>

    </ol>'
  keywords: []
  order: 12
  questions:
  - answers:
      correct:
      - We can create a frequency distribution of a list of strings with <code>FreqDist</code>
        and plot it with the <code>plot</code> method.
      incorrect:
      - <code>my_dist.most_common(50)</code> will check the first 50 words in the
        distribution and return you the most common one among them.
    question: 'Check all sentences below that are correct:'
  solution: '<ol>

    <li>

    <p>A solution using a list comprehension would look like this:</p>

    <div class="highlight highlight-source-python"><pre><span class="pl-s1">my_list2</span>
    <span class="pl-c1">=</span> [<span class="pl-s1">word</span> <span class="pl-k">for</span>
    <span class="pl-s1">word</span> <span class="pl-c1">in</span> <span class="pl-s1">b_words</span>
    <span class="pl-k">if</span> <span class="pl-s1">word</span> <span class="pl-c1">in</span>
    <span class="pl-s1">text1_clean</span>]</pre></div>

    </li>

    <li>

    <p>To compare the lists, you could run the following command:</p>

    <div class="highlight highlight-source-python"><pre><span class="pl-s1">my_list</span>
    <span class="pl-c1">==</span> <span class="pl-s1">my_list2</span></pre></div>

    </li>

    </ol>'
  text: "<p>Now that we've seen some of the differences between both, we will proceed\
    \ using our lemmatized corpus, which we saved as <code>text1_clean</code>:</p>\n\
    <div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">my_dist</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-v\">FreqDist</span>(<span class=\"\
    pl-s1\">text1_clean</span>)</pre></div>\n<p>If nothing happened, that is normal.\
    \ Check to make sure it is there by calling for the type of the \"my_dist\" object.</p>\n\
    <div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">type</span>(<span\
    \ class=\"pl-s1\">my_dist</span>)</pre></div>\n<p>The result should say it is\
    \ a nltk probability distribution (<code>nltk.probability.FreqDist</code>). It\
    \ doesn't matter too much right now what that is, only that it worked. We can\
    \ now plot this with <code>matplotlib</code>'s function called <code>plot</code>.\
    \ We want to plot the first 20 entries of the <code>my_dist</code> object.</p>\n\
    <div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">my_dist</span>.<span\
    \ class=\"pl-en\">plot</span>(<span class=\"pl-c1\">20</span>)</pre></div>\n<p><a\
    \ href=\"/static/website/images/lessons/text-analysis/nltk_plot.png\" rel=\"noopener\
    \ noreferrer\" target=\"_blank\"><img alt=\"nltk plot distribution\" class=\"\
    img-fluid d-block my-4\" src=\"/static/website/images/lessons/text-analysis/nltk_plot.png\"\
    \ style=\"max-width:100%;\"/></a></p>\n<p>We've made a nice image here, but it\
    \ might be easier to comprehend as a list. Because this is a special probability\
    \ distribution object we can call the <code>most_common</code> on this, too. Let's\
    \ find the twenty most common words:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-s1\">my_dist</span>.<span class=\"pl-en\">most_common</span>(<span\
    \ class=\"pl-c1\">20</span>)</pre></div>\n<p>What about if we are interested in\
    \ a list of specific words\u2014perhaps to identify texts that have biblical references.\
    \ Let's make a (short) list of words that might suggest a biblical reference and\
    \ see if they appear in <em>Moby Dick</em>. Set this list equal to a variable:</p>\n\
    <div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">b_words</span>\
    \ <span class=\"pl-c1\">=</span> [<span class=\"pl-s\">'god'</span>, <span class=\"\
    pl-s\">'apostle'</span>, <span class=\"pl-s\">'angel'</span>]</pre></div>\n<p>Then\
    \ we will loop through the words in our cleaned corpus, and see if any of them\
    \ are in our list of biblical words. We'll then save into another list just those\
    \ words that appear in both.</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-s1\">my_list</span> <span class=\"pl-c1\">=</span> []\n\
    <span class=\"pl-k\">for</span> <span class=\"pl-s1\">word</span> <span class=\"\
    pl-c1\">in</span> <span class=\"pl-s1\">b_words</span>:\n\_\_\_\_<span class=\"\
    pl-k\">if</span> <span class=\"pl-s1\">word</span> <span class=\"pl-c1\">in</span>\
    \ <span class=\"pl-s1\">text1_clean</span>:\n\_\_\_\_\_\_\_\_<span class=\"pl-s1\"\
    >my_list</span>.<span class=\"pl-en\">append</span>(<span class=\"pl-s1\">word</span>)\n\
    \_\_\_\_<span class=\"pl-k\">else</span>:\n\_\_\_\_\_\_\_\_<span class=\"pl-k\"\
    >pass</span></pre></div>\n<p>And then we will print the results.</p>\n<div class=\"\
    highlight highlight-source-python\"><pre><span class=\"pl-en\">print</span>(<span\
    \ class=\"pl-s1\">my_list</span>)</pre></div>\n<p>You can obviously do this with\
    \ much larger lists and even compare entire novels if you wish, though it would\
    \ take a while with this approach. You can use this to get similarity measures\
    \ and answer related questions.</p>"
  title: 'Data Cleaning: Results'
  workshop: text-analysis
- challenge: ''
  keywords: []
  order: 13
  questions: []
  solution: ''
  text: "<p>Now that we have seen and implemented a series of text analysis techniques,\
    \ let's go to the Internet to find a new text. You could use something such as\
    \ historic newspapers, or Supreme Court proceedings, or use any txt file on your\
    \ computer. Here we will use <a href=\"http://www.gutenberg.org\" rel=\"nofollow\"\
    \ target=\"_blank\">Project Gutenberg</a>. Project Gutenberg is an archive of\
    \ public domain written works, available in a wide variety of formats, including\
    \ <code>.txt</code>. You can download these to your computer or access them via\
    \ the url. We'll use the latter. We found <em>Don Quixote</em> in the archive\
    \ (see <a href=\"http://www.gutenberg.org/files/996/996-0.txt\" rel=\"nofollow\"\
    \ target=\"_blank\">here</a>), and will work with that.</p>\n<p>The Python package\
    \ <code>urllib</code> comes installed with Python, but is inactive by default,\
    \ so we still need to import it to utilize the functions. Since we are only going\
    \ to use the urlopen function, we will just import that one.</p>\n<p>In the next\
    \ cell, type:</p>\n<div class=\"highlight highlight-source-python\"><pre><span\
    \ class=\"pl-k\">from</span> <span class=\"pl-s1\">urllib</span>.<span class=\"\
    pl-s1\">request</span> <span class=\"pl-k\">import</span> <span class=\"pl-s1\"\
    >urlopen</span></pre></div>\n<p>The <code>urlopen</code> function allows your\
    \ program to interact with files on the internet by opening them. It does not\
    \ read them, however\u2014they are just available to be read in the next line.\
    \ This is the default behavior any time a file is opened and read by Python. One\
    \ reason is that you might want to read a file in different ways. For example,\
    \ if you have a <em>really</em> big file\u2014think big data\u2014you might want\
    \ to read line-by-line rather than the whole thing at once.</p>\n<p>Now let's\
    \ specify which URL we are going to use. Though you might be able to find <em>Don\
    \ Quixote</em> in the Project Gutenberg files, please type this in so that we\
    \ are all using the same format (there are multiple <code>.txt</code> files on\
    \ the site, one with utf-8 encoding, another with ascii encoding). We want the\
    \ utf-8 encoded one. The difference between these is beyond the scope of this\
    \ tutorial, but you can check out this <a href=\"https://www.w3.org/International/questions/qa-what-is-encoding\"\
    \ rel=\"nofollow\" target=\"_blank\">introduction to character encoding</a> from\
    \ The World Wide Web Consortium (W3C) if you are interested.</p>\n<p>Set the URL\
    \ we want to a variable:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-s1\">my_url</span> <span class=\"pl-c1\">=</span> <span\
    \ class=\"pl-s\">\"http://www.gutenberg.org/files/996/996-0.txt\"</span></pre></div>\n\
    <p>We still need to open the file and read the file. You will have to do this\
    \ with files stored locally as well. (in which case, you would type the path to\
    \ the file (i.e., <code>data/texts/mytext.txt</code>) in place of <code>my_url</code>)</p>\n\
    <div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">file</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-en\">urlopen</span>(<span class=\"\
    pl-s1\">my_url</span>)\n<span class=\"pl-s1\">raw</span> <span class=\"pl-c1\"\
    >=</span> <span class=\"pl-s1\">file</span>.<span class=\"pl-en\">read</span>()</pre></div>\n\
    <p>This file is in bytes, so we need to decode it into a string. In the next cell,\
    \ type:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"\
    pl-s1\">don</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">raw</span>.<span\
    \ class=\"pl-en\">decode</span>()</pre></div>\n<p>Now let's check on what kind\
    \ of object we have in the \"don\" variable. Type:</p>\n<div class=\"highlight\
    \ highlight-source-python\"><pre><span class=\"pl-en\">type</span>(<span class=\"\
    pl-s1\">don</span>)</pre></div>\n<p>This should be a string. Great! We have just\
    \ read in our first file.</p>"
  title: Make Your Own Corpus
  workshop: text-analysis
- challenge: '<p>Using the <code>dq_text</code> variable:</p>

    <ul>

    <li>Remove the stop words</li>

    <li>Remove punctuation</li>

    <li>Remove capitalization</li>

    <li>Lemmatize the words</li>

    </ul>

    <p>If you want to spice your challenge up, do the first three operations <em>in
    a single if statement</em>. Google "python nested if statements" for examples.</p>'
  keywords:
  - Metadata
  - Regular Expressions
  order: 14
  questions:
  - answers:
      correct:
      - <code>urlopen</code> can save the contents of a webpage into a variable.
      - To use NLTK functions on a string, we can transform it into a NLTK Text object.
      - NLTK let's you tokenize (split) a giant string into a list of substrings,
        considering punctuations and edge cases like <code>don't</code>.
      incorrect: []
    question: 'Check all sentences below that are correct:'
  solution: '<ol>

    <li>

    <p>Lowercase, remove punctuation and stop words:</p>

    <div class="highlight highlight-source-python"><pre><span class="pl-s1">dq_clean</span>
    <span class="pl-c1">=</span> []

    <span class="pl-k">for</span> <span class="pl-s1">word</span> <span class="pl-c1">in</span>
    <span class="pl-s1">dq_text</span>:

    &nbsp;&nbsp;&nbsp;&nbsp;<span class="pl-k">if</span> <span class="pl-s1">word</span>.<span
    class="pl-en">isalpha</span>():

    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="pl-k">if</span> <span
    class="pl-s1">word</span>.<span class="pl-en">lower</span>() <span class="pl-c1">not</span>
    <span class="pl-c1">in</span> <span class="pl-s1">stops</span>:

    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span
    class="pl-s1">dq_clean</span>.<span class="pl-en">append</span>(<span class="pl-s1">word</span>.<span
    class="pl-en">lower</span>())

    <span class="pl-en">print</span>(<span class="pl-s1">dq_clean</span>[:<span class="pl-c1">50</span>])</pre></div>

    </li>

    <li>

    <p>Lemmatize:</p>

    <div class="highlight highlight-source-python"><pre><span class="pl-k">from</span>
    <span class="pl-s1">nltk</span>.<span class="pl-s1">stem</span> <span class="pl-k">import</span>
    <span class="pl-v">WordNetLemmatizer</span>

    <span class="pl-s1">wordnet_lemmatizer</span> <span class="pl-c1">=</span> <span
    class="pl-v">WordNetLemmatizer</span>()

    <span class="pl-s1">dq_lemmatized</span> <span class="pl-c1">=</span> []

    <span class="pl-k">for</span> <span class="pl-s1">t</span> <span class="pl-c1">in</span>
    <span class="pl-s1">dq_clean</span>:

    &nbsp;&nbsp;&nbsp;&nbsp;<span class="pl-s1">dq_lemmatized</span>.<span class="pl-en">append</span>(<span
    class="pl-s1">wordnet_lemmatizer</span>.<span class="pl-en">lemmatize</span>(<span
    class="pl-s1">t</span>))</pre></div>

    </li>

    </ol>'
  text: "<p>Now we are going to transform that string into a text that we can perform\
    \ NLTK functions on. Since we already imported nltk at the beginning of our program,\
    \ we don't need to import it again, we can just use its functions by specifying\
    \ <code>nltk</code> before the function. The first step is to tokenize the words,\
    \ transforming the giant string into a list of words. A simple way to do this\
    \ would be to split on spaces, and that would probably be fine, but we are going\
    \ to use the NLTK tokenizer to ensure that edge cases are captured (i.e., \"don't\"\
    \ is made into 2 words: \"do\" and \"n't\"). In the next cell, type:</p>\n<div\
    \ class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">don_tokens</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">nltk</span>.<span class=\"\
    pl-en\">word_tokenize</span>(<span class=\"pl-s1\">don</span>)</pre></div>\n<p>You\
    \ can check out the type of <code>don_tokens</code> using the <code>type()</code>\
    \ function to make sure it worked\u2014it should be a list. Let's see how many\
    \ words there are in our novel:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-en\">len</span>(<span class=\"pl-s1\">don_tokens</span>)</pre></div>\n\
    <p>Since this is a list, we can look at any slice of it that we want. Let's inspect\
    \ the first ten words:</p>\n<div class=\"highlight highlight-source-python\"><pre><span\
    \ class=\"pl-s1\">don_tokens</span>[:<span class=\"pl-c1\">10</span>]</pre></div>\n\
    <p>That looks like metadata\u2014not what we want to analyze. We will strip this\
    \ off before proceeding. If you were doing this to many texts, you would want\
    \ to use <a href=\"https://regexone.com/\" rel=\"nofollow\" target=\"_blank\"\
    >Regular Expressions</a>. Regular Expressions are an extremely powerful way to\
    \ match text in a document. However, we are just using this text, so we could\
    \ either guess, or cut and paste the text into a text reader and identify the\
    \ position of the first content (i.e., how many words in is the first word). That\
    \ is the route we are going to take. We found that the content begins at word\
    \ 320, so let's make a slice of the text from word position 320 to the end.</p>\n\
    <div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">dq_text</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">don_tokens</span>[<span\
    \ class=\"pl-c1\">320</span>:]</pre></div>\n<p>Now print the first 30 words to\
    \ see if it worked:</p>\n<div class=\"highlight highlight-source-python\"><pre><span\
    \ class=\"pl-s1\">print</span>(<span class=\"pl-s1\">dq_text</span>[:<span class=\"\
    pl-c1\">30</span>]</pre></div>\n<p>Finally, if we want to use the NLTK specific\
    \ functions:</p>\n<ul>\n<li><code>concordance</code></li>\n<li><code>similar</code></li>\n\
    <li><code>dispersion_plot</code></li>\n<li>or others from the <a href=\"https://www.nltk.org/book/\"\
    \ rel=\"nofollow\" target=\"_blank\">NLTK book</a>\n</li>\n</ul>\n<p>we would\
    \ have to make a specific NLTK <code>Text</code> object.</p>\n<div class=\"highlight\
    \ highlight-source-python\"><pre><span class=\"pl-s1\">dq_nltk_text</span> <span\
    \ class=\"pl-c1\">=</span> <span class=\"pl-s1\">nltk</span>.<span class=\"pl-v\"\
    >Text</span>(<span class=\"pl-s1\">dq_text</span>)</pre></div>\n<p>And we could\
    \ check that it worked by running:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-en\">type</span>(<span class=\"pl-s1\">dq_nltk_text</span>)</pre></div>\n\
    <p>But if we only need to use the built-in Python functions, we can just stick\
    \ with our list of words in <code>dq_text</code>.</p>"
  title: Make Your Own Corpus (continued)
  workshop: text-analysis
- challenge: ''
  keywords:
  - part-of-speech (POS) tagging
  - Dictionaries
  - Tuples
  order: 15
  questions:
  - answers:
      correct:
      - Tuples are like lists, but you can't change their value once created.
      - <code>nltk.pos_tag</code> returns tuples of two values, the first being the
        word, and the second the tag.
      incorrect:
      - POS tagging does not work well with stop words, therefore you should always
        clean your text from stop words before using it.
    question: Which of the following are correct?
  solution: ''
  text: "<p><em>Note that we are going to use the pre-cleaned, <code>dq_text</code>\
    \ object for this section.</em></p>\n<p>POS (Part-of-Speech) tagging is going\
    \ through a text and identifying which part of speech each word belongs to (i.e.,\
    \ Noun, Verb, or Adjective). Every word belongs to a part of speech, but some\
    \ words can be confusing.</p>\n<ul>\n<li>Floyd is happy.</li>\n<li>Happy is a\
    \ state of being.</li>\n<li>Happy has five letters.</li>\n<li>I'm going to Happy\
    \ Cat tonight.</li>\n</ul>\n<p>Therefore, part of speech is as much related to\
    \ the word itself as its relationship to the words around it. A good part-of-speech\
    \ tagger takes this into account, but there are some impossible cases as well:</p>\n\
    <ul>\n<li>Wanda was entertaining last night.</li>\n</ul>\n<p>Part of Speech tagging\
    \ can be done very simply: with a very small <em>tag set</em>, or in a very complex\
    \ way: with a much more elaborate tag set. We are going to implement a compromise,\
    \ and use a neither small nor large tag set, the <a href=\"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\"\
    \ rel=\"nofollow\" target=\"_blank\">Penn Tree Bank POS Tag Set</a>.</p>\n<p>This\
    \ is the tag set that is pre-loaded into NLTK. When we call the tagger, we expect\
    \ it to return an object with the word and the tag associated. Because POS tagging\
    \ is dependent upon the stop words, we have to use a text that includes the stop\
    \ words. Therefore, we will go back to using the <code>dq_text</code> object for\
    \ this section. Let's try it out. Type:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-s1\">dq_tagged</span> <span class=\"pl-c1\">=</span> <span\
    \ class=\"pl-s1\">nltk</span>.<span class=\"pl-en\">pos_tag</span>(<span class=\"\
    pl-s1\">dq_text</span>)</pre></div>\n<p>Let's inspect what we have:</p>\n<div\
    \ class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">print</span>(<span\
    \ class=\"pl-s1\">dq_tagged</span>[:<span class=\"pl-c1\">10</span>])</pre></div>\n\
    <p>This is a list of ordered <em>tuples</em>. The Python native type <em>tuple</em>\
    \ is similar to a list, but can't be changed once it is created. They are also\
    \ denoted with parentheses, rather than square brackets. Each element in the list\
    \ is a tuple\u2014or a pairing\u2014consisting of <code>(word, POS-tag)</code>.\
    \ This is great, but it is very detailed. I would like to know how many nouns,\
    \ verbs, and adjectives I have.</p>\n<p>First, I'll make an empty dictionary to\
    \ hold my results. (If you don't know what a dictionary is and how they work,\
    \ you can check a quick explanation <a href=\"https://realpython.com/python-dicts/\"\
    \ rel=\"nofollow\" target=\"_blank\">here</a>.) After that, I will go through\
    \ this list of tuples and count the number of times each tag appears. Every time\
    \ I encounter a new tag, I'll add it to a dictionary and then increment by one\
    \ every time I encounter that tag again. Let's see what that looks like in code:</p>\n\
    <div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">tag_dict</span>\
    \ <span class=\"pl-c1\">=</span> {}\n<span class=\"pl-c\"># For every word/tag\
    \ pair in my list,</span>\n<span class=\"pl-k\">for</span> (<span class=\"pl-s1\"\
    >word</span>, <span class=\"pl-s1\">tag</span>) <span class=\"pl-c1\">in</span>\
    \ <span class=\"pl-s1\">dq_tagged</span>:\n\_\_\_\_<span class=\"pl-k\">if</span>\
    \ <span class=\"pl-s1\">tag</span> <span class=\"pl-c1\">in</span> <span class=\"\
    pl-s1\">tag_dict</span>:\n\_\_\_\_\_\_\_\_<span class=\"pl-s1\">tag_dict</span>[<span\
    \ class=\"pl-s1\">tag</span>]<span class=\"pl-c1\">+=</span><span class=\"pl-c1\"\
    >1</span>\n\_\_\_\_<span class=\"pl-k\">else</span>:\n\_\_\_\_\_\_\_\_<span class=\"\
    pl-s1\">tag_dict</span>[<span class=\"pl-s1\">tag</span>] <span class=\"pl-c1\"\
    >=</span> <span class=\"pl-c1\">1</span></pre></div>\n<p>Now let's see what we\
    \ got:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"\
    pl-s1\">tag_dict</span></pre></div>\n<p>This would be better with some order to\
    \ it, but dictionaries are made to be unordered. When we google \"sort dictionaries\
    \ python\" we find a solution in our great friend <a href=\"https://stackoverflow.com/a/613218\"\
    \ rel=\"nofollow\" target=\"_blank\"><em>Stack Overflow</em></a>. Even though\
    \ we cannot sort a dictionary, we can get a representation of a dictionary that\
    \ is sorted.</p>\n<p>Don't worry too much about understanding the following code,\
    \ as it uses functions and methods we have not discussed, and are out of the scope\
    \ of this course. It is useful to learn to reuse pieces of code even when we don't\
    \ fully understand them.</p>\n<p>Now let's do it and find out what the most common\
    \ tag is.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"\
    pl-s1\">tag_dict_sorted</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\"\
    >sorted</span>(<span class=\"pl-s1\">tag_dict</span>.<span class=\"pl-en\">items</span>(),\n\
    \_\_\_\_\_\_\_\_\_\_\_\_ <span class=\"pl-s1\">reverse</span><span class=\"pl-c1\"\
    >=</span><span class=\"pl-c1\">True</span>,\n\_\_\_\_\_\_\_\_\_\_\_\_ <span class=\"\
    pl-s1\">key</span><span class=\"pl-c1\">=</span><span class=\"pl-k\">lambda</span>\
    \ <span class=\"pl-s1\">kv</span>: <span class=\"pl-s1\">kv</span>[<span class=\"\
    pl-c1\">1</span>])</pre></div>\n<p>Now let's check out what we have:</p>\n<div\
    \ class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">print</span>(<span\
    \ class=\"pl-s1\">tag_dict_sorted</span>)</pre></div>\n<p>Your result should show\
    \ that NN is the most common tag. We can look up what NN means in the <a href=\"\
    https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\"\
    \ rel=\"nofollow\" target=\"_blank\">Penn Tree Bank</a>. Looks like NN is a Noun,\
    \ singular or mass. Great! This information will likely help us with genre classification,\
    \ or identifying the author of a text, or a variety of other functions.</p>"
  title: Part-of-Speech Tagging
  workshop: text-analysis
praxis:
  discussion_questions:
  - label: How can your research benefit from text analysis?
    order: 1
    workshop: text-analysis
  - label: What are the limits of the kind of text analysis we just went throught?
      What is it good for? What is it not well suited to do?
    order: 2
    workshop: text-analysis
  - label: What are the potential pitfalls for using massive data?
    order: 3
    workshop: text-analysis
  - label: How can we use text analysis in teaching?
    order: 4
    workshop: text-analysis
  further_readings:
  - annotation: '[A bit more advanced Jupyter Notebook tips and tricks](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)'
    title: A bit more advanced Jupyter Notebook tips and tricks
    url: https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts
  - annotation: '[The NLTK documentation](https://www.nltk.org/)'
    title: The NLTK documentation
    url: https://www.nltk.org
  intro: 'Congratulations! You are done with the Text Analysis workshop!

    As you may expect, this course was only the beginning of your jorney. NLTK is
    a vast and ever expanding world. The possibilies are numerous, with exciting new
    things coming everyday.

    Our goal was to show some of those possibilities, and equip you with enough tools/knowledge/skills
    to be able to keep advancing. In this page we suggest you further readings and
    other tutorials for which we feel you might be ready. Don''t be afraid to try!
    It is also useful to come back to this workshop with fresh eyes after a while.
    It will help you solidify some of the knowledge and make some things more clear.

    Have a happy journey!'
  next_steps: []
  tutorials:
  - annotation: '[Sentiment Analysis for Exploratory Data Analysis](https://programminghistorian.org/en/lessons/sentiment-analysis)'
    label: Sentiment Analysis for Exploratory Data Analysis
    url: https://programminghistorian.org/en/lessons/sentiment-analysis
  - annotation: '[A collection of Jupyter Notebooks on Mining the Social Web](https://github.com/mikhailklassen/Mining-the-Social-Web-3rd-Edition)'
    label: A collection of Jupyter Notebooks on Mining the Social Web
    url: https://github.com/mikhailklassen/Mining-the-Social-Web-3rd-Edition
  - annotation: '[Introduction to Stylometry](https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python)'
    label: Introduction to Stylometry
    url: https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python
  workshop: text-analysis
workshop:
  parent_backend: Github
  parent_branch: v2.0
  parent_repo: DHRI-Curriculum/text-analysis
  title: text-analysis
