[{"model": "workshop.workshop", "pk": 159, "fields": {"name": "Data And Ethics", "slug": "data-and-ethics", "created": "2020-07-09T19:01:19.054Z", "updated": "2020-07-09T19:01:19.054Z", "parent_backend": "Github", "parent_repo": "DHRI-Curriculum/data-and-ethics", "parent_branch": "v2.0-di-edits"}}, {"model": "frontmatter.frontmatter", "pk": 151, "fields": {"workshop": 159, "abstract": "What is data? Nearly all digital work requires dealing with data. In this workshop we will be discussing the basics of research data, in terms of material, transformation, and presentation.", "ethical_considerations": "['Data and data analysis is [not free from bias](https://medium.com/@angebassa/data-alone-isnt-ground-truth-9e733079dfd4). There is no magic blackbox for which data emerges from and is contextually driven. As we think about the automation process of looking at \"big\" data, we have to be aware of [the biases that gets reproduced that is \"hidden.\"](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)', 'De-identified information can be [reconstructed from piecemeal data](https://techscience.org/a/2015092903/)found across different sources. When we consider what we are doing with the data we have collected, we also need to think about the possible re-identification of our participants. ', 'Big data projects often times requiring sharing data sets across different individuals and teams. In addition, to ensure that our work is reproducable and accountable, we may also feel inclined to share the data collected. As such, figuring out [how to share such data](https://techscience.org/a/2015101601/) is crucial in the project planning stage.']", "estimated_time": 0, "projects": [373, 374], "resources": [], "readings": [741, 742, 743], "contributors": [471, 472, 473], "prerequisites": []}}, {"model": "praxis.praxis", "pk": 137, "fields": {"discussion_questions": "['What are some forms of data you use in your work? What about forms of data that you produce as your output? Perhaps there are some forms that are typical of your field? Where do you usually get your data from?', 'What is publically available data?', \"How do you decide the formats to store your data when you transition from 'raw' to 'processed/transformed' data? What are some of your considerations?\", 'How do we know when our data is cleaned enough? What happens to the data that is removed? What are we choosing to say about our dataset as we prepare them for analysis?', 'As we consider the types of analysis that we choose to apply onto our data set, what are we representing and leaving out? How do we guide our decisions of interpretation with our choices of analyses? Are we comfortable with the (un)intended use of our research? What are potential misuses of our outputs? ', 'What can happen when we are trying to just go for the next big thing (tool/methods/algorithms) or just ran out of time and/or budget for our project?', 'What are we assuming when we choose to visually represent data in particular ways? How can data visualization mislead us?']", "next_steps": "[]", "workshop": 159, "further_readings": [744, 745, 746], "more_projects": [], "more_resources": [], "tutorials": [362, 363]}}, {"model": "lesson.lesson", "pk": 1106, "fields": {"title": "Data is Foundational", "created": "2020-07-09T19:01:19.077Z", "updated": "2020-07-09T19:01:19.077Z", "workshop": 159, "text": "<p>In this brief workshop we will be discussing the basics of research data, in terms of material, transformation, and presentation. We will also be focusing on the ethics of data cleaning and representation. Because everyone has a different approach to data and ethics, this workshop will also include multiple sites for discussions to help us think together as a group.</p>\n<h2>What Constitutes Research Data?</h2>\n<p>\"Material or information on which an argument, theory, test or hypothesis, or another research output is based.\"</p><p>\nQueensland University of Technology. Manual of Procedures and Policies. Section 2.8.3. http://www.mopp.qut.edu.au/D/D_02_08.jsp</p><p>\n\"What constitutes such data will be determined by the community of interest through the process of peer review and program management. This may include, but is not limited to: data, publications, samples, physical collections, software and models\"</p><p>\nMarieke Guy. http://www.slideshare.net/MariekeGuy/bridging-the-gap-between-researchers-and-research-data-management , #2</p><p>\n\"Units of information created in the course of research\"</p><p>\nhttps://www.nsf.gov/bfa/dias/policy/dmpfaqs.jsp</p><p>\n\"(i) Research data is defined as the recorded factual material commonly accepted in the scientific community as necessary to validate research findings, but not any of the following: preliminary analyses, drafts of scientific papers, plans for future research, peer reviews, or communications with colleagues.\"</p><p>\nOMB-110, Subpart C, section 36, (d) (i), http://www.whitehouse.gov/omb/circulars_a110/</p><p>\n\"The short answer is that we can\u2019t always trust empirical measures at face value: data is always biased, measurements always contain errors, systems always have confounders, and people always make assumptions.\" Angela Bassa. https://medium.com/@angebassa/data-alone-isnt-ground-truth-9e733079dfd4</p><p>\nIn summary, research data is:</p><p>\n<strong>Material or information necessary to come to your conclusion.</strong></p>\n<h2>Forms of Data</h2>\n<p>There are many ways to represent data, just as there are many sources of data. After processing our data, we turn it into a number of products. For example:\n* Non-digital text (lab books, field notebooks)\n* Digital texts or digital copies of text\n* Spreadsheets\n* Audio\n* Video\n* Computer Aided Design/CAD\n* Statistical analysis (SPSS, SAS)\n* Databases\n* Geographic Information Systems (GIS) and spatial data\n* Digital copies of images\n* Web files\n* Scientific sample collections\n* Matlab files &amp; 3D Models\n* Metadata &amp; Paradata\n* Data visualizations\n* Computer code\n* Standard operating procedures and protocols\n* Protein or genetic sequences\n* Artistic products\n* Curriculum materials\n* Collection of digital objects acquired and generated during research</p><p>\nAdapted from: Georgia Tech\u2013http://libguides.gatech.edu/content.php?pid=123776&amp;sid=3067221</p>\n<h3>Challenge: Forms of Data</h3>\n<p>These are some (most!) of the shapes your research data might transform into. </p><p>\n1. What are some forms of data you use in your work? </p><p>\n2. What about forms of data that you produce as your output? Perhaps there are some forms that are typical of your field. </p><p>\n3. Where do you usually get your data from?</p>", "order": 1}}, {"model": "lesson.lesson", "pk": 1107, "fields": {"title": "Stages of Data", "created": "2020-07-09T19:01:19.113Z", "updated": "2020-07-09T19:01:19.113Z", "workshop": 159, "text": "<p>We begin without data. Then it is observed, or made, or imagined, or generated. After that, it goes through further transformations:</p>\n<h2>Raw</h2>\n<p>Raw data is yet to be processed, meaning it has yet to be manipulated by a human or computer. Received or collected data could be in any number of formats, locations, etc.. It could be in any of the forms listed above.</p><p>\nBut \"raw data\" is a relative term, inasmuch as when one person finishes processing data and presents it as a finished product, another person may take that product and work on it further, and for them that data is \"raw data\". </p>\n<h2>Data and Labor</h2>\n<p>As we think about data collection, we should also consider the labor involved in the process. Many researchers rely on Amazon Mechanical Turk (sometimes also refered to as MTurk) for data collection, <a href=\"https://www.theatlantic.com/business/archive/2018/01/amazon-mechanical-turk/551192/\">often paying less than minimum wage for the task.</a> Often the assumption made of these workers is someone who is retired, bored, and participating in online gig work for fun or to kill time. While this may be true for some, <a href=\"https://www.pewresearch.org/internet/2016/11/17/labor-platforms-technology-enabled-gig-work/\">more than half of those surveyed in a Pew Research study cite that the income from this work is essential or important.</a> Often, those who view the income from this work as essential or important are also from underserved communities. </p><p>\nIn addition to being mindful of paying a fair wage to the workers on such platforms, this working environment also brings some further considerations to the data that is collected. Often times, for workers to get close to minimum wage, they cannot afford to spend much time on each task, increasing potential errors in the collected data. </p>\n<h3>Challenge: Raw Data and Labor</h3>\n<ol>\n<li>For example, is \"big data\" \"raw data\"? How do we understand data that we have \"scraped\"? </li>\n</ol>\n<h2>Processed/Transformed</h2>\n<p>Processing data puts it into a state more readily available for analysis, and makes the data legible. For instance it could be rendered as <strong>structured data</strong>. This can also take many forms, e.g., a table. </p><p>\nHere are a few you're likely to come across, all representing the same data:</p><p>\n<strong>XML</strong></p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"nt\">&lt;Cats&gt;</span> \n    <span class=\"nt\">&lt;Cat&gt;</span> \n        <span class=\"nt\">&lt;firstName&gt;</span>Smally<span class=\"nt\">&lt;/firstName&gt;</span> <span class=\"nt\">&lt;lastName&gt;</span>McTiny<span class=\"nt\">&lt;/lastName&gt;</span> \n    <span class=\"nt\">&lt;/Cat&gt;</span> \n    <span class=\"nt\">&lt;Cat&gt;</span> \n        <span class=\"nt\">&lt;firstName&gt;</span>Kitty<span class=\"nt\">&lt;/firstName&gt;</span> <span class=\"nt\">&lt;lastName&gt;</span>Kitty<span class=\"nt\">&lt;/lastName&gt;</span> \n    <span class=\"nt\">&lt;/Cat&gt;</span> \n    <span class=\"nt\">&lt;Cat&gt;</span> \n        <span class=\"nt\">&lt;firstName&gt;</span>Foots<span class=\"nt\">&lt;/firstName&gt;</span> <span class=\"nt\">&lt;lastName&gt;</span>Smith<span class=\"nt\">&lt;/lastName&gt;</span> \n    <span class=\"nt\">&lt;/Cat&gt;</span> \n    <span class=\"nt\">&lt;Cat&gt;</span> \n        <span class=\"nt\">&lt;firstName&gt;</span>Tiger<span class=\"nt\">&lt;/firstName&gt;</span> <span class=\"nt\">&lt;lastName&gt;</span>Jaws<span class=\"nt\">&lt;/lastName&gt;</span> \n    <span class=\"nt\">&lt;/Cat&gt;</span> \n<span class=\"nt\">&lt;/Cats&gt;</span> \n</code></pre></div>\n<p><strong>JSON</strong></p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"p\">{</span><span class=\"nt\">\"Cats\"</span><span class=\"p\">:[</span> \n    <span class=\"p\">{</span> <span class=\"nt\">\"firstName\"</span><span class=\"p\">:</span><span class=\"s2\">\"Smally\"</span><span class=\"p\">,</span> <span class=\"nt\">\"lastName\"</span><span class=\"p\">:</span><span class=\"s2\">\"McTiny\"</span> <span class=\"p\">},</span> \n    <span class=\"p\">{</span> <span class=\"nt\">\"firstName\"</span><span class=\"p\">:</span><span class=\"s2\">\"Kitty\"</span><span class=\"p\">,</span> <span class=\"nt\">\"lastName\"</span><span class=\"p\">:</span><span class=\"s2\">\"Kitty\"</span> <span class=\"p\">},</span> \n    <span class=\"p\">{</span> <span class=\"nt\">\"firstName\"</span><span class=\"p\">:</span><span class=\"s2\">\"Foots\"</span><span class=\"p\">,</span> <span class=\"nt\">\"lastName\"</span><span class=\"p\">:</span><span class=\"s2\">\"Smith\"</span> <span class=\"p\">},</span> \n    <span class=\"p\">{</span> <span class=\"nt\">\"firstName\"</span><span class=\"p\">:</span><span class=\"s2\">\"Tiger\"</span><span class=\"p\">,</span> <span class=\"nt\">\"lastName\"</span><span class=\"p\">:</span><span class=\"s2\">\"Jaws\"</span> <span class=\"p\">}</span> \n<span class=\"p\">]}</span> \n</code></pre></div>\n<p><strong>CSV</strong></p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">First</span> <span class=\"n\">Name</span><span class=\"p\">,</span><span class=\"k\">Last</span> <span class=\"n\">Name</span><span class=\"o\">/</span><span class=\"n\">n</span>\n<span class=\"n\">Smally</span><span class=\"p\">,</span><span class=\"n\">McTiny</span><span class=\"o\">/</span><span class=\"n\">n</span>\n<span class=\"n\">Kitty</span><span class=\"p\">,</span><span class=\"n\">Kitty</span><span class=\"o\">/</span><span class=\"n\">n</span>\n<span class=\"n\">Foots</span><span class=\"p\">,</span><span class=\"n\">Smith</span><span class=\"o\">/</span><span class=\"n\">n</span>\n<span class=\"n\">Tiger</span><span class=\"p\">,</span><span class=\"n\">Jaws</span><span class=\"o\">/</span><span class=\"n\">n</span>\n</code></pre></div>\n<h3>The importance of using open data formats</h3>\n<p>A small detour to discuss (the ethics of?) data formats. For accessibility, future-proofing, and preservation, keep your data in open, sustainable formats. A demonstration:</p><p>\n1. Open <a href=\"cats.csv\">this file</a> in a text editor, and then in an app like Excel. This is a CSV, an open, text-only, file format.</p><p>\n2. Now do the same with <a href=\"cats.xlsx\">this one</a>. This is a proprietary format! </p><p>\nSustainable formats are generally unencrypted, uncompressed, and follow an open standard. A small list:\n* ASCII\n* PDF \n* .csv\n* FLAC\n* TIFF\n* JPEG2000\n* MPEG-4\n* XML\n* RDF\n* .txt\n* .r</p>\n<h3>Challenge: Processed/Transformed</h3>\n<p>How do you decide the formats to store your data when you transition from 'raw' to 'processed/transformed' data? What are some of your considerations?</p>\n<h2>Tidy Data</h2>\n<p>There are guidelines to the processing of data, sometimes referred to as <strong>Tidy Data</strong>.<sup>1</sup> One manifestation of these rules:</p><p>\n1. Each variable is in a column.</p><p>\n2. Each observation is a row.</p><p>\n3. Each value is a cell.</p><p>\nLook back at our example of cats to see how they may or may not follow those guidelines. <strong>Important note</strong>: some data formats allow for more than one dimension of data! How might that complicate the concept of <strong>Tidy Data</strong>?</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"p\">{</span><span class=\"nt\">\"Cats\"</span><span class=\"p\">:[</span>\n    <span class=\"p\">{</span><span class=\"nt\">\"Calico\"</span><span class=\"p\">:[</span>\n    <span class=\"p\">{</span> <span class=\"nt\">\"firstName\"</span><span class=\"p\">:</span><span class=\"s2\">\"Smally\"</span><span class=\"p\">,</span> <span class=\"nt\">\"lastName\"</span><span class=\"p\">:</span><span class=\"s2\">\"McTiny\"</span> <span class=\"p\">},</span>\n    <span class=\"p\">{</span> <span class=\"nt\">\"firstName\"</span><span class=\"p\">:</span><span class=\"s2\">\"Kitty\"</span><span class=\"p\">,</span> <span class=\"nt\">\"lastName\"</span><span class=\"p\">:</span><span class=\"s2\">\"Kitty\"</span> <span class=\"p\">}],</span>\n    <span class=\"nt\">\"Tortoiseshell\"</span><span class=\"p\">:[</span>\n    <span class=\"p\">{</span> <span class=\"nt\">\"firstName\"</span><span class=\"p\">:</span><span class=\"s2\">\"Foots\"</span><span class=\"p\">,</span> <span class=\"nt\">\"lastName\"</span><span class=\"p\">:</span><span class=\"s2\">\"Smith\"</span> <span class=\"p\">},</span> \n    <span class=\"p\">{</span> <span class=\"nt\">\"firstName\"</span><span class=\"p\">:</span><span class=\"s2\">\"Tiger\"</span><span class=\"p\">,</span> <span class=\"nt\">\"lastName\"</span><span class=\"p\">:</span><span class=\"s2\">\"Jaws\"</span> <span class=\"p\">}]}]}</span>\n</code></pre></div>\n<p><sup>1</sup>Wickham, Hadley. \"Tidy Data\". Journal of Statistical Software.</p>", "order": 2}}, {"model": "lesson.lesson", "pk": 1108, "fields": {"title": "More Stages of Data", "created": "2020-07-09T19:01:19.118Z", "updated": "2020-07-09T19:01:19.118Z", "workshop": 159, "text": "<h2>Cleaned</h2>\n<p>High quality data is measured in its <strong>validity</strong>, <strong>accuracy</strong>, <strong>completeness</strong>, <strong>consistency</strong>, and <strong>uniformity</strong>.</p><p>\nProcessed data, even in a table, is going to be full of errors:</p><p>\n1. Empty fields</p><p>\n2. Multiple formats, such as \"yes\" or \"y\" or \"1\" for a positive response.</p><p>\n3. Suspect answers, like a date of birth of 00/11/1234</p><p>\n4. Impossible negative numbers, like an age of \"-37\"</p><p>\n5. Dubious outliers</p><p>\n6. Duplicated rows</p><p>\n7. And many more!</p><p>\nCleaning data is the work of correcting the errors listed above, and moving towards high quality. This work can be done manually or programatically. </p><p>\n<strong>Validity</strong></p><p>\nMeasurements must be valid, in that they must conform to set constraints:</p><p>\n1. The aforementioned \"yes\" or \"y\" or \"1\" should all be changed to one response.</p><p>\n2. Certain fields cannot be empty, or the whole observation must be thrown out.</p><p>\n3. Uniqueness, for instance no two people should have the same social security number.</p><p>\n<strong>Accuracy</strong></p><p>\nMeasurements must be accurate, in that they must represent the correct values. While an observation may be valid, it might at the same time be inaccurate. 123 Fake street is a valid, inaccurate street address.</p><p>\nUnfortunately, accuracy is mostly acheived in the observation process. To be achieved in the cleaning process, an outside trusted source would have to be cross-referenced.</p><p>\n<strong>Completeness</strong></p><p>\nMeasurements must be complete, in that they must represent everything that might be known. This also is nearly impossible to achieve in the cleaning process! For instance in a survey, it would be necessary to re-interview someone whose previous answer to a question was left blank. </p><p>\n<strong>Consistency</strong></p><p>\nMeasurements must be consistent, in that different observations must not contradict each other. For instance, one person cannot be represented as both dead and still alive in different observations. </p><p>\n<strong>Uniformity</strong></p><p>\nMeasurements must be uniform, in that the same unit of measure must be used in all relevant measurements. If one person's height is listed in meters and another in feet, one measurement must be converted.</p>\n<h3>Challenge: When do we stop cleaning?</h3>\n<p>How do we know when our data is cleaned enough? What happens to the data that is removed? What are we choosing to say about our dataset as we prepare them for analysis?</p>\n<h2>Analyzed</h2>\n<p>Analysis can take many forms (just like the rest of this stuff!), but many techniques fall within a couple of categories:</p>\n<h3>Descriptive Analysis</h3>\n<p>Techniques geared towards summarizing a data set, such as:\n* Mean\n* Median\n* Mode\n* Average\n* Standard deviation</p>\n<h3>Inferential Analysis</h3>\n<p>Techniques geared towards testing a hypothesis about a population, based on your data set, such as:\n* Extrapolation\n* P-Value calculation</p>\n<h3>Challenge: Analysis</h3>\n<p>As we consider the types of analysis that we choose to apply onto our data set, what are we representing and leaving out? How do we guide our decisions of interpretation with our choices of analyses? Are we comfortable with the intended use of our research? Are we comfortable with the unintended use of our research? What are potential misuses of our outputs? What can happen when we are trying to just go for the next big thing (tool/methods/algorithms) or just ran out of time and/or budget for our project?</p>\n<h2>Visualized</h2>\n<ul>\n<li>Comparisons<ul>\n<li>bar &amp; line</li>\n<li>side-by-side bars</li>\n<li>dot plot</li>\n<li>scatter plot</li>\n</ul>\n</li>\n<li>Time<ul>\n<li>dot plot</li>\n<li>slope graph</li>\n<li>line graph</li>\n<li>sankey diagram</li>\n<li>timeline</li>\n</ul>\n</li>\n<li>Small numbers/percentages<ul>\n<li>pie chart</li>\n<li>tree map</li>\n<li>waterfall</li>\n</ul>\n</li>\n<li>Survey responses<ul>\n<li>bar chart</li>\n<li>stacked bar</li>\n<li>nested map</li>\n</ul>\n</li>\n<li>Place<ul>\n<li>chloropleth map</li>\n<li>hex or tile map</p><p>\nAdapted from Evergreen, Stephanie D. Effective data visualization : the right chart for the right data. Los Angeles: SAGE, 2017.</li>\n</ul>\n</li>\n</ul>\n<h3>Challenge: Visualizations</h3>\n<p>As we transform our results into visuals, we are also trying to tell a narrative about the data we collected. Data visualization can help us to decode information and share quickly and simply. What are we assuming when we choose to visually represent data in particular ways? How can data visualization mislead us?</p>", "order": 3}}, {"model": "lesson.lesson", "pk": 1109, "fields": {"title": "Data Literacy and Ethics", "created": "2020-07-09T19:01:19.122Z", "updated": "2020-07-09T19:01:19.122Z", "workshop": 159, "text": "<p>Throughout the workshop we have been thinking together through some of</p><p>\nthe potential ethical concerns that might crop up as we proceed with our</p><p>\nown projects. Just as we have disucssed thus far, we hope that you see</p><p>\nthat data and ethics is an ongoing process throughout the lifespans of</p><p>\nyour project(s) and don\u2019t often come with easy answers.</p>\n<h2>Activity</h2>\n<p>In this final activity, we would like for you to think about some of the</p><p>\npotential concerns that might come up in the scenario below and discuss</p><p>\nhow you might approach them:</p><p>\nYou are interested in looking at the reactions to the democratic party</p><p>\npresidential debates across time. You decided that you would use data</p><p>\nfrom twitter to analyze the responses. After collecting your data, you</p><p>\nlearned that your data has information from users who were later banned</p><p>\nand included some tweets that were removed/deleted from the site.</p>\n<h3>If you would like some guiding questions:</h3>\n<ul>\n<li>Would your approach differ if the responses were anonymized v. not?</li>\n<li>Would the number of tweets generated impact your decisions?</li>\n<li>How might where you are at in your project (e.g. \"raw\" data v. \"cleaned\" data v. analysed) affect your choices?</li>\n</ul>\n<h2>Some concluding thoughts</h2>\n<p>Data and ethics are contextually driven. As such, there isn\u2019t always a</p><p>\nrisk-free approach. We often have to work through ethical dilemmas while</p><p>\nthinking through information that we may not have (what are the risks of</p><p>\ndoing/not doing this work?). We may be approaching a moment where the</p><p>\nquestion is no longer what we could do but what we should do.</p>", "order": 4}}, {"model": "frontmatter.learningobjective", "pk": 935, "fields": {"frontmatter": 151, "label": "Understand the stages of data analysis."}}, {"model": "frontmatter.learningobjective", "pk": 936, "fields": {"frontmatter": 151, "label": "Understand the beginning of cleaning/tidying data"}}, {"model": "frontmatter.learningobjective", "pk": 937, "fields": {"frontmatter": 151, "label": "Experience the difference between proprietary and open data formats."}}, {"model": "frontmatter.learningobjective", "pk": 938, "fields": {"frontmatter": 151, "label": "Become familiar with the specific requirements of \"high quality data.\""}}, {"model": "frontmatter.learningobjective", "pk": 939, "fields": {"frontmatter": 151, "label": "Have an understanding of potential ethical concerns around working with different types of data and analysis."}}, {"model": "frontmatter.contributor", "pk": 471, "fields": {"first_name": "Stephen", "last_name": "Zweibel", "role": null, "url": null}}, {"model": "frontmatter.contributor", "pk": 472, "fields": {"first_name": "Di", "last_name": "Yoong", "role": null, "url": null}}, {"model": "frontmatter.contributor", "pk": 473, "fields": {"first_name": "Ian", "last_name": "Phillips", "role": null, "url": null}}, {"model": "library.reading", "pk": 741, "fields": {"title": "Big? Smart? Clean? Messy? Data in the Humanities", "url": "http://journalofdigitalhumanities.org/2-3/big-smart-clean-messy-data-in-the-humanities", "annotation": "[Big? Smart? Clean? Messy? Data in the Humanities](http://journalofdigitalhumanities.org/2-3/big-smart-clean-messy-data-in-the-humanities/)", "zotero_item": null}}, {"model": "library.reading", "pk": 742, "fields": {"title": "Bit By Bit: Social Research in Digital Age", "url": "https://www.bitbybitbook.com/en/1st-ed/preface", "annotation": "[Bit By Bit: Social Research in Digital Age](https://www.bitbybitbook.com/en/1st-ed/preface/)", "zotero_item": null}}, {"model": "library.reading", "pk": 743, "fields": {"title": "Ten Simple Rules for Responsible Big Data Research", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5373508", "annotation": "[Ten Simple Rules for Responsible Big Data Research](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5373508/)", "zotero_item": null}}, {"model": "library.project", "pk": 373, "fields": {"title": "Data for Public Good", "url": "https://dataforgood.commons.gc.cuny.edu", "annotation": "[Data for Public Good](https://dataforgood.commons.gc.cuny.edu/): Graduate student fellows creates a semester-long collaborative project that makes public-interest dataset useful and informative to a public audience.", "zotero_item": null}}, {"model": "library.project", "pk": 374, "fields": {"title": "SAFElab", "url": "https://safelab.socialwork.columbia.edu", "annotation": "[SAFElab](https://safelab.socialwork.columbia.edu/): Uses computational and social work approaches to understand mechanisms of violence and how to prevent and intervene in violence that occur in neighbourhoods and on social media.", "zotero_item": null}}, {"model": "library.tutorial", "pk": 362, "fields": {"label": "Computational social science with R", "url": "https://compsocialscience.github.io/summer-institute/curriculum#day_2", "annotation": "[Computational social science with R](https://compsocialscience.github.io/summer-institute/curriculum#day_2) by the Summer Institutes in Computational Social Science", "zotero_item": null}}, {"model": "library.tutorial", "pk": 363, "fields": {"label": "SQLite Tutorial", "url": "https://www.sqlitetutorial.net", "annotation": "[SQLite Tutorial](https://www.sqlitetutorial.net/) by SQLiteTutorial", "zotero_item": null}}, {"model": "library.reading", "pk": 744, "fields": {"title": "data management presentation", "url": "https://www.slideshare.net/MariekeGuy/bridging-the-gap-between-researchers-and-research-data-management", "annotation": "Marieke Guy's [data management presentation](https://www.slideshare.net/MariekeGuy/bridging-the-gap-between-researchers-and-research-data-management)", "zotero_item": null}}, {"model": "library.reading", "pk": 745, "fields": {"title": "Management of Research Data", "url": "http://www.mopp.qut.edu.au/D/D_02_08.jsp", "annotation": "Queensland University of Technology's [Management of Research Data](http://www.mopp.qut.edu.au/D/D_02_08.jsp).", "zotero_item": null}}, {"model": "library.reading", "pk": 746, "fields": {"title": "Perspectives on Big Data, Ethics, and Society", "url": "https://bdes.datasociety.net/council-output/perspectives-on-big-data-ethics-and-society", "annotation": "The Council for Big Data, Ethics, and Society's publication [Perspectives on Big Data, Ethics, and Society](https://bdes.datasociety.net/council-output/perspectives-on-big-data-ethics-and-society/).", "zotero_item": null}}, {"model": "workshop.workshop", "pk": 160, "fields": {"name": "Text Analysis", "slug": "text-analysis", "created": "2020-07-09T19:01:21.212Z", "updated": "2020-07-09T19:01:21.212Z", "parent_backend": "Github", "parent_repo": "DHRI-Curriculum/text-analysis", "parent_branch": "v2.0-rafa-edits"}}, {"model": "frontmatter.frontmatter", "pk": 152, "fields": {"workshop": 160, "abstract": "Digital technologies have made vast amounts of text available to researchers, and this same technological moment has provided us with the capacity to analyze that text. The first step in that analysis is to transform texts designed for human consumption into a form a computer can analyze. Using Python and the Natural Language ToolKit (commonly called NLTK), this workshop introduces strategies to turn qualitative texts into quantitative objects. Through that process, we will present a variety of strategies for simple analysis of text-based data.", "ethical_considerations": "['In working with massive amounts of text, it is natural to lose the original context. We must be aware of that and be careful when analizing it.', 'It is important to constantly question our assumptions and the indexes we are using. Numbers and graphs do not tell the story, our analysis does. We must be careful not to draw hasty and simplistic conclusions for things that are complex. Just because we found out that author A uses more unique words than author B, does it mean that A is a better writer than B?']", "estimated_time": "10", "projects": [375, 376, 377], "resources": [], "readings": [747, 748], "contributors": [474, 475, 476, 477, 478, 479, 480, 481, 482], "prerequisites": []}}, {"model": "praxis.praxis", "pk": 138, "fields": {"discussion_questions": "['Content TBD']", "next_steps": "[]", "workshop": 160, "further_readings": [749, 750], "more_projects": [], "more_resources": [], "tutorials": [364, 365, 366]}}, {"model": "lesson.lesson", "pk": 1110, "fields": {"title": "Overview", "created": "2020-07-09T19:01:21.218Z", "updated": "2020-07-09T19:01:21.218Z", "workshop": 160, "text": "<p>This tutorial will give a brief overview of the considerations and tools involved in basic text analysis with Python. By completing this tutorial, you will have a general sense of how to turn text into data using the Python package, NLTK. You will also be able to take publicly available text files and transform them into a corpus that you can perform your own analysis on. Finally, you will have some insight into the types of questions that can be addressed with text analysis.</p>\n<h2>Setup and installation</h2>\n<p>If you have not already installed the <a href=\"https://www.anaconda.com/download/\">Anaconda</a> distribution of Python 3, please do so.</p><p>\nYou will also need <a href=\"https://github.com/DHRI-Curriculum/install/blob/master/sections/nltk.md\"><code>nltk</code></a> and <a href=\"https://github.com/DHRI-Curriculum/install/blob/master/sections/conda.md\"><code>matplotlib</code></a> to complete this tutorial. Both packages come installed with Anaconda. To check to be sure you have them, open a new Jupyter Notebook (or any IDE to run Python).</p><p>\nFind Anaconda Navigator on your computer (it should be located in the folder with your other applications), and from Acadonda Navigator's interface, launch a Jupyter Notebook.</p><p>\n<img alt=\"jupyter notebook launch screen\" src=\"/static/images/lessons/text-analysis/jupyter.png\"/></p><p>\nIt will open in the browser. All of the directories (folders) in your home directory will appear \u2014 we'll get to that later. For now, select <code>New</code> &gt;&gt; <code>Python3</code> in the upper right corner.</p><p>\n<img alt='jupyter notebook \"open new file\" screen' src=\"/static/images/lessons/text-analysis/jupyter1.png\"/></p><p>\nA blank page with an empty box should appear.</p><p>\n<img alt=\"empty box on jupyter notebook new file\" src=\"/static/images/lessons/text-analysis/jupyter2.png\"/></p><p>\nIn the box, type:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">nltk</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib</span>\n</code></pre></div>\n<p>Press <code>Shift + Enter</code> to run the cell (or click run at the top of the page). Don't worry too much about what this is doing - that will be explained later in this tutorial. For now, we just want to make sure the packages we will need are installed.</p><p>\n<img alt=\"commands above in a jupyter notebook cell\" src=\"/static/images/lessons/text-analysis/jupyter3.png\"/></p><p>\nIf nothing happens, they are installed and you are ready to move on! If you get an error message, either you have a typo or they are not installed. If it is the latter, open the command line and type:</p>\n<div class=\"codehilite\"><pre><span></span><code>conda install nltk -y\nconda install matplotlib -y\n</code></pre></div>\n<p>Now we need to install the nltk corpus. This is very large and may take some time if you are on a weak connection.</p><p>\nIn the next cell, type:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">nltk</span><span class=\"o\">.</span><span class=\"n\">download</span><span class=\"p\">()</span>\n</code></pre></div>\n<p>and run the cell.</p><p>\nThe NLTK downloader should appear. Please install all of the packages. If you are short on time, focus on \"book\" for this tutorial\u2014you can download the other packages at another time for later use.</p><p>\nYours will look a little different, but the same interface. Click on the 'all' option and then 'Download'. Once they all trun green, you can close the Downloader dialogue box.</p><p>\n<img alt=\"nltk downloader screen\" src=\"/static/images/lessons/text-analysis/nltk.png\"/></p><p>\nReturn to your Jupyter Notebook and type:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">nltk.book</span> <span class=\"kn\">import</span> <span class=\"o\">*</span>\n</code></pre></div>\n<p>A list of books should appear. If this happens, great! If not, return to the downloader to make sure everything is ok.</p><p>\nClose this Notebook without saving \u2014 the only purpose was to check if we have the appropriate packages installed.</p>", "order": 1}}, {"model": "lesson.lesson", "pk": 1111, "fields": {"title": "Text as Data", "created": "2020-07-09T19:01:21.224Z", "updated": "2020-07-09T19:01:21.224Z", "workshop": 160, "text": "<p>When we think of \"data,\" we often think of numbers, things that can be summarized, statisticized, and graphed. Rarely when I ask people \"what is data?\" do they respond \"<em>Moby Dick</em>.\" And yet, more and more, text is data. Whether it is <em>Moby Dick</em>, or every romance novel written since 1750, or today's newspaper or twitter feed, we are able to transform written (and spoken) language into data that can be quantified and visualized.</p>\n<h2>Corpora</h2>\n<p>The first step in gathering insights from texts is to create a <strong>corpus</strong>. A corpus is a collection of texts that are somehow related to each other. For example, the <a href=\"https://corpus.byu.edu/coca/\">Corpus of Contemporary American English</a>, <a href=\"http://www.trumptwitterarchive.com/\">Donald Trump's Tweets</a>, <a href=\"https://byts.commons.gc.cuny.edu/\">text messages</a> sent by bilingual young adults, <a href=\"https://chroniclingamerica.loc.gov/newspapers/\">digitized newspapers</a>, or <a href=\"https://www.gutenberg.org/\">books</a> in the public domain are all corpora. There are infinitely many corpora, and, sometimes, you will want to make your own\u2014that is, one that best fits your research question.</p><p>\nThe route you take from here will depend on your research question. Let's say, for example, that you want to examine gender differences in writing style. Based on previous linguistic research, you hypothesize that male-identified authors use more definitives than female-identified. So you collect two corpora\u2014one written by men, one written by women\u2014and you count the number of <em>the</em>s, <em>this</em>s, and <em>that</em>s compared to the number of <em>a</em>s, <em>an</em>s, and <em>one</em>s. Maybe you find a difference, maybe you don't. We can already see that this is a relatively crude way of going about answering this question, but it is a start. (More likely, you'd use a <em>supervised classification task</em>, which you will learn about in the <a href=\"https://www.github.com/DHRI-Curriculum/machine-learning\">Machine Learning Tutorial</a>.)</p><p>\nThere has been some research about how the <a href=\"http://science.sciencemag.org/content/sci/331/6014/176.full.pdf\">linguistic complexity of written language</a> in long-form pieces (i.e., books, articles, letters, etc.) has decreased over time. Simply put, people today use shorter sentences with fewer embedded clauses and complex tense constructions than people did in the past. (Note that this is not necessarily a bad or good thing.) Based on this research, we want to know if short-form platforms are emblematic of the change (we predict that they are based on our own experience with short-form platforms like email and Twitter). One way to do this would be to use Part-of-Speech tagging. Part-of-Speech (POS) tagging is a way to identify the category of words in a given text.</p><p>\nFor example, the sentence:</p>\n<blockquote>\n<p>I like the red bicycle.</p><p>\nhas one pronoun, one verb, one determiner, one adjective, and one noun.</p><p>\n(I : Pronoun), (like : Verb), (the : Determiner), (red : Adjective), (bicycle : Noun)</p><p>\nNLTK uses the <a href=\"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\">Penn Tree Bank Tag Set</a>. This is a very detailed tag list that goes far beyond just nouns, verbs, and adjectives, but gives insight into different types of nouns, prepositions, and verbs as well. Virtually all POS taggers will create a list of (word, POS) pairs. If newspaper articles have a higher ratio of function words (prepositions, auxiliaries, determiners, etc.) to semantic words (nouns, verbs, adjectives), than tweets, then we have one piece of evidence supporting our hypothesis. It's important to note here that we must use either ratios or otherwise normalized data (in the sense that raw numbers will not work). Because of the way that language works (function words are often repeated, for example), a sample of 100 words will have more unique words than a sample of 1,000. Therefore, to compare different data types (articles vs. tweets), this fact should be taken into account.</p>\n</blockquote>", "order": 2}}, {"model": "lesson.lesson", "pk": 1112, "fields": {"title": "Cleaning and Normalizing", "created": "2020-07-09T19:01:21.235Z", "updated": "2020-07-09T19:01:21.235Z", "workshop": 160, "text": "<p>Generally, however, our questions are more about topics rather than writing style. So, once we have a corpus\u2014whether that is one text or millions\u2014we usually want to clean and normalize it. There are three terms we are going to need:</p><p>\n- <strong>Text normalization</strong> is the process of taking a list of words and transforming it into a more uniform sequence. Usually, this involves removing punctuation, making the words all the same case, removing <em>stop words</em>, and either <em>stemming</em> or <em>lemmatizing</em> the words. It can also include expanding abbreviations or matching misspellings (but these are advanced practices that we will not cover).</p><p>\nYou probably know what removing punctuation and capitalization refer to, but the other terms may be new:</p><p>\n- <strong>Stop words</strong> are words that appear frequently in a language, often adding grammatical structure, but little semantic content. There is no official list of stop words for any language, though there are some common, all-purpose lists built in to NLTK. However, different tasks require different lists. The purpose of removing stop words is to remove words that are so common that their meaning is diminished across a large number of texts.</p><p>\n- <strong>Stemming and lemmatizing</strong> both of these processes try to consolidate words like \"laughs\" and \"laughing\" to  \"laugh\" since they all mean essentially the same thing, they are just inflected differently. So again, in an attempt to reduce the number of words, and get a realistic understanding of the meaning of a text, these words are collapsed. Stemming does this by cutting off the end (very fast), lemmatizing does this by looking up the dictionary form (very slow).</p><p>\nLanguage is messy, and created for and by people, not computers. There is a lot of grammatical information in a sentence that a computer cannot use. For example, I could say to you:</p>\n<blockquote>\n<p>The house is burning.</p><p>\nand you would understand me. You would also understand if I say</p><p>\nhouse burn.</p><p>\nThe first has more information about tense, and which house in particular, but the sentiment is the same either way.</p><p>\nIn going from the first sentence to the normalized words, we removed the stop words (<em>the</em> and <em>is</em>), and removed punctuation and case, and lemmatized what was left (<em>burning</em> becomes <em>burn</em>\u2014though we might have stemmed this, its impossible to tell from the example). This results in what is essentially a \"bag of words,\" or a corpus of words without any structure. Because normalizing your text reduces the number of words (and therefore the number of dimensions in your data), and keeps only the words that contribute meaning to the document, this cleaning is usually desirable.</p><p>\nAgain, this will be covered more in depth in the Machine Learning Tutorial, but for the time being, we just need to know that there is \"clean\" and \"dirty\" versions of text data. Sometimes our questions are about the clean data, but sometimes our questions are in the \"dirt.\"</p>\n</blockquote>\n<h2>Words into numbers</h2>\n<p>In the next section, we are going to go through a series of methods that come built-in to NLTK that allow us to turn our words into numbers and visualizations. This is just scratching the surface, but should give you an idea of what is possible beyond just counting words.</p>", "order": 3}}, {"model": "lesson.lesson", "pk": 1113, "fields": {"title": "NLTK Methods with the NLTK Corpus", "created": "2020-07-09T19:01:21.242Z", "updated": "2020-07-09T19:01:21.242Z", "workshop": 160, "text": "<p>All of the code for this section is in a Jupyter Notebook in the GitHub repository. I encourage you to follow along by retyping all of the code, but if you get lost, or want another reference, the code is there as well.</p><p>\nTo open the notebook, first create a <code>projects</code> folder if you don't already have one by entering this command in your terminal:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"go\">mkdir -p ~/Desktop/projects</span>\n</code></pre></div>\n<p>If you already have a projects folder, you can skip this step.</p><p>\nNext, clone the text analysis session repository into your projects folder by entering this command:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"go\">git clone https://github.com/DHRI-Curriculum/text-analysis.git ~/Desktop/projects/text-analysis</span>\n</code></pre></div>\n<p>Then move to the new directory:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"go\">cd ~/Desktop/projects/text-analysis</span>\n</code></pre></div>\n<p>Now launch the Jupyter Notebook application by typing this into the terminal:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"go\">jupyter notebook</span>\n</code></pre></div>\n<p>If it's your first time opening the notebook, you may be prompted to enter a URL into your browser. Copy out the URL and paste it into the Firefox or Google Chrome search bar.</p><p>\nFinally, in the Jupyter Notebook file browser, find the notebook file and open it. It should be called <code>TextAnalysis.ipynb</code>. You will use this file for reference in case you get stuck in the next few sections, so keep it open.</p><p>\nReturn to the Jupyter Home Tab in your Browser (or Launch the Jupyter Notebook again), and start a New Python3 Notebook using the <code>New</code> button in the upper right corner.</p><p>\nEven though Jupyter Notebook doesn't force you to do so, it is very important to name your file, or you will end up later with a bunch of untitled files and you will have no idea what they are about. In the top left, click in the word <code>Untitled</code> and give your file a name such as \"intro_nltk\".</p><p>\nIn the first blank cell, type the following to import the NLTK library:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">nltk</span>\n</code></pre></div>\n<p><strong>Libraries</strong> are sets of instructions that Python can use to perform specialized functions. The Natural Language ToolKit (<code>nltk</code>) is one such library. As the name suggests, its focus is on language processing.</p><p>\nWe will also need the matplotlib library later on, so import it now:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">matplotlib</span>\n</code></pre></div>\n<p><code>matplotlib</code> is a library for making graphs. In the middle of this tutorial, we are going to make a dispersion plot of words in our texts.</p><p>\nFinally, because of a quirk of Jupyter notebooks, we need to specify that matplotlib should display its graphs in the notebook (as opposed to in a separate window), so we type this command (this is technically a Jupyter command, not Python):</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"o\">%</span><span class=\"n\">matplotlib</span> <span class=\"n\">inline</span>\n</code></pre></div>\n<p>All three of these commands can be written in the same cell and run all at once (<code>Shift + Enter</code>) or in different cells.</p><p>\n<img alt=\"Image showing that the three lines given above can be written in a single cell in the Jupyter notebook, one after another\" src=\"/static/images/lessons/text-analysis/imports.png\"/></p><p>\nIf you don't see an error when you run the notebook\u2014that is, if nothing happens\u2014you can move on to the next step.</p><p>\nNext, we need to load all of the NLTK corpora into our program. Even though we downloaded them to our computer, we need to tell Python we want to use them.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">nltk.book</span> <span class=\"kn\">import</span> <span class=\"o\">*</span>\n</code></pre></div>\n<p>The pre-loaded NLTK texts should appear again. These are preformatted data sets. We will still have to do some minor processing, but having the data in this format saves us a few steps. At the end of this tutorial, we will make our own corpus. This is a special type of python object specific to NLTK (it isn't a string, list, or dictionary). Sometimes it will behave like a string, and sometimes like a list of words. How it is behaving is noted for each function as we try it out.</p><p>\n<img alt='Image showing a second cell with the output of \"from nltk.book import *\"' src=\"/static/images/lessons/text-analysis/nltkbook.png\"/></p><p>\nLet's start by analyzing <em>Moby Dick</em>, which is <code>text1</code> for NLTK.</p>", "order": 4}}, {"model": "lesson.lesson", "pk": 1114, "fields": {"title": "Searching For Words", "created": "2020-07-09T19:01:21.248Z", "updated": "2020-07-09T19:01:21.248Z", "workshop": 160, "text": "<p>The first function we will look at is <code>concordance</code>. \"Concordance\" in this context means the characters on either side of the word. Our text is behaving like a string. As discussed in the <a href=\"https://github.com/DHRI-Curriculum/python\">Python tutorial LINK</a>, Python does not <em>evaluate</em> strings, so it just counts the number of characters on either side. By default, this is 25 characters on either side of our target word (including spaces).</p><p>\nIn the Jupyter Notebook, type:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">text1</span><span class=\"o\">.</span><span class=\"n\">concordance</span><span class=\"p\">(</span><span class=\"s2\">\"whale\"</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>The output shows us the 25 characters on either side of the word \"whale\" in <em>Moby Dick</em>. Let's try this with another word, \"love.\" Just replace the word \"whale\" with \"love,\" and we get the contexts in which Melville uses \"love\" in <em>Moby Dick</em>. <code>concordance</code> is used (behind the scenes) for several other functions, including <code>similar</code> and <code>common_contexts</code>.</p><p>\nLet's now see which words appear in similar contexts as the word \"love.\" NLTK has a built-in function for this as well: <code>similar</code>.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">text1</span><span class=\"o\">.</span><span class=\"n\">similar</span><span class=\"p\">(</span><span class=\"s2\">\"love\"</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>Behind the scenes, Python found all the contexts where the word \"love\" appears. It also finds similar environments, and then what words were common among the similar contexts. This gives a sense of what other words appear in similar contexts. This is somewhat interesting, but more interesting if we can compare it to something else. Let's take a look at another text. What about <em>Sense and Sensibility</em>? Let's see what words are similar to \"love\" in Jane Austen's writing. In the next cell, type:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">text2</span><span class=\"o\">.</span><span class=\"n\">similar</span><span class=\"p\">(</span><span class=\"s2\">\"love\"</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>We can compare the two and see immediately that Melville and Austen use the word \"love\" differently.</p>\n<h2>Investigating \"lol\"</h2>\n<p>Let's expand from novels for a minute and take a look at the NLTK Chat Corpus. In chats, text messages, and other digital communication platforms, \"lol\" is exceedingly common. We know it doesn't simply mean \"laughing out loud\"\u2014maybe the <code>similar</code> function can provide some insight into what it does mean.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">text5</span><span class=\"o\">.</span><span class=\"n\">similar</span><span class=\"p\">(</span><span class=\"s2\">\"lol\"</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>The resulting list is a lot of greetings, indicating that \"lol\" probably has more of a <a href=\"http://www.oxfordreference.com/view/10.1093/oi/authority.20110803100321840\">phatic function</a>. Phatic language is language primarily for communicating social closeness. Phatic words stand in contrast to semantic words, which contribute meaning to the utterance.</p><p>\nIf you are interested in this type of analysis, take a look at the <code>common_contexts</code> function in the <a href=\"https://www.nltk.org/book/\">NLTK book</a> or in the <a href=\"https://www.nltk.org/\">NLTK docs</a>.</p>", "order": 5}}, {"model": "lesson.lesson", "pk": 1115, "fields": {"title": "Positioning Words", "created": "2020-07-09T19:01:21.252Z", "updated": "2020-07-09T19:01:21.253Z", "workshop": 160, "text": "<p>In many ways, <code>concordance</code> and <code>similar</code> are heightened word searches that tell us something about what is happening near the target words. Another metric we can use is to visualize where the words appear in the text. In the case of <em>Moby Dick</em>, we want to compare where \"whale\" and \"monster\" appear throughout the text. In this case, the text is functioning as a list of words, and will make a mark where each word appears, offset from the first word. We will <em>pass</em> this <em>function</em> a <em>list</em> of <em>strings</em> to plot. This will likely help us develop a visual of the story \u2014 where the whale goes from being a whale to being a monster to being a whale again. In the next cell, type:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">text1</span><span class=\"o\">.</span><span class=\"n\">dispersion_plot</span><span class=\"p\">([</span><span class=\"s2\">\"whale\"</span><span class=\"p\">,</span> <span class=\"s2\">\"monster\"</span><span class=\"p\">])</span>\n</code></pre></div>\n<p>A graph should appear with a tick mark everywhere that \"whale\" appears and everywhere that \"monster\" appears. Knowing the story, we can interpret this graph and align it to what we know of how the narrative progresses. If we did not know the story, this could give us a picture of the narrative arc.</p><p>\nTry this with <code>text2</code>, <em>Sense and Sensibility</em>. Some relevant words are \"marriage,\" \"love,\" \"home,\" \"mother,\" \"husband,\" \"sister,\" and \"wife.\" Pick a few to compare. You can compare an unlimited number, but it's easier to read a few at a time. (Note that the comma in our writing here is <em>inside</em> the quotation mark but for Python, this would be unreadable and you would have to put commas outside of quotation marks to create a <em>list</em>.)</p><p>\nNLTK has many more functions built-in, but some of the most powerful functions are related to cleaning, part-of-speech tagging, and other stages in the text analysis pipeline (where the pipeline refers to the process of loading, cleaning, and analyzing text).</p>", "order": 6}}, {"model": "lesson.lesson", "pk": 1116, "fields": {"title": "Built-In Python Functions", "created": "2020-07-09T19:01:21.266Z", "updated": "2020-07-09T19:01:21.266Z", "workshop": 160, "text": "<p>We will now turn our attention away from the NLTK library and work with our text using the built-in Python functions\u2014the ones that come included with the Python language, rather than the NLTK library.</p>\n<h2>Types vs. tokens</h2>\n<p>First, let's find out how many times a given word appears in the corpus. In this case (and all cases going forward), our text will be treated as a list of words. Therefore, we will use the <code>count</code> function. We could just as easily do this with a text editor, but performing this in Python allows us to save it to a variable and then utilize this statistic in other calculations (for example, if we want to know what percentage of words in a corpus are 'lol', we would need a count of the 'lol's). In the next cell, type:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">text1</span><span class=\"o\">.</span><span class=\"n\">count</span><span class=\"p\">(</span><span class=\"s2\">\"whale\"</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>We see that \"whale\" occurs 906 times, but that seems a little low. Let's check on \"Whale\" and see how often that appears:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">text1</span><span class=\"o\">.</span><span class=\"n\">count</span><span class=\"p\">(</span><span class=\"s2\">\"Whale\"</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>\"Whale\" with a capital \"W\" appears 282 times. This is a problem for us\u2014we actually want them to be collapsed into one word, since \"whale\" and \"Whale\" really are the same for our purposes. We will deal with that in a moment. For the time being, we will accept that we have two entries for \"whale.\"</p><p>\nThis gets at a distinction between <strong>type</strong> and <strong>token</strong>. \"Whale\" and \"whale\" are different types (as of now) because they do not match identically. Every instance of \"whale\" in the corpus is another <strong>token</strong>\u2014it is an instance of the type, \"whale.\" Therefore, there are 906 tokens of \"whale\" in our corpus.</p><p>\nLet's fix this by making all of the words lowercase. We will make a new list of words, and call it \"text1_tokens\". We will fill this list with all the words in text1, but in their lowercase form. Python has a built-in function, <code>lower()</code> that takes all letters and makes them lowercase. In this same step, we are going to do a kind of tricky move, and only keep the words that are alphabetical and pass over anything that is punctuation or numbers. There is a built-in function, <code>isalpha()</code>, that will allow us to save only those words that are made of letters. If <code>isalpha()</code> is true, we'll make the word lowercase, and keep the word. If not, we'll pass over it and move to the next one.</p><p>\nType the following code into a new cell in your notebook. Pay special attention to the indentation, which must appear as below. (Note that in Jupyter Notebook, indentation usually comes automatically. If not, make sure to type the <code>space</code> key 4 times)</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">text1_tokens</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">text1</span><span class=\"p\">:</span>\n  <span class=\"k\">if</span> <span class=\"n\">t</span><span class=\"o\">.</span><span class=\"n\">isalpha</span><span class=\"p\">():</span>\n    <span class=\"n\">t</span> <span class=\"o\">=</span> <span class=\"n\">t</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">()</span>\n    <span class=\"n\">text1_tokens</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span>\n</code></pre></div>\n<p><img alt=\"image with code above in a jupyter notebook cell\" src=\"/static/images/lessons/text-analysis/for_loop_tokens.png\"/></p><p>\nAnother way to perform the same action more tersely is to use what's called a <a href=\"https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions\">list comprehension</a>. A list comprehension is a shorter, faster way to write a for-loop. It is syntactically a little more difficult to read (for a human), but, in this case, it's much faster to process. Don't worry too much about understanding the syntax of list comprehensions right now. For every example, we will show both the for loop and list comprehension options.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">text1_tokens</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">t</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">text1</span> <span class=\"k\">if</span> <span class=\"n\">t</span><span class=\"o\">.</span><span class=\"n\">isalpha</span><span class=\"p\">()]</span>\n</code></pre></div>\n<p>Great! Now <code>text1_tokens</code> is a list of all of the tokens in our corpus, with the punctuation removed, and all the words in lowercase.</p><p>\nNow we want to know how many words there are in our corpus\u2014that is, how many tokens in total. Therefore, we want to ask, \"What is the length of that list of words?\" Python has a built-in <code>len</code> function that allows you to find out the length of many types. Pass it a list, and it will tell you how many items are in the list. Pass it a string, and it will tell you how many characters are in the string. Pass it a dictionary, and it will tell you how many items are in the dictionary. In the next cell, type:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">text1_tokens</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>Just for comparison, check out how many words were in \"text1\"\u2014before we removed the punctuation and the numbers.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">text1</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>We see there are over 218,000 words in <em>Moby Dick</em> (including metadata). But this is the number of words total\u2014we want to know the number of unique words. That is, we want to know how many <em>types</em>, not just how many tokens.</p><p>\nIn order to get unique words, rather than just all words in general, we will make a <strong>set</strong> from the list. A <code>set</code> in Python work just like it would <a href=\"https://en.wikipedia.org/wiki/Set_(mathematics)\">in math</a>, it's all the unique values, with any duplicate items removed.</p><p>\nSo let's find out the length of our set. just like in math, we can also nest our functions. So, rather than saying <code>x = set(text1_tokens)</code> and then finding the length of \"x\", we can do it all in one step.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">text1_tokens</span><span class=\"p\">))</span>\n</code></pre></div>\n<p>Great! Now we can calculate the <strong>lexical density</strong> of <em>Moby Dick</em>. <a href=\"https://pdfs.semanticscholar.org/c2a8/56959d7f5880c98ccd4cfeb4b4f5b7133ec7.pdf\">Statistical studies</a> have shown that lexical density (the number of unique words per total words) is a good metric to approximate lexical diversity\u2014the range of vocabulary an author uses. For our first pass at lexical density, we will simply divide the number of unique words by the total number of words:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">text1_tokens</span><span class=\"p\">))</span><span class=\"o\">/</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">text1_tokens</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>If we want to use this metric to compare texts, we immediately notice a problem. Lexical density is dependent upon the length of a text and therefore is strictly a comparative measure. It is possible to compare 100 words from one text to 100 words from another, but because language is finite and repetitive, it is not possible to compare 100 words from one to 200 words from another. Even with these restrictions, lexical density is a useful metric in grade level estimations, <a href=\"http://www.mdpi.com/2226-471X/2/3/7\">vocabulary use</a> and genre classification, and a reasonable proxy for lexical diversity.</p><p>\nLet's take this constraint into account by working with only the first 10,000 words of our text. First we need to slice our list, returning the words in position 0 to position 9,999 (we'll actually write it as \"up to, but not including\" 10,000).</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">text1_slice</span> <span class=\"o\">=</span> <span class=\"n\">text1_tokens</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">:</span><span class=\"mi\">10000</span><span class=\"p\">]</span>\n</code></pre></div>\n<p>Now we can do the same calculation we did above:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">text1_slice</span><span class=\"p\">))</span> <span class=\"o\">/</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">text1_slice</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>This is a much higher number, though the number itself is arbitrary. When comparing different texts, this step is essential to get an accurate measure.</p>", "order": 7}}, {"model": "lesson.lesson", "pk": 1117, "fields": {"title": "Making Your Own Corpus: Data Cleaning", "created": "2020-07-09T19:01:21.295Z", "updated": "2020-07-09T19:01:21.295Z", "workshop": 160, "text": "<p>Thus far, we have been asking questions that take stopwords and grammatical features into account. For the most part, we want to exclude these features since they don't actually contribute very much semantic content to our models. Therefore, we will:</p><p>\n1. Remove capitalization and punctuation (we've already done this).</p><p>\n2. Remove stop words.</p><p>\n3. Lemmatize (or stem) our words, i.e. \"jumping\" and \"jumps\" become \"jump.\"</p>\n<h2>Removing Stopwords</h2>\n<p>We already completed step one, and are now working with our <code>text1_tokens</code>. Remember, this variable, <code>text1_tokens</code>, contains a list of strings that we will work with. We want to remove the stop words from that list. The NLTK library comes with fairly comprehensive lists of stop words for many languages. Stop words are function words that contribute very little semantic meaning and most often have grammatical functions. Usually, these are function words such as determiners, prepositions, auxiliaries, and others.</p><p>\nTo use NLTK's stop words, we need to import the list of words from the corpus. (We could have done this at the beginning of our program, and in more fully developed code, we would put it up there, but this works, too.) In the next cell, type:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">nltk.corpus</span> <span class=\"kn\">import</span> <span class=\"n\">stopwords</span>\n</code></pre></div>\n<p>We need to specify the English list, and save it into its own variable that we can use in the next step:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">stops</span> <span class=\"o\">=</span> <span class=\"n\">stopwords</span><span class=\"o\">.</span><span class=\"n\">words</span><span class=\"p\">(</span><span class=\"s1\">'english'</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>Now let's take a look at those words:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">stops</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>Now we want to go through all of the words in our text, and if that word is in the stop words list, remove it from our list. Otherwise, we want it to skip it. (The code below is VERY slow, so it may take some time to process). The way we can write this in Python is:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">text1_stops</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">text1_tokens</span><span class=\"p\">:</span>\n    <span class=\"k\">if</span> <span class=\"n\">t</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"n\">stops</span><span class=\"p\">:</span>\n        <span class=\"n\">text1_stops</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>A faster option, if you are feeling bold, would be using <a href=\"https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions\">list comprehensions</a>: </p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">text1_stops</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">t</span> <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">text1_tokens</span> <span class=\"k\">if</span> <span class=\"n\">t</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"n\">stops</span><span class=\"p\">]</span>\n</code></pre></div>\n<p>To check the result:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">text1_stops</span><span class=\"p\">[:</span><span class=\"mi\">30</span><span class=\"p\">])</span>\n</code></pre></div>\n<h3>Verifying List Contents</h3>\n<p>Now that we removed our stop words, let's see how many words are left in our list:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">text1_stops</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>You should get a much lower number.</p><p>\nFor reference, let's also check how many unique words there are. We will do this by making a set of words. Sets are the same in Python as they are in math, they are all of the unique words rather than all the words. So, if \"whale\" appears 200 times in the list of words, it will only appear once in the set.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">text1_stops</span><span class=\"p\">))</span>\n</code></pre></div>\n<h2>Lemmatizing Words</h2>\n<p>Now that we've removed the stop words from our corpus, the next step is to stem or lemmatize the remaining words. This means that we will strip off the grammatical structure from the words. For example, <code>cats \u2014&gt; cat</code>, and <code>walked \u2014&gt; walk</code>. If that was all we had to do, we could stem the corpus and achieve the correct result, because stemming (as the name implies) really just means cutting off affixes to find the root (or the stem). Very quickly, however, this gets complicated, such as in the case of <code>men \u2014&gt; man</code> and <code>sang \u2014&gt; sing</code>. Lemmatization deals with this by looking up the word in a reference and finding the appropriate root (though note that this still is not entirely accurate). Lemmatization, therefore, takes a relatively long time, since each word must be looked up in a reference. NLTK comes with pre-built stemmers and lemmatizers.</p><p>\nWe will use the WordNet Lemmatizer from the NLTK Stem library, so let's import that now:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">nltk.stem</span> <span class=\"kn\">import</span> <span class=\"n\">WordNetLemmatizer</span>\n</code></pre></div>\n<p>Because of the way that it is written \"under the hood,\" an instance of the lemmatizer needs to be called. We know this from reading <a href=\"https://www.nltk.org/\">the docs</a>.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">wordnet_lemmatizer</span> <span class=\"o\">=</span> <span class=\"n\">WordNetLemmatizer</span><span class=\"p\">()</span>\n</code></pre></div>\n<p>Let's quickly see what lemmatizing does.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">wordnet_lemmatizer</span><span class=\"o\">.</span><span class=\"n\">lemmatize</span><span class=\"p\">(</span><span class=\"s2\">\"children\"</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>Now try this one:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">wordnet_lemmatizer</span><span class=\"o\">.</span><span class=\"n\">lemmatize</span><span class=\"p\">(</span><span class=\"s2\">\"better\"</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>It didn't work, but...</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">wordnet_lemmatizer</span><span class=\"o\">.</span><span class=\"n\">lemmatize</span><span class=\"p\">(</span><span class=\"s2\">\"better\"</span><span class=\"p\">,</span> <span class=\"n\">pos</span><span class=\"o\">=</span><span class=\"s1\">'a'</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>... sometimes we can get better results if we define a specific part of speech(pos). \"a\" is for \"adjective\", as we learned <a href=\"http://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html\">here</a>.</p><p>\nNow we will lemmatize the words in the list.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">text1_clean</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">text1_stops</span><span class=\"p\">:</span>\n    <span class=\"n\">t_lem</span> <span class=\"o\">=</span> <span class=\"n\">wordnet_lemmatizer</span><span class=\"o\">.</span><span class=\"n\">lemmatize</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span>\n    <span class=\"n\">text1_clean</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">t_lem</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>And again, there is a faster version for you to use once you feel comfortable with list comprehensions:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">text1_clean</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">wordnet_lemmatizer</span><span class=\"o\">.</span><span class=\"n\">lemmatize</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">text1_stops</span><span class=\"p\">]</span>\n</code></pre></div>\n<h3>Verifying Clean List Contents</h3>\n<p>Let's check now to see the length of our final, cleaned version of the data, and then check the unique set of words. Notice how we will use the <code>print</code> function this time. Jupyter Notebook does print commands without the <code>print</code> function, but it will only print one thing per cell (the last command), and we wanted to print two different things:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">text1_clean</span><span class=\"p\">))</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">text1_clean</span><span class=\"p\">)))</span>\n</code></pre></div>\n<p>If everything went right, you should have the same length as before, but a smaller number of unique words. That makes sense since we did not remove any word, we only changed some of them.</p><p>\nNow if we were to calculate lexical density, we would be looking at how many word stems with semantic content are represented in <em>Moby Dick</em>, which is a different question than the one in our first analysis of lexical density.</p><p>\nWhy don't you try that by yourself? Try to remember how to calculate lexical density without looking back first. It is ok if you have forgotten.</p><p>\nNow let's have a look at the words Melville uses in <em>Moby Dick</em>. We'd like to look at all of the <em>types</em>, but not necessarily all of the <em>tokens.</em> We will order this set so that it is in an order we can handle. In the next cell, type:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">text1_clean</span><span class=\"p\">))[:</span><span class=\"mi\">30</span><span class=\"p\">]</span>\n</code></pre></div>\n<p><code>Sorted</code> + <code>set</code> should give us a list of list of all the words in <em>Moby Dick</em> in alphabetical order, but we only want to see the first ones. Notice how there are some words we wouldn't have expected, such as 'abandon', 'abandoned', 'abandonedly', and 'abandonment'. This process is far from perfect, but it is useful. However, depending on your goal, a different process, like <code>stemming</code> might be better. </p>\n<h2>Stemming Words</h2>\n<p>The code to implement this and view the output is below:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">nltk.stem</span> <span class=\"kn\">import</span> <span class=\"n\">PorterStemmer</span>\n<span class=\"n\">porter_stemmer</span> <span class=\"o\">=</span> <span class=\"n\">PorterStemmer</span><span class=\"p\">()</span>\n</code></pre></div>\n<p>The Porter is the most common Stemmer. Let's see what stemming does to words and compare it with lemmatizers:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">porter_stemmer</span><span class=\"o\">.</span><span class=\"n\">stem</span><span class=\"p\">(</span><span class=\"s1\">'berry'</span><span class=\"p\">))</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">porter_stemmer</span><span class=\"o\">.</span><span class=\"n\">stem</span><span class=\"p\">(</span><span class=\"s1\">'berries'</span><span class=\"p\">))</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">wordnet_lemmatizer</span><span class=\"o\">.</span><span class=\"n\">lemmatize</span><span class=\"p\">(</span><span class=\"s2\">\"berry\"</span><span class=\"p\">))</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">wordnet_lemmatizer</span><span class=\"o\">.</span><span class=\"n\">lemmatize</span><span class=\"p\">(</span><span class=\"s2\">\"berries\"</span><span class=\"p\">))</span>\n</code></pre></div>\n<p>Stemmer doesn't look so good, right? But how about checking how stemmer handles some of the words that our lemmatized \"failed\" us?</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">porter_stemmer</span><span class=\"o\">.</span><span class=\"n\">stem</span><span class=\"p\">(</span><span class=\"s1\">'abandon'</span><span class=\"p\">))</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">porter_stemmer</span><span class=\"o\">.</span><span class=\"n\">stem</span><span class=\"p\">(</span><span class=\"s1\">'abandoned'</span><span class=\"p\">))</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">porter_stemmer</span><span class=\"o\">.</span><span class=\"n\">stem</span><span class=\"p\">(</span><span class=\"s1\">'abandonedly'</span><span class=\"p\">))</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">porter_stemmer</span><span class=\"o\">.</span><span class=\"n\">stem</span><span class=\"p\">(</span><span class=\"s1\">'abandonment'</span><span class=\"p\">))</span>\n</code></pre></div>\n<p>Still not perfect, but a bit better. So the question is, how to choose between stemming and lemmatizing? As many things in text analysis, that depends. The best way to go is experimenting, seeing the results and chosing the one that better fits your goals.</p><p>\nAs a general rule, stemming is faster while lemmatizing is more accurate (but not always, as we just saw). For academics, usually the choice goes for the latter.</p><p>\nAnyway, let's stem our text with the Porter Stemmer:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">t1_porter</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">text1_clean</span><span class=\"p\">:</span>\n    <span class=\"n\">t_stemmed</span> <span class=\"o\">=</span> <span class=\"n\">porter_stemmer</span><span class=\"o\">.</span><span class=\"n\">stem</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span>\n    <span class=\"n\">t1_porter</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">t_stemmed</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>Or, if we want a faster way:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">t1_porter</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">porter_stemmer</span><span class=\"o\">.</span><span class=\"n\">stem</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">text1_clean</span><span class=\"p\">]</span>\n</code></pre></div>\n<p>And let's check the results:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">t1_porter</span><span class=\"p\">)))</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">t1_porter</span><span class=\"p\">))[:</span><span class=\"mi\">30</span><span class=\"p\">])</span>\n</code></pre></div>\n<p>A very different list of words is produced. This list is shorter than the list produced by the lemmatizer, but is also less accurate, and some of the words will completely change their meaning (like 'berry' becoming 'berri').</p>\n<h2>Back to the Lemmatized text</h2>\n<p>Now that we've seen some of the differences between both, we will proceed using our lemmatized corpus, which we saved as \"text1_clean\":</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">my_dist</span> <span class=\"o\">=</span> <span class=\"n\">FreqDist</span><span class=\"p\">(</span><span class=\"n\">text1_clean</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>If nothing happened, that is normal. Check to make sure it is there by calling for the type of the \"my_dist\" object.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">my_dist</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>The result should say it is a nltk probability distribution (<code>nltk.probability.FreqDist</code>). It doesn't matter too much right now what that is, only that it worked. We can now plot this with the matplotlib function, <code>plot</code>. We want to plot the first 20 entries of the my_dist object.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">my_dist</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">)</span>\n</code></pre></div>\n<p><img alt=\"nltk plot distribution\" src=\"/static/images/lessons/text-analysis/nltk_plot.png\"/></p><p>\nWe've made a nice image here, but it might be easier to comprehend as a list. Because this is a special probability distribution object we can call the <code>most_common</code> on this, too. Let's find the twenty most common words:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">my_dist</span><span class=\"o\">.</span><span class=\"n\">most_common</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>What about if we are interested in a list of specific words\u2014perhaps to identify texts that have biblical references. Let's make a (short) list of words that might suggest a biblical reference and see if they appear in <em>Moby Dick</em>. Set this list equal to a variable:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">b_words</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">'god'</span><span class=\"p\">,</span> <span class=\"s1\">'apostle'</span><span class=\"p\">,</span> <span class=\"s1\">'angel'</span><span class=\"p\">]</span>\n</code></pre></div>\n<p>Then we will loop through the words in our cleaned corpus, and see if any of them are in our list of biblical words. We'll then save into another list just those words that appear in both.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">my_list</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">word</span> <span class=\"ow\">in</span> <span class=\"n\">b_words</span><span class=\"p\">:</span>\n    <span class=\"k\">if</span> <span class=\"n\">word</span> <span class=\"ow\">in</span> <span class=\"n\">text1_clean</span><span class=\"p\">:</span>\n        <span class=\"n\">my_list</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">word</span><span class=\"p\">)</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"k\">pass</span>\n</code></pre></div>\n<p>And then we will print the results.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">my_list</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>You can obviously do this with much larger lists and even compare entire novels if you wish, though it would take a while with this approach. You can use this to get similarity measures and answer related questions.</p>", "order": 8}}, {"model": "lesson.lesson", "pk": 1118, "fields": {"title": "Make Your Own Corpus", "created": "2020-07-09T19:01:21.307Z", "updated": "2020-07-09T19:01:21.308Z", "workshop": 160, "text": "<p>Now that we have seen and implemented a series of text analysis techniques, let's go to the Internet to find a new text. You could use something such as historic newspapers, or Supreme Court proceedings, or use any txt file on your computer. Here we will use <a href=\"http://www.gutenberg.org\">Project Gutenberg</a>. Project Gutenberg is an archive of public domain written works, available in a wide variety of formats, including .txt. You can download these to your computer or access them via the url. We'll use the url method. We found <em>Don Quixote</em> in the archive, and will work with that.</p><p>\nThe Python package, urllib, comes installed with Python, but is inactive by default, so we still need to import it to utilize the functions. Since we are only going to use the urlopen function, we will just import that one.</p><p>\nIn the next cell, type:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">urllib.request</span> <span class=\"kn\">import</span> <span class=\"n\">urlopen</span>\n</code></pre></div>\n<p>The <code>urlopen</code> function allows your program to interact with files on the internet by opening them. It does not read them, however\u2014they are just available to be read in the next line. This is the default behavior any time a file is opened and read by Python. One reason is that you might want to read a file in different ways. For example, if you have a <strong>really</strong> big file\u2014think big data\u2014you might want to read line-by-line rather than the whole thing at once.</p><p>\nNow let's specify which URL we are going to use. Though you might be able to find <em>Don Quixote</em> in the Project Gutenberg files, please type this in so that we are all using the same format (there are multiple .txt files on the site, one with utf-8 encoding, another with ascii encoding). We want the utf-8 encoded one. The difference between these is beyond the scope of this tutorial, but you can check out this <a href=\"https://www.w3.org/International/questions/qa-what-is-encoding\">introduction to character encoding</a> from The World Wide Web Consortium (W3C) if you are interested.</p><p>\nSet the URL we want to a variable:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">my_url</span> <span class=\"o\">=</span> <span class=\"s2\">\"http://www.gutenberg.org/files/996/996-0.txt\"</span>\n</code></pre></div>\n<p>We still need to open the file and read the file. You will have to do this with files stored locally as well. (in which case, you would type the path to the file (i.e., <code>data/texts/mytext.txt</code>) in place of <code>my_url</code>)</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"nb\">file</span> <span class=\"o\">=</span> <span class=\"n\">urlopen</span><span class=\"p\">(</span><span class=\"n\">my_url</span><span class=\"p\">)</span>\n<span class=\"n\">raw</span> <span class=\"o\">=</span> <span class=\"nb\">file</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"p\">()</span>\n</code></pre></div>\n<p>This file is in bytes, so we need to decode it into a string. In the next cell, type:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">don</span> <span class=\"o\">=</span> <span class=\"n\">raw</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">()</span>\n</code></pre></div>\n<p>Now let's check on what kind of object we have in the \"don\" variable. Type:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">don</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>This should be a string. Great! We have just read in our first file and now we are going to transform that string into a text that we can perform NLTK functions on. Since we already imported nltk at the beginning of our program, we don't need to import it again, we can just use its functions by specifying <code>nltk</code> before the function. The first step is to tokenize the words, transforming the giant string into a list of words. A simple way to do this would be to split on spaces, and that would probably be fine, but we are going to use the NLTK tokenizer to ensure that edge cases are captured (i.e., \"don't\" is made into 2 words: \"do\" and \"n't\"). In the next cell, type:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">don_tokens</span> <span class=\"o\">=</span> <span class=\"n\">nltk</span><span class=\"o\">.</span><span class=\"n\">word_tokenize</span><span class=\"p\">(</span><span class=\"n\">don</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>You can check out the type of <code>don_tokens</code> using the <code>type()</code> function to make sure it worked\u2014it should be a list. Let's see how many words there are in our novel:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">don_tokens</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>Since this is a list, we can look at any slice of it that we want. Let's inspect the first ten words:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">don_tokens</span><span class=\"p\">[:</span><span class=\"mi\">10</span><span class=\"p\">]</span>\n</code></pre></div>\n<p>That looks like metadata\u2014not what we want to analyze. We will strip this off before proceeding. If you were doing this to many texts, you would want to use <a href=\"https://regexone.com/\">Regular Expressions</a>. Regular Expressions are an extremely powerful way to match text in a document. However, we are just using this text, so we could either guess, or cut and paste the text into a text reader and identify the position of the first content (i.e., how many words in is the first word). That is the route we are going to take. We found that the content begins at word 315, so let's make a slice of the text from word position 315 to the end.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">dq_text</span> <span class=\"o\">=</span> <span class=\"n\">don_tokens</span><span class=\"p\">[</span><span class=\"mi\">315</span><span class=\"p\">:]</span>\n</code></pre></div>\n<p>Finally, if we want to use the NLTK specific functions:</p><p>\n- <code>concordance</code></p><p>\n- <code>similar</code></p><p>\n- <code>dispersion_plot</code></p><p>\n- or others from the <a href=\"https://www.nltk.org/book/\">NLTK book</a></p><p>\nwe would have to make a specific NLTK <code>Text</code> object.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">dq_nltk_text</span> <span class=\"o\">=</span> <span class=\"n\">nltk</span><span class=\"o\">.</span><span class=\"n\">Text</span><span class=\"p\">(</span><span class=\"n\">dq_text</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>If we wanted to use the built-in Python functions, we can just stick with our list of words in <code>dq_text</code>. Since we've already covered all of those functions, we are going to move ahead with cleaning our text.</p><p>\nJust as we did earlier, we are going to remove the stopwords based on a list provided by NLTK, remove punctuation, and capitalization, and lemmatize the words. You can do it one by one as we did before, and that is totally fine. You can also merge some of the steps as you see below.</p><p>\n1. Lowercase, remove punctuation and stopwords</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">dq_clean</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">dq_text</span><span class=\"p\">:</span>\n    <span class=\"k\">if</span> <span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">isalpha</span><span class=\"p\">():</span>\n        <span class=\"k\">if</span> <span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">()</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"n\">stops</span><span class=\"p\">:</span>\n            <span class=\"n\">dq_clean</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">())</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">dq_clean</span><span class=\"p\">[:</span><span class=\"mi\">50</span><span class=\"p\">])</span>\n</code></pre></div>\n<p>2. Lemmatize</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">nltk.stem</span> <span class=\"kn\">import</span> <span class=\"n\">WordNetLemmatizer</span>\n<span class=\"n\">wordnet_lemmatizer</span> <span class=\"o\">=</span> <span class=\"n\">WordNetLemmatizer</span><span class=\"p\">()</span>\n<span class=\"n\">dq_lemmatized</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">dq_clean</span><span class=\"p\">:</span>\n    <span class=\"n\">dq_lemmatized</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">wordnet_lemmatizer</span><span class=\"o\">.</span><span class=\"n\">lemmatize</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">))</span>\n</code></pre></div>\n<p>From here, you could perform all of the operations that we did after cleaning our text in the previous session. Instead, we will perform another type of analysis: part-of-speech (POS) tagging.</p>", "order": 9}}, {"model": "lesson.lesson", "pk": 1119, "fields": {"title": "Part-of-Speech Tagging", "created": "2020-07-09T19:01:21.314Z", "updated": "2020-07-09T19:01:21.314Z", "workshop": 160, "text": "<p><em>Note that we are going to use the pre-cleaned, <code>dq_text</code> object for this section.</em></p><p>\nPOS tagging is going through a text and identifying which part of speech each word belongs to (i.e., Noun, Verb, or Adjective). Every word belongs to a part of speech, but some words can be confusing.</p><p>\n- Floyd is happy.</p><p>\n- Happy is a state of being.</p><p>\n- Happy has five letters.</p><p>\n- I'm going to Happy Cat tonight.</p><p>\nTherefore, part of speech is as much related to the word itself as its relationship to the words around it. A good part-of-speech tagger takes this into account, but there are some impossible cases as well:</p><p>\n- Wanda was entertaining last night.</p><p>\nPart of Speech tagging can be done very simply: with a very small <em>tag set</em>, or in a very complex way: with a much more elaborate tag set. We are going to implement a compromise, and use a neither small nor large tag set, the <a href=\"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\">Penn Tree Bank POS Tag Set</a>.</p><p>\nThis is the tag set that is pre-loaded into NLTK. When we call the tagger, we expect it to return an object with the word and the tag associated. Because POS tagging is dependent upon the stop words, we have to use a text that includes the stop words. Therefore, we will go back to using the <code>dq_text</code> object for this section. Let's try it out. Type:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">dq_tagged</span> <span class=\"o\">=</span> <span class=\"n\">nltk</span><span class=\"o\">.</span><span class=\"n\">pos_tag</span><span class=\"p\">(</span><span class=\"n\">dq_text</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>Let's inspect what we have:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">dq_tagged</span><span class=\"p\">[:</span><span class=\"mi\">10</span><span class=\"p\">])</span>\n</code></pre></div>\n<p>This is a list of ordered tuples. (A tuple is like a list, but can't be changed once it is created.) Each element in the list is a pairing of <code>(word, POS-tag)</code>. (Tuples are denoted with parentheses, rather than square brackets.) This is great, but it is very detailed. I would like to know how many Nouns, Verbs, and Adjectives I have. </p><p>\nFirst, I'll make an empty dictionary to hold my results. Then I will go through this list of tuples and count the number of times each tag appears. Every time I encounter a new tag, I'll add it to a dictionary and then increment by one every time I encounter that tag again. Let's see what that looks like in code:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">tag_dict</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n<span class=\"c1\"># For every word/tag pair in my list,</span>\n<span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">word</span><span class=\"p\">,</span> <span class=\"n\">tag</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"n\">dq_tagged</span><span class=\"p\">:</span>\n    <span class=\"k\">if</span> <span class=\"n\">tag</span> <span class=\"ow\">in</span> <span class=\"n\">tag_dict</span><span class=\"p\">:</span>\n        <span class=\"n\">tag_dict</span><span class=\"p\">[</span><span class=\"n\">tag</span><span class=\"p\">]</span><span class=\"o\">+=</span><span class=\"mi\">1</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"n\">tag_dict</span><span class=\"p\">[</span><span class=\"n\">tag</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n</code></pre></div>\n<p>Now let's see what we got:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">tag_dict</span>\n</code></pre></div>\n<p>This would be better with some order to it, but dictionaries are made to be unordered. When we google \"sort dictionaries python\" we find a solution in our great friend <a href=\"https://stackoverflow.com/a/613218\">stack overflow</a>. Even though we cannot sort a dictionary, we can get a representation of a dictionary that is sorted. Don't worry too much about understanding the following code, as it uses things we have not discussed, and are out of the scope of this course. It is useful to see how we can reuse pieces of code even when we don't fully understand them.</p><p>\nNow let's do it and find out what the most common tag is.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">tag_dict_sorted</span> <span class=\"o\">=</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">tag_dict</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">(),</span>\n             <span class=\"n\">reverse</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n             <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"k\">lambda</span> <span class=\"n\">kv</span><span class=\"p\">:</span> <span class=\"n\">kv</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">tag_dict_sorted</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>Now check out what we have. It looks like NN is the most common tag. We can look up what NN means in the <a href=\"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\">Penn Tree Bank</a>. Looks like NN is a Noun, singular or mass. Great! This information will likely help us with genre classification, or identifying the author of a text, or a variety of other functions.</p>", "order": 10}}, {"model": "lesson.lesson", "pk": 1120, "fields": {"title": "Conclusion", "created": "2020-07-09T19:01:21.318Z", "updated": "2020-07-09T19:01:21.318Z", "workshop": 160, "text": "<p>At this point, you should have a familiarity with what is possible with text analysis, and some of the most important functions (i.e., cleaning and part-of-speech tagging). Yet, this tutorial has only scratched the surface of what is possible with text analysis and natural language processing. It is a rapidly growing field, if you are interested, be sure to work through the online <a href=\"https://www.nltk.org/book/\">NLTK Book</a> as well as peruse the resources in the Zotero Library.</p>\n<h2>More Resources</h2>\n<ul>\n<li><a href=\"http://www.nltk.org/index.html\">NLTK Documentation</a></li>\n<li><a href=\"http://nlp.lsi.upc.edu/freeling/index.php\">FreeLing</a>, an open source language analysis tool suite.</li>\n<li><a href=\"https://programminghistorian.org/en/lessons/sentiment-analysis\">\"Sentiment Analysis for Exploratory Data Analysis\"</a>, a lesson to conduct \u2018sentiment analysis\u2019 on texts with Python and NLTK.</li>\n<li><a href=\"http://journalofdigitalhumanities.org/2-1/\">Journal of Digital Humanities, Vol. 2, No. 1 Winter 2012</a> contains interesting several interesting articles about topic modeling and text analysis.</li>\n<li><a href=\"https://github.com/cltk\">Classical Language Toolkit on GitHub</a>, Natural Language Processing specifically for Classical languages.</li>\n<li><a href=\"https://medium.com/agatha-codes/a-bossy-sort-of-voice-3c3a18de3093\">\"A Bossy Sort of Voice\"</a>, describes a Python/NLTK project quantifying gender bias in <em>Harry Pottery</em> with Python and NLTK.</li>\n<li><a href=\"https://kite.com/blog/python/python-digital-humanities-gothic-literature-nltk\">\"Finding Patterns in Gothic Literature\"</a>, describes a Python/NLTK project analyzing color in Gothic Literature.</li>\n<li><a href=\"https://www.pitt.edu/~naraehan/python3/faq.html\">\"Python and NLTK FAQs\"</a>, resources compiled by Na-Rae Han.</li>\n</ul>", "order": 11}}, {"model": "lesson.challenge", "pk": 264, "fields": {"lesson": 1116, "title": "", "text": "<p>Let's compare the lexical density of <em>Moby Dick</em> with <em>Sense and Sensibility</em>. Make sure to:</p>\n<ol>\n<li>Make all the words lowercase and remove punctuation.</li>\n<li>Make a slice of the first 10,000 words.</li>\n<li>Calculate lexical density by dividing the length of the set of the slice by the length of the slice.</li>\n</ol>"}}, {"model": "frontmatter.learningobjective", "pk": 940, "fields": {"frontmatter": 152, "label": "Identify strategies for transforming texts into numbers"}}, {"model": "frontmatter.learningobjective", "pk": 941, "fields": {"frontmatter": 152, "label": "Explain what a concordance is, how to find one, and why it matters"}}, {"model": "frontmatter.learningobjective", "pk": 942, "fields": {"frontmatter": 152, "label": "Compare frequency distribution of words in a text to quantify the narrative arc"}}, {"model": "frontmatter.learningobjective", "pk": 943, "fields": {"frontmatter": 152, "label": "Explain what stop words are and why they are often removed"}}, {"model": "frontmatter.learningobjective", "pk": 944, "fields": {"frontmatter": 152, "label": "Remove stop words in a variety of languages"}}, {"model": "frontmatter.learningobjective", "pk": 945, "fields": {"frontmatter": 152, "label": "Utilize Part-of-Speech tagging to gather insights about a text"}}, {"model": "frontmatter.learningobjective", "pk": 946, "fields": {"frontmatter": 152, "label": "Transform any document that you have (or have access to) in a .txt format into a text that can be analyzed computationally"}}, {"model": "frontmatter.contributor", "pk": 474, "fields": {"first_name": "Michelle", "last_name": "McSweeney", "role": null, "url": "https://github.com/michellejm"}}, {"model": "frontmatter.contributor", "pk": 475, "fields": {"first_name": "Rachel", "last_name": "Rakov", "role": null, "url": "https://github.com/rachelrakov"}}, {"model": "frontmatter.contributor", "pk": 476, "fields": {"first_name": "Rafael Davis", "last_name": "Portela", "role": null, "url": "https://github.com/rafadavis"}}, {"model": "frontmatter.contributor", "pk": 477, "fields": {"first_name": "Kalle", "last_name": "Westerling", "role": null, "url": "https://github.com/kallewesterling"}}, {"model": "frontmatter.contributor", "pk": 478, "fields": {"first_name": "Patrick", "last_name": "Smyth", "role": null, "url": "https://github.com/smythp"}}, {"model": "frontmatter.contributor", "pk": 479, "fields": {"first_name": "Hannah", "last_name": "Aizenman", "role": null, "url": "https://github.com/story645"}}, {"model": "frontmatter.contributor", "pk": 480, "fields": {"first_name": "Lisa", "last_name": "Rhody", "role": null, "url": "https://github.com/lmrhody"}}, {"model": "frontmatter.contributor", "pk": 481, "fields": {"first_name": "Kelsey", "last_name": "Chatlosh", "role": null, "url": "https://github.com/kchatlosh"}}, {"model": "frontmatter.contributor", "pk": 482, "fields": {"first_name": "Filipa", "last_name": "Calado", "role": null, "url": "https://github.com/gofilipa"}}, {"model": "library.reading", "pk": 747, "fields": {"title": "A Beginner\u2019s Tutorial to Jupyter Notebooks", "url": "https://towardsdatascience.com/a-beginners-tutorial-to-jupyter-notebooks-1b2f8705888a", "annotation": "[A Beginner\u2019s Tutorial to Jupyter Notebooks](https://towardsdatascience.com/a-beginners-tutorial-to-jupyter-notebooks-1b2f8705888a)", "zotero_item": null}}, {"model": "library.reading", "pk": 748, "fields": {"title": "What is text analysis", "url": "https://www.scribbr.com/methodology/textual-analysis", "annotation": "[What is text analysis](https://www.scribbr.com/methodology/textual-analysis/) <!--- I don't love this one, but haven't found a better one yet -->", "zotero_item": null}}, {"model": "library.project", "pk": 375, "fields": {"title": "Short list of academic Text & Data mining projects", "url": "https://libguides.bc.edu/textdatamining/projects", "annotation": "[Short list of academic Text & Data mining projects](https://libguides.bc.edu/textdatamining/projects) ", "zotero_item": null}}, {"model": "library.project", "pk": 376, "fields": {"title": "Building a Simple Chatbot from Scratch in Python", "url": "https://github.com/parulnith/Building-a-Simple-Chatbot-in-Python-using-NLTK", "annotation": "[Building a Simple Chatbot from Scratch in Python](https://github.com/parulnith/Building-a-Simple-Chatbot-in-Python-using-NLTK)", "zotero_item": null}}, {"model": "library.project", "pk": 377, "fields": {"title": "Classifying personality type by social media posts", "url": "https://github.com/TGDivy/MBTI-Personality-Classifier", "annotation": "[Classifying personality type by social media posts](https://github.com/TGDivy/MBTI-Personality-Classifier)", "zotero_item": null}}, {"model": "library.tutorial", "pk": 364, "fields": {"label": "Sentiment Analysis for Exploratory Data Analysis", "url": "https://programminghistorian.org/en/lessons/sentiment-analysis", "annotation": "[Sentiment Analysis for Exploratory Data Analysis](https://programminghistorian.org/en/lessons/sentiment-analysis)", "zotero_item": null}}, {"model": "library.tutorial", "pk": 365, "fields": {"label": "A collection of Jupyter Notebooks on Mining the Social Web", "url": "https://github.com/mikhailklassen/Mining-the-Social-Web-3rd-Edition", "annotation": "[A collection of Jupyter Notebooks on Mining the Social Web](https://github.com/mikhailklassen/Mining-the-Social-Web-3rd-Edition)", "zotero_item": null}}, {"model": "library.tutorial", "pk": 366, "fields": {"label": "Introduction to Stylometry", "url": "https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python", "annotation": "[Introduction to Stylometry](https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python)", "zotero_item": null}}, {"model": "library.reading", "pk": 749, "fields": {"title": "A bit more advanced Jupyter Notebook tips and tricks", "url": "https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts", "annotation": "[A bit more advanced Jupyter Notebook tips and tricks](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)", "zotero_item": null}}, {"model": "library.reading", "pk": 750, "fields": {"title": "The NLTK documentation", "url": "https://www.nltk.org", "annotation": "[The NLTK documentation](https://www.nltk.org/)", "zotero_item": null}}, {"model": "workshop.workshop", "pk": 161, "fields": {"name": "Command Line", "slug": "command-line", "created": "2020-07-09T19:01:23.000Z", "updated": "2020-07-09T19:01:23.000Z", "parent_backend": "Github", "parent_repo": "DHRI-Curriculum/command-line", "parent_branch": "v2.0-smorello-edits"}}, {"model": "frontmatter.frontmatter", "pk": 153, "fields": {"workshop": 161, "abstract": "By this point in our academic careers, most of us have figured out some ways we like to interact with computers. Whether that involves avoiding them as much as possible or constantly testing new software, we likely have some ideas about how we feel comfortable getting things done. How would you show a person who had never seen a computer, say [Kimmy Schmidt](https://youtu.be/LIdFa1qLgNQ) or [Brendan Fraser in *Blast from the Past*](https://youtu.be/Xq29uTtKW4M), how to *do* something on your computer?\nMany of us would explain what a screen and a cursor are, and then show how to point and click on icons. This approach relies on a graphical user interface, or GUI (pronounced \"gooey!\").\nToday we're going to explore another way to make your computer do things: through the command line. Instead of pointing and clicking, we'll be typing in either git bash (Windows) or terminal (macOS) to tell the computer directly what task we'd like it to perform.\nWhile this new technique can seem intimidating if you haven't used text-based interfaces before, luckily, you can use 90% of the functionality of the command line by becoming comfortable with a very small set of the most common commands.", "ethical_considerations": "[]", "estimated_time": "3", "projects": [378, 379], "resources": [], "readings": [751, 752, 753], "contributors": [483, 484], "prerequisites": []}}, {"model": "praxis.praxis", "pk": 139, "fields": {"discussion_questions": "['Content TBD']", "next_steps": "[]", "workshop": 161, "further_readings": [754, 755, 756, 757], "more_projects": [], "more_resources": [], "tutorials": [367, 368, 369]}}, {"model": "lesson.lesson", "pk": 1121, "fields": {"title": "What Is the Command Line?", "created": "2020-07-09T19:01:23.003Z", "updated": "2020-07-09T19:01:23.003Z", "workshop": 161, "text": "<p>The command line is a text-based way of interacting with your computer. You may hear it called different names, such as the terminal, the shell, or bash. In practice, you can use these terms interchangeably. (If you're curious, though, you can read more about them <a href=\"https://github.com/DHRI-Curriculum/glossary/blob/master/sections/command-line.md\">in the glossary</a>.) The shell we use (whether terminal, shell, or bash) is a program that accepts commands as text input and converts commands into appropriate operating system functions.</p><p>\nThe command line (of computers today) receives these commands as text that is typed in.</p>\n<h2>What does \"text-based\" mean?</h2>\n<p>For those of us comfortable reading and writing, the idea of \"text-based\" in the context of computers can seem a bit strange. As we start to get comfortable typing commands to the computer, it's important to distinguish \"text\" from word processed, desktop publishing (think Microsoft Word or Google Docs) in which we use software that displays what we want to produce without showing us the code the computer is reading to render the formatting. Plain text has the advantage of being manipulable in different contexts.</p><p>\nLet's take a quick moment to discuss text and text editors.</p>", "order": 1}}, {"model": "lesson.lesson", "pk": 1122, "fields": {"title": "Text editors", "created": "2020-07-09T19:01:23.009Z", "updated": "2020-07-09T19:01:23.009Z", "workshop": 161, "text": "<h2>What is text?</h2>\n<p>Before we explain which program we'll be using for editing text, we want to give a general sense of this \"text\" we keep mentioning. For those of us in the humanities, whether we follow literary theorists who read any object as a \"text\" or we dive into philology, paleography, codicology or any of the fields <a href=\"https://en.wikipedia.org/wiki/David_Greetham_(textual_scholar)\">David Greetham</a> lays out in <em>Textual Scholarship</em>, \"text\" has its specific meanings. As scholars working with computers, we need to be aware of the ways plain text and formatted text differ. Words on a screen may have hidden formatting. Many of us grew up using Microsoft Word and don't realize how much is going on behind the words shown on the screen. For the purposes of communicating with the computer and for easier movement between different programs, we need to use text without hidden formatting.</p><p>\n<img alt=\"Word Doc\" src=\"/static/images/lessons/command-line/worddoc.png\"/></p><p>\nUsers with visual disabilities, <a href=\"https://github.com/DHRI-Curriculum/command-line/blob/v2.0-smorello-edits/WordProcessorExample.docx?raw=true\">click here</a> to dowload the Word file.</p><p>\nIf you ask the command line to read that file, this Word .docx file will look something like this</p><p>\n<img alt=\"Word Doc as visualized by Command Line\" src=\"/static/images/lessons/command-line/CatWordDoc.png\"/></p><p></p><p>\nUsers with visual disabilities, <a href=\"PK.md\">click here</a> to dowload the text file.</p><p>\nWord documents which look like \"just words!\" are actually comprised of an archive of extensible markup language (XML) instructions that only Microsoft Word can read. Plain text files can be opened in a number of different editors and can be read within the command line.</p>\n<h2>Plain text</h2>\n<p>For the purposes of communicating with machines and between machines, we need characters to be as flexible as possible. Plain text include characters of readable material but not graphical representation.</p><p>\nAccording to the <a href=\"https://www.unicode.org/versions/Unicode12.1.0/\">Unicode Standard</a>,</p>\n<blockquote>\n<p>Plain text is a pure sequence of character codes; plain Unicode-encoded text is therefore a sequence of Unicode character codes.</p><p>\nPlain text has two main properties in regard to rich text:</p><p>\nplain text is the underlying content stream to which formatting can be applied. Plain text is public, standardized, and universally readable.</p><p>\nPlain text shows its cards\u2014if it's marked up, the markup will be human readable. Plain text can be moved between programs more fluidly and can respond to programmatic manipulations. Because it is not tied to a particular font or color or placement, plain text can be styled externally.</p><p>\nA counterpoint to plain text is rich text (sometimes denoted by the Microsoft rich text format .rtf file extension) or \"enriched text\" (sometimes seen as an option in email programs). In rich text files, plain text is elaborated with formatting specific to the program in which they are made.</p>\n</blockquote>\n<h2>Choosing a text editor</h2>\n<p>An important tool for programming and working in the command line is a text editor. A text editor is a program that allows you to edit plain text files, such as .txt, .csv, or .md. Text editors are not used to edit rich text documents, such as .docx or .rtf, and rich text editors should not be used to edit plain text files. This is because rich text editors will add many invisible special characters that will prevent programs from running and configuration files from being read correctly.</p><p>\nWhile it doesn't really matter which text editor you choose, you should try to become comfortable with at least one text editor.</p><p>\nChoosing a text editor has as much to do with personality as it does with functionality. Graphical user interfaces (GUIs), user options, and \"hackability\" vary from program to program.</p>\n<h2>Default recommendation</h2>\n<p>For our workshops, we will be using <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>. Not only is Visual Studio Code free and open source, but it is also consistent across OSX, Windows, and Linux systems.</p><p>\nYou will have downloaded VS Code according to the <a href=\"https://github.com/DHRI-Curriculum/install/blob/master/sections/vscode.md\">instructions</a> on the installations page. We won't be using the editor a lot in this tutorial, so don't worry about getting to know the editor now. In later workshops we will discuss syntax highlighting and version control, which Visual Studio Code supports. For now we will get back to working in the command line itself.</p>", "order": 2}}, {"model": "lesson.lesson", "pk": 1123, "fields": {"title": "Why is the command line useful?", "created": "2020-07-09T19:01:23.012Z", "updated": "2020-07-09T19:01:23.012Z", "workshop": 161, "text": "<p>Initially, for some of us, the command line can feel a bit unfamiliar. Why step away from a point-and-click workflow? By using the command line, we move into an environment where we have more minute control over each task we'd like the computer to perform. Instead of ordering your food in a restaurant, you're stepping into the kitchen. It's more work, but there are also more possibilities.</p><p>\nThe command line allows you to...</p><p>\n- Easily automate tasks such as creating, copying, and converting files.</p><p>\n- Set up your programming environment.</p><p>\n- Run programs you create.</p><p>\n- Access the (many) programs and utilities that do not have graphical equivalents.</p><p>\n- Control other computers remotely.</p><p>\nIn addition to being a useful tool in itself, the command line gives you access to a second set of programs and utilities and is a complement to learning programming.</p><p>\nWhat if all these cool possibilities seem a bit abstract to you right now? That's alright! On a very basic level, most uses of the command line are about <strong>showing information</strong> that the computer has, or <strong>modifying or making</strong> things (files, programs, etc.) on the computer.</p><p>\nIn the next section, we'll make this a little more clear by getting started with the command line.</p>", "order": 3}}, {"model": "lesson.lesson", "pk": 1124, "fields": {"title": "Getting to the command line", "created": "2020-07-09T19:01:23.016Z", "updated": "2020-07-09T19:01:23.016Z", "workshop": 161, "text": "<h2>macOS</h2>\n<p>If you're using macOS:</p><p>\n1. Click the Spotlight Search button (the magnifying glass) in the top right of your desktop.</p><p>\n2. Type \"terminal\" into the bar that appears.</p><p>\n3. Select the first item that appears in the list.</p><p>\n4. When the Terminal pops up, you will likely see either a window with black text over white background or colored text over a black background.</p><p>\n<img alt=\"Terminal in Mac OS\" src=\"/static/images/lessons/command-line/osx_term.png\"/></p><p>\nWhen you see the <code>$</code>, you're in the right place. We call the <code>$</code> the command prompt; the <code>$</code> lets us know the computer is ready to receive a command.</p><p>\n<em>You can change the color of your Terminal or BashShell background and text by selecting <code>Shell</code> from the top menu bar, then selecting a theme from the menu under <code>New Window</code>.</em></p><p>\nBonus points: if you really want to get the groove of just typing instead of pointing and clicking, you can press \"Command (\u2318)\" and the space bar at the same time to pull up Spotlight search, start typing \"Terminal,\" and then hit \"Enter\" to open a terminal window. This will pull up a terminal window without touching your mousepad. For super bonus points, try to navigate like this for the next fifteen minutes, or even the rest of this session\u2014it is tricky and sometimes a bit tiring when you start, but you can really pick up speed when you practice!</p>\n<h2>Windows</h2>\n<p>We won't be using Windows's own non-UNIX version of the command line. We installed Git Bash, following <a href=\"https://github.com/DHRI-Curriculum/install/blob/master/sections/git.md\">these instructions</a>, so that we can work in the cross-platform Unix command line for this session.</p><p>\n1. Look for Git Bash in your programs menu and open.</p><p>\n2. If you can't find the git folder, just type \"git bash\" in the search box and select \"git bash\" when it appears.</p><p>\n3. Open the program.</p><p>\n4. When the terminal pops up, you will likely see either a window with black text over white background or colored text over a black background.You know you're in the right place when you see the <code>$</code>.</p>\n<h2>Command prompt <code>$</code></h2>\n<p><code>$</code>, which we will refer to as the \"command prompt,\" is the place you type commands you wish the computer to execute. We will now learn some of the most common commands.</p><p>\nIn the next section, we'll learn how to navigate the filesystem in the command line.</p>", "order": 4}}, {"model": "lesson.lesson", "pk": 1125, "fields": {"title": "Navigation", "created": "2020-07-09T19:01:23.025Z", "updated": "2020-07-09T19:01:23.025Z", "workshop": 161, "text": "<h2>Prefatory pro tips</h2>\n<p>Go slow at first and check your spelling!</p><p>\nOne of the biggest things you can do to make sure your code runs correctly and you can use the command line successfully is to make sure you check your spelling! Keep this in mind today, this week, and your whole life. If at first something doesn't work, check your spelling! Unlike in human reading, where letters operate simultaneously as atomistic symbols and as complex contingencies (check <a href=\"https://genius.com/Johanna-drucker-from-a-to-screen-annotated\">Johanna Drucker</a> on the alphabet), in coding, each character has a discrete function including (especially!) spaces.</p><p>\nKeep in mind that the command line and file systems on macOS and Unix are usually pre-configured as cAsE-pReSeRvInG\u2014so capitalizations also matter when typing commands and file and folder names.</p><p>\nAlso, while copying and pasting from this handy tutorial may be tempting to avoid spelling errors and other things, we encourage you not to! Typing out each command will help you remember them and how they work.</p>\n<h3>Getting started: know thyself</h3>\n<p>You may also see your username to the left of the command prompt <code>$</code>. Let's try our first command. Type the following and press the <code>enter</code> key:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"gp\">$</span> whoami\n</code></pre></div>\n<p>The <code>whoami</code> command should print out your username. Congrats, you've executed your first command! This is a basic pattern of use in the command line: type a command, press <code>enter</code> on your keyboard, and receive output.</p>\n<h2>Orienting yourself in the command line: folders</h2>\n<p>OK, we're going to try another command. But first, let's make sure we understand some things about how your computer's filesystem works.</p><p>\nYour computer's files are organized in what's known as a hierarchical filesystem. That means there's a top level or \"root\" folder on your system. That folder has other folders in it, and those folders have folders in them, and so on. You can draw these relationships in a tree:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">Users</span>\n<span class=\"o\">|</span>\n <span class=\"err\">\u2014\u2014</span> <span class=\"n\">your</span><span class=\"o\">-</span><span class=\"n\">username</span>\n   <span class=\"o\">|</span>\n    <span class=\"err\">\u2014\u2014</span> <span class=\"n\">Applications</span>\n    <span class=\"err\">\u2014\u2014</span> <span class=\"n\">Desktop</span>\n    <span class=\"err\">\u2014\u2014</span> <span class=\"n\">Documents</span>\n</code></pre></div>\n<p>The root or highest-level folder on macOS is just called <code>/</code>. We won't need to go in there, though, since that's mostly just files for the operating system. On Windows, the root directory is usually called <code>C:</code> (<a href=\"http://www.todayifoundout.com/index.php/2015/04/c-drive-default-windows-based-computers-2/\">More on why C is default on Windows</a>).</p><p>\nNote that we are using the word \"directory\" interchangeably with \"folder\"\u2014they both refer to the same thing.</p><p>\nOK, let's try a command that tells us where we are in the filesystem:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"gp\">$</span> <span class=\"nb\">pwd</span>\n</code></pre></div>\n<p>You should get output like <code>/Users/your-username</code>. That means you're in the <code>your-username</code> directory in the <code>Users</code> folder inside the <code>/</code> or root directory. On Windows, your output would instead be <code>C:/Users/your-username</code>. The folder you're in is called the working directory, and <code>pwd</code> stands for \"print working directory.\"</p><p>\nThe command <code>pwd</code> won't actually print anything except on your screen. This command is easier to grasp when we interpret \"print\" as \"display.\"</p><p>\nOK, we know where we are. But what if we want to know what files and folders are in the <code>your-username</code> directory, a.k.a. the working directory?</p><p>\nTry entering:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"gp\">$</span> ls\n</code></pre></div>\n<p>You should see a number of folders, probably including <code>Documents</code>, <code>Desktop</code>, and so on. You may also see some files. These are the contents of the current working directory. <code>ls</code> will \"list\" the contents of the directory you are in.</p><p>\nWonder what's in the Desktop folder? Let's try navigating to it with the following command:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"gp\">$</span> <span class=\"nb\">cd</span> Desktop\n</code></pre></div>\n<p>The <code>cd</code> command lets us \"change directory.\" (Make sure the \"D\" in \"Desktop\" is capitalized.) If the command was successful, you won't see any output. This is normal\u2014often, the command line will succeed silently.</p><p>\nSo how do we know it worked? That's right, let's use our <code>pwd</code> command again. We should get:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"gp\">$</span> <span class=\"nb\">pwd</span>\n<span class=\"go\">/Users/your-username/Desktop</span>\n</code></pre></div>\n<p>Now try <code>ls</code> again to see what's on your desktop. These three commands\u2014<code>pwd</code>, <code>ls</code>, and <code>cd</code>\u2014are the most commonly used in the terminal. Between them, you can orient yourself and move around.</p><p>\nBefore we move on, let's take a minute to navigate through our computer's file system using the command line.</p>\n<h3>Compare with the GUI</h3>\n<p>It's important to note that this is the same old information you can get by pointing and clicking displayed to you in a different way.</p><p>\nGo ahead and use pointing and clicking to navigate to your working directory\u2014you can get there a few ways, but try starting from \"My Computer\" and clicking down from there. You'll notice that the folder names should match the ones that the command line spits out for you, since it's the same information! We're just using a different mode of navigation around your computer to see it.</p>\n<hr/>", "order": 5}}, {"model": "lesson.lesson", "pk": 1126, "fields": {"title": "Creating files and folders", "created": "2020-07-09T19:01:23.036Z", "updated": "2020-07-09T19:01:23.036Z", "workshop": 161, "text": "<h2>Creating a file</h2>\n<p>So far, we've only performed commands that give us information. Let's use a command that creates something on the computer.</p><p>\nFirst, make sure you're in the home directory:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"gp\">$</span> <span class=\"nb\">pwd</span>\n<span class=\"go\">/Users/your-username</span>\n</code></pre></div>\n<p>Let's move to the Desktop folder, or \"change directory\" with <code>cd</code>:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"go\">cd Desktop</span>\n</code></pre></div>\n<p>Once you've made sure you're in the Desktop folder with <code>pwd</code>, let's try a new command:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"go\">touch foo.txt</span>\n</code></pre></div>\n<p>If the command succeeds, you won't see any output. Now move the terminal window and look at your \"real\" desktop, the graphical one. See any differences? If the command was successful and you were in the right place, you should see an empty text file called \"foo.txt\" on the desktop. Pretty cool, right?</p>\n<h2>Handy tip: up arrow</h2>\n<p>Let's say you liked that \"foo.txt\" file so much you'd like another! In the terminal window, press the \"up arrow\" on your keyboard. You'll notice this populates the line with the command that you just wrote. You can hit \"Enter\" to create another \"foo.txt,\" (note - <a href=\"https://en.wikipedia.org/wiki/Touch_(Unix)\"><code>touch</code></a> command will not overwrite your document nor will it add another document to the same directory, but it will update info about that file.) or you could use your left/right arrows to change the file name to \"foot.txt\" to create something different.</p><p>\nAs we start to write more complicated and longer commands in our terminal, the \"up arrow\" is a great shortcut so you don't have to spend lots of time typing.</p>\n<h2>Creating folders</h2>\n<p>OK, so we're going to be doing a lot of work during the Digital Research Institute. Let's create a project folder in our Desktop so that we can keep all our work in one place.</p><p>\nFirst, let's check to make sure we're still in the Desktop folder with <code>pwd</code>:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"gp\">$</span> <span class=\"nb\">pwd</span>\n<span class=\"go\">/Users/your-username/Desktop</span>\n</code></pre></div>\n<p>Once you've double-checked you're in Desktop, we'll use the <code>mkdir</code> or \"make directory\" command to make a folder called \"projects\":</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"go\">mkdir projects</span>\n</code></pre></div>\n<p>Now run <code>ls</code> to see if a projects folder has appeared. Once you confirm that the projects folder was created successfully, <code>cd</code> into it.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"gp\">$</span> <span class=\"nb\">cd</span> projects\n<span class=\"gp\">$</span> <span class=\"nb\">pwd</span>\n<span class=\"go\">/Users/your-username/Desktop/projects</span>\n</code></pre></div>\n<h2>OK, now you've got a projects folder that you can use throughout the Institute. It should be visible on your graphical desktop, just like the <code>foo.txt</code> file we created earlier.</h2>", "order": 6}}, {"model": "lesson.lesson", "pk": 1127, "fields": {"title": "Creating a cheat sheet", "created": "2020-07-09T19:01:23.055Z", "updated": "2020-07-09T19:01:23.055Z", "workshop": 161, "text": "<p>In this section, we'll create a text file that we can use as a cheat sheet. You can use it to keep track of all the awesome commands you're learning.</p>\n<h2><code>Echo</code></h2>\n<p>Instead of creating an empty file like we did with <code>touch</code>, let's try creating a file with some text in it. But first, let's learn a new command: <code>echo</code></p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"gp\">$</span> <span class=\"nb\">echo</span> <span class=\"s2\">\"Hello from the command line\"</span>\n<span class=\"go\">Hello from the command line</span>\n</code></pre></div>\n<h2>Redirect (<code>&gt;</code>)</h2>\n<p>By default, the echo command just prints out the text we give it. Let's use it to create a file with some text in it:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"go\">echo \"This is my cheat sheet\" &gt; cheat-sheet.txt</span>\n</code></pre></div>\n<p>Now let's check the contents of the directory:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"gp\">$</span> <span class=\"nb\">pwd</span>\n<span class=\"go\">/Users/your-username/projects</span>\n<span class=\"gp\">$</span> ls\n<span class=\"go\">cheat-sheet.txt</span>\n</code></pre></div>\n<p>OK, so the file has been created. But what was the <code>&gt;</code> in the command we used? On the command line, a <code>&gt;</code> is known as a \"redirect.\" It takes the output of a command and puts it in a file. Be careful, since it's possible to overwrite files with the <code>&gt;</code> command.</p><p>\nIf you want to add text to a file but <em>not</em> overwrite it, you can use the <code>&gt;&gt;</code> command, known as the redirect and append command, instead. If there's already a file with text in it, this command can add text to the file <em>without</em> destroying and recreating it.</p>\n<h2><code>Cat</code></h2>\n<p>Let's check if there's any text in cheat-sheet.txt.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"go\">cat cheat-sheet.txt</span>\n<span class=\"go\">This is my cheat sheet</span>\n</code></pre></div>\n<p>As you can see, the <code>cat</code> command prints the contents of a file to the screen. <code>cat</code> stands for \"concatenate,\" because it can link strings of characters or files together from end to end.</p>\n<h2>A note on file naming</h2>\n<p>Your cheat sheet is titled <code>cheat-sheet.txt</code> instead of <code>cheat sheet.txt</code> for a reason. Can you guess why?</p><p>\nTry to make a file titled <code>cheat sheet.txt</code> and report to the class what happens.</p><p>\nNow imagine you're attempting to open a very important data file using the command line that is titled <code>cheat sheet.txt</code></p><p>\nFor your digital best practices, we recommend making sure that file names contain no spaces\u2014you can use creative capitalization, dashes, or underscores instead. Just keep in mind that the macOS and Unix file systems are usually pre-configured as cAsE-pReSeRvInG, which means that capitalization matters when you type commands to navigate between or do things to directories and files.</p>\n<h2>Using a text editor</h2>\n<p>The challenge for this section will be using a text editor, specifically Visual Studio Code (<a href=\"https://github.com/DHRI-Curriculum/install/blob/master/sections/vscode.md\">install guide here</a>), to add some of the commands that we've learned to the newly created cheat sheet. Text editors are programs that allow you to edit plain text files, such as .txt, .py (Python scripts), and .csv (comma-separated values, also known as spreadsheet files). Remember not to use programs such as Microsoft Word to edit text files, since they add invisible characters that can cause problems.</p>\n<h1>Pipes</h1>\n<p>So far, you've learned a number of commands and one special symbol, the <code>&gt;</code> or redirect. Now we're going to learn another, the <code>|</code> or \"pipe.\"</p><p>\nPipes let you take the output of one command and use it as the input for another.</p><p>\nLet's start with a simple example:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"gp\">$</span> <span class=\"nb\">echo</span> <span class=\"s2\">\"Hello from the command line\"</span> <span class=\"p\">|</span> wc -w\n<span class=\"go\">5</span>\n</code></pre></div>\n<p>In this example, we take the output of the <code>echo</code> command (\"Hello from the command line\") and pipe it to the <code>wc</code> or word count command, adding a flag <code>-w</code> for number of words. The result is the number of words in the text that we entered.</p><p>\nLet's try another. What if we wanted to put the commands in our cheat sheet in alphabetical order?</p><p>\nUse <code>pwd</code> and <code>cd</code> to make sure you're in the folder with your cheat sheet. Then try:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"go\">cat cheat-sheet.txt | sort</span>\n</code></pre></div>\n<p>You should see the contents of the cheat sheet file with each line rearranged in alphabetical order. If you wanted to save this output, you could use a <code>&gt;</code> to print the output to a file, like this:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"go\">cat cheat-sheet.txt | sort &gt; new-cheat-sheet.txt</span>\n</code></pre></div>\n<hr/>\n<h2>Example</h2>\n<h2><img alt=\"Pipes example\" src=\"/static/images/lessons/command-line/pipes.gif\"/></h2>\n<h1>Exploring text data</h1>\n<p>So far the only text file we've been working with is our cheat sheet. Now, this is where the command line can be a very powerful tool: let's try working with a large text file, one that would be too large to work with by hand.</p><p>\nLet's download the data we're going to work with:</p><p>\n<a href=\"https://github.com/DHRI-Curriculum/command-line/blob/v2.0-smorello-edits/nypl_items.csv?raw=true\">Download nypl_items.csv</a></p><p>\nOur data set is a list of public domain items from the New York Public Library. It's in .csv format, which is a plain text spreadsheet format. CSV stands for \"comma separated values,\" and each field in the spreadsheet is separated with a comma. It's all still plain text, though, so we can manipulate the data using the command line.</p>\n<h2>Move command</h2>\n<p>Once the file is downloaded, move it from your <code>Downloads</code> folder to the <code>projects</code> folder on your desktop\u2014either through the command line, or drag and drop in the GUI. Since this is indeed a command line workshop, you should try the former!</p><p>\nTo move this file using the command line, you first need to navigate to your <code>Downloads</code> folder where that file is saved. Then type the <code>mv</code> command followed by the name of the file you want to move and then the file path to your <code>projects</code> folder on your desktop, which is where you want to move that file to (note that <code>~</code> refers to your home folder):</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"go\">mv nypl_items.csv ~/Desktop/projects/</span>\n</code></pre></div>\n<p>You can then navigate to that <code>projects</code> folder and use the <code>ls</code> command to check that the file is now there.</p>\n<h2>Viewing data in the command line</h2>\n<p>Try using <code>cat</code> to look at the data. You'll find it all goes by too fast to get any sense of it. (You can click <code>Control</code> and <code>C</code> on your keyboard to cancel the output if it's taking too long.)</p><p>\nInstead, let's use another tool, the <code>less</code> command, to get the data one page at a time:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"gp\">$</span> less nypl_items.csv\n<span class=\"go\">...</span>\n</code></pre></div>\n<p><code>less</code> gives you a paginated view of the data; it will show you contents of a file or the output from a command or string of commands, page by page.</p><p>\nTo view the file contents page by page, you may use the following keyboard shortcuts (that should work on Windows using Git Bash or on macOS terminal):</p><p>\nClick the <code>f</code> key to view forward one page, or the <code>b</code> key to view back one page.</p><p>\nOnce you're done, click the <code>q</code> key to return to the command line.</p><p>\nLet's try two more commands for viewing the contents of a file:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"gp\">$</span> head nypl_items.csv\n<span class=\"go\">...</span>\n<span class=\"gp\">$</span> tail nypl_items.csv\n<span class=\"go\">...</span>\n</code></pre></div>\n<p>These commands print out the very first (the \"head\") and very last (the \"tail\") sections of the file, respectively.</p>\n<h2>Interlude for a favorite command line feature: tab completion</h2>\n<p>When you are navigating in the command line, typing folder and file names can seem to go against the promise of easier communication with your computer. Here comes <code>tab</code> completion, stage right!</p><p>\nWhen you need to type out a file or folder name\u2014for example, the name of that csv file we've been working with: nypl_items.csv\u2014in the command line and want to move more quickly, you can just type out the beginning characters of that file name up until it's distinct in that folder and then click the <code>tab</code> key. And voil\u00e0! Clicking that <code>tab</code> key will complete the rest of that name for you, and it only works if that file or folder already exists within your working directory.</p><p>\nIn other words, anytime in the command line you can type as much of the file or folder name that is unique within that directory, and <code>tab</code> complete the rest!</p>\n<h2>Note: Clearing Text</h2>\n<p>If all the text remaining in your terminal window is starting to overwhelm you, you have some options. You may type the <code>clear</code> command into the command line, or click the <code>command</code> and <code>k</code> keys to clear the scrollback. In macOS terminal, clicking the <code>command</code> and <code>l</code> keys will clear the output from your most recent command.</p>\n<h2>Cleaning the data</h2>\n<p>We didn't tell you this before, but there are duplicate lines in our data! Two, to be exact. Before we try removing them, let's see how many entries are in our .csv file:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"gp\">$</span> cat nypl_items.csv <span class=\"p\">|</span> wc -l\n<span class=\"go\">100001</span>\n</code></pre></div>\n<p>This tells us there are 100,001 lines in our file. The <code>wc</code> tool stands for \"word count,\" but it can also count characters and lines in a file. We tell <code>wc</code> to count lines by using the <code>-l</code> flag. If we wanted to count characters, we could use <code>wc -m</code>. Flags marked with hyphens, such as <code>-l</code> or <code>-m</code>, indicate options which belong to specific commands. See the <a href=\"https://github.com/DHRI-Curriculum/glossary/blob/master/sections/command-line.md\">glossary</a> for more information about flags and options.</p><p>\nTo find and remove duplicate lines, we can use the <code>uniq</code> command. Let's try it out:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"gp\">$</span> cat nypl_items.csv <span class=\"p\">|</span> uniq <span class=\"p\">|</span> wc -l\n<span class=\"go\">99999</span>\n</code></pre></div>\n<p>OK, the count went down by two because the <code>uniq</code> command removed the duplicate lines. But which lines were duplicated?</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"gp\">$</span> cat nypl_items.csv <span class=\"p\">|</span> uniq -d\n<span class=\"go\">...</span>\n</code></pre></div>\n<p>The <code>uniq</code> command with the <code>-d</code> flag prints out the lines that have duplicates.</p><p>\n<img alt=\"exploring data\" src=\"/static/images/lessons/command-line/data.gif\"/></p>\n<h1>Searching text data</h1>\n<p>So we've cleaned our data set, but how do we find entries that use a particular term?</p><p>\nLet's say I want to find all the entries in our data set that use the term \"Paris.\"</p><p>\nHere we can use the <code>grep</code> command. <code>grep</code> stands for \"global regular expression print.\" The <code>grep</code> command processes text line by line and prints any lines which match a specified pattern. Regular expressions are infamously human-illegible commands that use character by character matching to return a pattern. <code>grep</code> gives us access to the power of regular expressions as we search for text.</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"gp\">$</span> cat nypl_items.csv <span class=\"p\">|</span> grep -i <span class=\"s2\">\"paris\"</span>\n<span class=\"go\">...</span>\n</code></pre></div>\n<p>This will print out all the lines that contain the word \"Paris.\" (The <code>-i</code> flag makes the command ignore capitalization.) Let's use our <code>wc -l</code> command to see how many lines that is:</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"gp\">$</span> cat nypl_items.csv <span class=\"p\">|</span> grep -i <span class=\"s2\">\"paris\"</span> <span class=\"p\">|</span> wc -l\n<span class=\"go\">191</span>\n</code></pre></div>\n<p>Here we have asked <code>cat</code> to read nypl_items.csv, take the output and pipe it into the <code>grep -i</code> command, which will ignore capitalization and find all instances of the word \"paris.\" We then take the output of that <code>grep</code> command and pipe it into the word count <code>wc</code> command with the <code>-l</code> lines option. The pipeline returns <code>191</code> letting us know that Paris (or paris) occurs on 191 lines of our data set.</p>\n<h2>Example</h2>\n<p><img alt=\"Searching a text file with grep\" src=\"/static/images/lessons/command-line/grep.gif\"/></p>\n<h1>What we've learned</h1>\n<p>You've made it through your introduction to the command line! By now, you have experienced some of the power of communicating with your computer using text commands. The basic steps you learned today will help as you move forward through the week\u2014you'll work with the command line interface to set up your <a href=\"https://github.com/DHRI-Curriculum/git\">version control with git</a> and you'll have your text editor open while <a href=\"https://github.com/DHRI-Curriculum/python\">writing python scripts</a> and building basic websites with <a href=\"https://github.com/DHRI-Curriculum/html-css\">HTML and CSS</a>.</p><p>\nNow is a good time to do a quick review!</p><p>\nIn this session, we learned:</p><p>\n- how to use <code>touch</code> and <code>echo</code> to create files </p><p>\n- how to use <code>mkdir</code> to create folders</p><p>\n- how to navigate our file structure by <code>cd</code>(change directory), <code>pwd</code> (print working directory), and <code>ls</code> (list)</p><p>\n- how to use redirects (<code>&gt;</code>) and pipes (<code>|</code>) to create a pipeline</p><p>\n- how to explore a comma separated values (.csv) dataset using word and line counts, <code>head</code> and <code>tail</code>, and the concatenate command <code>cat</code></p><p>\n- how to search text files using the <code>grep</code> command</p><p>\nAnd we made a <a href=\"12-commands.md\">cheat sheet</a> for reference!</p><p>\nWhen we started, we reviewed what text is\u2014whether plain or enriched. We learned that text editors that don't fix formatting of font, color, and size, do allow for more flexible manipulation and multi-program use. If text is allowed to be a string of characters (and not specific characters chosen for their compliance with a designer's intention), that text can be fed through programs and altered with automated regularity. Text editors are different software than Bash (or Terminal), which is a text-based shell that allows you to interact directly with your operating system giving direct input and receiving output.</p><p>\nHaving a grasp of command line basics will not only make you more familiar with how your computer and basic programming work, but it will also give you access to tools and communities that will expand your research.</p>", "order": 7}}, {"model": "lesson.challenge", "pk": 265, "fields": {"lesson": 1125, "title": "", "text": "<p>Use the three commands you've just learned\u2014<code>pwd</code>, <code>ls</code> and <code>cd</code>\u2014eight (8) times each. Go poking around your Photos folder, or see what's so special about that root <code>/</code> directory. When you're done, come back to the home folder with</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"go\">cd ~</span>\n</code></pre></div>\n\n\n<p>(That's a tilde, on the top left of your keyboard.) One more command you might find useful is</p>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"go\">cd ..</span>\n</code></pre></div>\n\n\n<p>which will move you one directory up in the filesystem. That's a <code>cd</code> with two periods after it.</p>"}}, {"model": "lesson.challenge", "pk": 266, "fields": {"lesson": 1126, "title": "", "text": "<p>Try and create a sub-folder and file on your own!</p>"}}, {"model": "lesson.challenge", "pk": 267, "fields": {"lesson": 1127, "title": "", "text": "<p>You <em>could</em> use the GUI to open your Visual Studio Code text editor\u2014from your programs menu, via Finder or Applications or Launchpad in Mac OSX, or via the Windows button in Windows\u2014and then click \"File\" and then \"Open\" from the drop-down menu and navigate to your Desktop folder and click to open the cheat-sheet.txt file.</p>\n<p><em>Or</em>, you can open that specific cheat-sheet.txt file in the Visual Studio Code text editor directly from the command line! Let's try that by using the <code>code</code> command followed by the name of your file in the command line.</p>\n<p>Once you've got your cheat sheet open in the Visual Studio Code text editor, type to add the commands we've learned so far to the file. Include descriptions about what each command does. Remember, this cheat sheet is for you. Write descriptions that make sense to you or take notes about questions.</p>\n<p>Save the file.</p>\n<p>Once you're done, check the contents of the file on the command line with the <code>cat</code> command followed by the name of your file. </p>\n<p>Use the commands you've learned so far to create a new version of the <code>nypl_items.csv</code> file with the duplicated lines removed. (Hint: <a href=\"07-creating_a_cheat_sheet.md#redirect-\">redirects</a> are your friend.)</p>\n<p>Use the <code>grep</code> command to explore our .csv file a bit. What areas are best covered by the data set?</p>"}}, {"model": "lesson.solution", "pk": 149, "fields": {"challenge": 265, "title": "", "text": "<p>Type <code>pwd</code> to see where on your computer you are located </p><p></p><p>\nType <code>cd name-of-your-folder</code> to enter a subfolder </p><p></p><p>\nType <code>ls</code> to see the content of that folder </p><p></p><p>\nType <code>cd ..</code> to leave that folder </p><p></p><p>\nType <code>pwd</code> to make sure you are back to the folder where you wish to be </p><p></p><p>\nType <code>cd ~</code> to go back to your home folder </p><p></p><p>\nType <code>pwd</code> to make sure you are in the folder where you wish to be </p><p></p><p>\nType <code>cd /</code> to go back to your root folder </p><p></p><p>\nType <code>ls</code> to see the content of folder you are currently in </p><p></p><p>\nType <code>pwd</code> to make sure you are in the folder where you wish to be </p><p></p><p>\nType <code>cd name-of-your-folder</code> to enter a subfolder </p><p></p>\n<p><img alt=\"Navigating the command line\" src=\"sections/images/nav.gif\" /></p>\n<hr />"}}, {"model": "lesson.solution", "pk": 150, "fields": {"challenge": 266, "title": "", "text": "<p>Type <code>pwd</code> to see where on your computer you are located. If you are not in the \"projects\" folder we just created, navigate to that folder using the commands you learned in the previous <a href=\"https://github.com/DHRI-Curriculum/command-line/blob/v2.0-smorello-edits/lessons.md#navigation\">lesson</a> </p><p></p><p>\nType <code>mkdir name-of-your-subfolder</code> to create a subfolder </p><p></p><p>\nType <code>cd name-of-your-folder</code> to navigate to that folder </p><p></p><p>\nType <code>challenge.txt</code> to create a new text file </p><p></p><p>\nType <code>ls</code> to check whether you created the file correctly</p><p></p>\n<p><img alt=\"Creating files and folders\" src=\"sections/images/make-file-folder.gif\" /></p>\n<hr />"}}, {"model": "lesson.solution", "pk": 151, "fields": {"challenge": 267, "title": "", "text": "<ul>\n<li>Step 1 </li>\n</ul>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"gp\">$</span> code cheat-sheet.txt\n</code></pre></div>\n\n\n<ul>\n<li>Step 2 </p><p>\n ```console</p><p>\n$ cat cheat-sheet.txt</p><p>\nMy Institute Cheat Sheet</li>\n</ul>\n<p>ls</p><p>\nlists files and folders in a directory</p>\n<p>cd ~</p><p>\nchange directory to home folder</p>\n<p>...</p><p>\n```</p>\n<hr />\n<p>Type <code>pwd</code> to see where on your computer you are located. If you are not in the \"projects\" folder we just created, navigate to that folder using the commands you learned in the previous <a href=\"https://github.com/DHRI-Curriculum/command-line/blob/v2.0-smorello-edits/lessons.md#navigation\">lesson</a> </p><p></p><p>\nType <code>ls</code> to check whether the file <code>nypl_items.csv</code> is in your projects folder </p><p></p><p>\nType <code>cat nypl_items.csv | uniq -d &gt; new_nypl_items.csv</code> </p><p> to create a new version of the <code>nypl_items.csv</code> file with the duplicated lines removed.</p>\n<p>If you want to get a little more milage out of the grep command, refer to <a href=\"https://www.digitalocean.com/community/tutorials/using-grep-regular-expressions-to-search-for-text-patterns-in-linux\">this tutorial on grep and regular expressions</a>. Regular expressions (or regex) provide methods to search for text in more advanced ways, including specific wildcards, matching ranges of characters such as letters and numbers, and detecting features such as the beginning and end of lines. If you want to experiment with regular expressions in an easy-to-use environment, numerous regex test interfaces are available from <a href=\"https://www.google.com/search?w&amp;q=regex+tester\">a simple google search</a>, such as <a href=\"https://regexr.com/\">RegExr</a>, which includes a handy cheat sheet.</p>\n<hr />"}}, {"model": "frontmatter.learningobjective", "pk": 947, "fields": {"frontmatter": 153, "label": "Learn common commands to create files (`touch` and `echo`)"}}, {"model": "frontmatter.learningobjective", "pk": 948, "fields": {"frontmatter": 153, "label": "Learn commands to create directories (`mkdir`)"}}, {"model": "frontmatter.learningobjective", "pk": 949, "fields": {"frontmatter": 153, "label": "Navigate our file structure using change directory (`cd`), print working directory (`pwd`), and list (`ls`)"}}, {"model": "frontmatter.learningobjective", "pk": 950, "fields": {"frontmatter": 153, "label": "Move content from one place to another using redirects (`>`) and pipes (`|`)"}}, {"model": "frontmatter.learningobjective", "pk": 951, "fields": {"frontmatter": 153, "label": "Explore a comma separated values (.csv) dataset using word and line counts, `head` and `tail`, and the concatenate command `cat`"}}, {"model": "frontmatter.learningobjective", "pk": 952, "fields": {"frontmatter": 153, "label": "Search text files using the `grep` command"}}, {"model": "frontmatter.learningobjective", "pk": 953, "fields": {"frontmatter": 153, "label": "Create and sort cheat sheets for the commands we learn"}}, {"model": "frontmatter.contributor", "pk": 483, "fields": {"first_name": "Stefano", "last_name": "Morello", "role": "Current Author", "url": null}}, {"model": "frontmatter.contributor", "pk": 484, "fields": {"first_name": "Patrick Smyth", "last_name": "Kelsey Chatlosh", "role": "Contributing Authors", "url": null}}, {"model": "library.reading", "pk": 751, "fields": {"title": "Neal Stephenson, \"In the Beginning Was the Command Line\"", "url": "http://cristal.inria.fr/~weis/info/commandline.html", "annotation": "[Neal Stephenson, \"In the Beginning Was the Command Line\"](http://cristal.inria.fr/~weis/info/commandline.html)", "zotero_item": null}}, {"model": "library.reading", "pk": 752, "fields": {"title": "Douglas Rushkoff, *Program or Be Programmed*", "url": "https://rushkoff.com/books/program-or-be-programmed", "annotation": "[Douglas Rushkoff, *Program or Be Programmed*](https://rushkoff.com/books/program-or-be-programmed/)", "zotero_item": null}}, {"model": "library.reading", "pk": 753, "fields": {"title": "Free Code Camp, A Brief History of the Command Line", "url": "https://www.freecodecamp.org/news/the-command-line-1fdbc692b3bf", "annotation": "[Free Code Camp, A Brief History of the Command Line](https://www.freecodecamp.org/news/the-command-line-1fdbc692b3bf/)", "zotero_item": null}}, {"model": "library.project", "pk": 378, "fields": {"title": "", "url": null, "annotation": "Most Python- and R-based projects will require you to have some knowledge of the command line. At a very basic level, you will be invoking a Python script and will be using values of command line arguments when creating and running your scripts.", "zotero_item": null}}, {"model": "library.project", "pk": 379, "fields": {"title": "Omeka", "url": "www.omeka.org", "annotation": "The command line is also useful for setting up installations of server side software like [Omeka](www.omeka.org). The command line will allow you to navigate the file structure of your server. Commands like ls, mkdir, rmdir, cd, etc. are really important. For example, grep could help you find a plugin directory that you might have accidentally placed in the wrong location.", "zotero_item": null}}, {"model": "library.tutorial", "pk": 367, "fields": {"label": "Ryans Tutorials, Linux Tutorial", "url": "https://ryanstutorials.net/linuxtutorial", "annotation": "[Ryans Tutorials, Linux Tutorial](https://ryanstutorials.net/linuxtutorial/)", "zotero_item": null}}, {"model": "library.tutorial", "pk": 368, "fields": {"label": "The Linux command line for Beginners", "url": "https://ubuntu.com/tutorials/command-line-for-beginners#1-overview", "annotation": "[The Linux command line for Beginners](https://ubuntu.com/tutorials/command-line-for-beginners#1-overview)", "zotero_item": null}}, {"model": "library.tutorial", "pk": 369, "fields": {"label": "Data Science at the Command Line", "url": "https://www.datascienceatthecommandline.com", "annotation": "[Data Science at the Command Line](https://www.datascienceatthecommandline.com/)", "zotero_item": null}}, {"model": "library.reading", "pk": 754, "fields": {"title": "Free Software Foundation, The Bash Manual", "url": "https://www.gnu.org/savannah-checkouts/gnu/bash/manual/bash.html", "annotation": "[Free Software Foundation, The Bash Manual](https://www.gnu.org/savannah-checkouts/gnu/bash/manual/bash.html)", "zotero_item": null}}, {"model": "library.reading", "pk": 755, "fields": {"title": "Dennis Tenen and Grant Wythoff, Sustainable Authorship in Plain Text using Pandoc and Markdown", "url": "https://programminghistorian.org/en/lessons/sustainable-authorship-in-plain-text-using-pandoc-and-markdown", "annotation": "[Dennis Tenen and Grant Wythoff, Sustainable Authorship in Plain Text using Pandoc and Markdown](https://programminghistorian.org/en/lessons/sustainable-authorship-in-plain-text-using-pandoc-and-markdown)", "zotero_item": null}}, {"model": "library.reading", "pk": 756, "fields": {"title": "Stephen Ramsay, Life on the Command Line", "url": "https://files.zotero.net/eyJleHBpcmVzIjoxNTkyNjY1MDk3LCJoYXNoIjoiODFkNDJmZmU1ZjU3YzRmMDE2YTQ1ZmQwY2YzOTUwYmIiLCJjb250ZW50VHlwZSI6InRleHRcL2h0bWwiLCJjaGFyc2V0IjoidXRmLTgiLCJ6aXAiOjF9/07826342b83ea870f846cfa48f1b0eb8d3d51b78ceb1b05b1e014467d7241904/life-on-the-command-line.html", "annotation": "[Stephen Ramsay, Life on the Command Line](https://files.zotero.net/eyJleHBpcmVzIjoxNTkyNjY1MDk3LCJoYXNoIjoiODFkNDJmZmU1ZjU3YzRmMDE2YTQ1ZmQwY2YzOTUwYmIiLCJjb250ZW50VHlwZSI6InRleHRcL2h0bWwiLCJjaGFyc2V0IjoidXRmLTgiLCJ6aXAiOjF9/07826342b83ea870f846cfa48f1b0eb8d3d51b78ceb1b05b1e014467d7241904/life-on-the-command-line.html)", "zotero_item": null}}, {"model": "library.reading", "pk": 757, "fields": {"title": "Stephen Ramsay, Programming with Humanists: Reflections on Raising an Army of Hacker-Scholars in the Digital Humanities", "url": "https://www.openbookpublishers.com/htmlreader/DHP/chap09.html", "annotation": "[Stephen Ramsay, Programming with Humanists: Reflections on Raising an Army of Hacker-Scholars in the Digital Humanities](https://www.openbookpublishers.com/htmlreader/DHP/chap09.html)", "zotero_item": null}}, {"model": "workshop.workshop", "pk": 162, "fields": {"name": "Project Lab", "slug": "project-lab", "created": "2020-07-09T19:01:26.000Z", "updated": "2020-07-09T19:01:26.000Z", "parent_backend": "Github", "parent_repo": "DHRI-Curriculum/project-lab", "parent_branch": "v2.0rhody-edits"}}, {"model": "frontmatter.frontmatter", "pk": 154, "fields": {"workshop": 162, "abstract": "The Project Lab is designed to help you develop a  **reasonable, informed, and purposeful plan** for your digital project that can be used with modifications for multiple purposes: to propose a seminar, capstone, or dissertation project; to submit a grant proposal or funding request; to communicate clearly with potential partners or collaborators; to guide project management; or to build out a publication about your work.", "ethical_considerations": "['Keralis, Spencer D. C., and Pamela Andrews. [\u201cLabor.\u201d](https://digitalpedagogy.mla.hcommons.org/keywords/labor/) Digital Pedagogy in the Humanities: Concepts Models, and Experiments, edited by Rebecca Frost Davis et al., Modern Language Association. digitalpedagogy.mla.hcommons.org, https://digitalpedagogy.mla.hcommons.org/keywords/labor/. Accessed 16 June 2020.', 'Ess, Charles, and Association of Internet Researchers. [Ethical Decision-Making and Internet Research.](http://aoir.org/reports/ethics.pdf) 27 Nov. 2002, http://aoir.org/reports/ethics.pdf.', 'Hoffmann, Anna Lauren. [\u201cData Violence and How Bad Engineering Choices Can Damage Society.\u201d](https://medium.com/s/story/data-violence-and-how-bad-engineering-choices-can-damage-society-39e44150e1d4) Medium, https://medium.com/s/story/data-violence-and-how-bad-engineering-choices-can-damage-society-39e44150e1d4. Accessed 23 May 2018.', 'Losh, Elizabeth. [\u201cHacktivism and the Humanities: Programming Protest in the Era of the Digital University](https://dhdebates.gc.cuny.edu/read/untitled-88c11800-9446-469b-a3be-3fdb36bfbd1e/section/f6fe2a59-8937-4446-a2fb-86dc6ba1975b#ch10) Debates in the Digital Humanities, edited by Matthew K. Gold and Lauren Klein, University of Minnesota Press, 2012. dhdebates.gc.cuny.edu, https://dhdebates.gc.cuny.edu/read/untitled-88c11800-9446-469b-a3be-3fdb36bfbd1e/section/f6fe2a59-8937-4446-a2fb-86dc6ba1975b#ch10.']", "estimated_time": "3", "projects": [380, 381], "resources": [], "readings": [758, 759], "contributors": [485, 486, 487, 488], "prerequisites": []}}, {"model": "praxis.praxis", "pk": 140, "fields": {"discussion_questions": "['Now that you have a project proposal and draft toward a plan, what areas do you need to further develop to make the plan viable? ', 'What models of collaboration are you considering for your project? What challenges do you think your project may confront in working with other scholars? ', 'How does your project proposal recognize the contributions of various types of intellectual, techical, and emotional labor?']", "next_steps": "['building a project as part of a class;', 'writing a proposal for NEH funding; ', 'applying for faculty develompent or institutional funding to develop a pilot or early version of your project idea.']", "workshop": 162, "further_readings": [760, 761, 762, 763], "more_projects": [], "more_resources": [], "tutorials": [370, 371]}}, {"model": "lesson.lesson", "pk": 1128, "fields": {"title": "Begin by Thinking About the End", "created": "2020-07-09T19:01:26.010Z", "updated": "2020-07-09T19:01:26.010Z", "workshop": 162, "text": "<h2>What does \"done\" look like?</h2>\n<p>Most digital projects come to an end at some point, in one way or another. We either simply stop working on them, or we forget about them, or we move on to something else. Few digital projects have an end \"form\" in the way that we think of a monograph. We rarely think of digital scholarship in its \"done\" form, but sooner or later even if they're not \"finished\"\u2014so to speak\u2014at some point, these projects end. </p><p>\nDone can take many different shapes: \n* it can morph into something new;\n* it can be retired;\n* it can be archived in a repository;\n* it can be saved on some form of storage media;\n* it can run out of funding; \n* and sometimes <strong>you</strong> are done with it!</p><p>\nSo it's helpful to think about what you want \"done\" to look like before you begin, because then you always have a sense of what will make a satisfactory ending to the work you're about to embark on. </p>\n<h2>Activity</h2>\n<ul>\n<li>Imagine what your project is like when it's over. Imagine what it means to you to set it down and walk away. What will you do with it? How will you know if it succeded? Who was the last person to care about it? Why? </li>\n<li>Now, Describe your 2-3 sentences to a non-expert audience the purpose of your project. </li>\n<li>Does it solve a problem? </li>\n<li>Meet an institutional need? </li>\n<li>Put an existing resource to new use?</p><p>\n # Identifying Audiences, Constituencies, and Collaborators</li>\n<li>Who will participate in, use, and/or benefit from the project? </li>\n<li>Is there a specific group already asking for this new resource? Who? </li>\n<li>The \"general public\" is too general an audience. the more specific your audience is the more likely you are to meet their needs. </li>\n<li>Is there anything that my audience can bring to my project?</li>\n</ul>\n<h2>Your project <strong>always</strong> has an audience.</h2>\n<ul>\n<li><em>You</em> are an audience. </li>\n<li>Your dissertation advisor</li>\n<li>Your dissertation committee</li>\n<li>Researchers interested in your subject area</li>\n<li>Researchers working on related questions</p><p>\n<em>Projects typically satisfy more than one audience's need. The key to identifying a well-defined audience is research and creating several narrow profiles</em></li>\n</ul>\n<h3>Sample 1:</h3>\n<ol>\n<li>Needs/Interest: Faculty who teach undergraduate linguistics classes are looking for an engaging way to teach fundamental linguistics concepts through guided practice and repetition. </li>\n<li>Resources: They have access to chrome books, laptops, and tablets in the classroom with limited wifi. They are unable to update software frequently, but they can use web applications. </li>\n<li>Limitations: This audience needs clear and specific documentation and has a low threshold for errors. </li>\n</ol>\n<h3>Sample 2:</h3>\n<ol>\n<li>Needs/Interest: My dissertation committee is interested in hearing how my model of financial data offers a new approach to predicting corporate fraud. </li>\n<li>Resources/Relationship: The committee is familiar with some of the models that already exist. They have deep familiarity with the qualitative indicators for corporate fraud. They can connect my data and my research to a wider community of scholars who have the datasets that I need to finish my work. My advisor wants me to succeed. </li>\n<li>Limitations: My committee is skeptical about technology and feels uncertain about how they will evaluate my work. They have expressed concern about not knowing how to \"check\" if the model is accurate. </p><p>\nIf you are working on a project that is institutionally based (such as creating a platform, creating a resource, or building a teaching tool), you may have institutaional partners who have a stake in your project's success.  It's a good idea to identify these folks and consider their interests and needs as well. </p><p>\nPossible stakeholders include: your library, colleagues, IT division, academic program, a center, or institute who shares your mission and/or goals. </p><p>\nExample of a \"stakeholder\":</li>\n</ol>\n<h3>Sample:</h3>\n<ol>\n<li>Needs: The New Media Lab has student fellows who develop digital projects. They have expressed interest in finding a data management tool that allows students to create a plan for how to handle various types of data files. </li>\n<li>Resources/Relationships: I'm also a fellow in the New Media Lab, and they might be willing to help test my tool. </li>\n<li>Limitations/Concessions: They are more interested in using open source software than many of the other students I work with. </li>\n</ol>\n<h2>Activity: Your Turn</h2>\n<h3>Audience 1:</h3>\n<ol>\n<li>Needs/Interest: </li>\n<li>Resources/Relationship: </li>\n<li>Limitations/Concessions: </li>\n</ol>\n<h3>Audience 2:</h3>\n<ol>\n<li>Needs/Interest: </li>\n<li>Resources/Relationship: </li>\n<li>Limitations/Concessions: </li>\n</ol>\n<h3>Audience 3:</h3>\n<ol>\n<li>Needs/Interest: </li>\n<li>Resources/Relationship: </li>\n<li>Limitations/Concessions: </li>\n</ol>\n<h3>Audience 4:</h3>\n<ol>\n<li>Needs/Interest: </li>\n<li>Resources/Relationship: </li>\n<li>Limitations/Concessions: </li>\n</ol>\n<h3>Internal Stakeholder:</h3>\n<ol>\n<li>Needs/Interest: </li>\n<li>Resources/Relationship: </li>\n<li>Limitations/Concessions:</li>\n</ol>", "order": 1}}, {"model": "lesson.lesson", "pk": 1129, "fields": {"title": "Environmental Scan (Literature and Technology Review)", "created": "2020-07-09T19:01:26.016Z", "updated": "2020-07-09T19:01:26.016Z", "workshop": 162, "text": "<p>Conducting an in-depth environmental scan and literature review early in the planning process is a critical step to see if there are existing projects that are similar to your own or that may accomplish similar goals to your potential project. Sometimes, the planning process stops after the scan because you find that someone has already done it! Typically, a scan is useful in articulating and justifying the \"need\" for your research OR to justify your choice of one technology in lieu of others. Performing an environmental scan early and reviewing and revising it periodically will go a long way to help you prove that your project fills a current need for an actual audience. </p><p>\nSuccessful project proposals demonstrate knowledge of the ecosystem of existing projects in your field, and the field's response to those projects. Scans often help organizations identify potential collaborators, national intitiatives, publications, articles, or professional organizations, which in turn can demonstraate a wider exigency for your project. Following a preliminary scan, you should be able to explain why your project is important to the field, what it provides that does not currently exist, and how your project can serve as a leader or example to other organizations in such a way that they can put your findings to new issue. </p><p>\nBelow are <strong>suggestions</strong> for finding similar projects and initiatives in and outside of your field: </p>\n<h2>Federal grant agencies maintain repositories with white papers from previously funded grant projects:</h2>\n<ul>\n<li>Institute of Museum and Library Services</li>\n<li>National Endowment for the Humanities Funded Projects Query Form</li>\n<li>National Endowment for the Arts (NEA) Recent Grants </li>\n<li>National Science Foundation</li>\n<li>National Institutes of Health</li>\n</ul>\n<h2>Search and browse through literature in the field and resources for digital tools and innovations. Some examples of places to look include:</h2>\n<ul>\n<li>IMLS UpNext https://www.imls.gov/news-events/upnext-blog</li>\n<li>D-Lib Magazine http://www.dlib.org/</li>\n<li>The Signal: Digital Preservation http://blogs.loc.gov/digitalpreservation/</li>\n<li>Curator Journal http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)2151-6952</li>\n<li>American Archivist http://www2.archivists.org/american-archivist#.V1kWCZMrLGI</li>\n<li>Informal Science http://www.informalscience.org/</li>\n<li>Center for the Future of Museums http://www.aam-us.org/resources/center-for-the-future-of-museums</li>\n<li>OCLC blogs http://www.oclc.org/blog/main/</li>\n<li>DiRT registry of digital humanities tools http://dirtdirectory.org/</li>\n<li>Digital Humanities Now http://digitalhumanitiesnow.org/</li>\n<li>SSRC's Items http://items.ssrc.org/</li>\n<li>Ant, Spider, Bee http://www.antspiderbee.net/</li>\n<li>PLOS http://blogs.plos.org/ &amp; http://blogs.plos.org/collections/</li>\n<li>The Winnower https://thewinnower.com/topics</li>\n<li>HubZero https://hubzero.org/groups/browse</li>\n<li>AAAS Trellis https://www.trelliscience.com/#/site-home</li>\n</ul>\n<h3>Other places to check:</h3>\n<ul>\n<li>Search preprint repositories, academic repositories, and data warehouses for similar datasets</li>\n<li>Check conference programs and gray literature from your field and related materials. </li>\n<li>Discuss your project idea with your colleagues inside and outside of your own department at your institution, at conferences, and even with peers in different fields.</li>\n</ul>", "order": 2}}, {"model": "lesson.lesson", "pk": 1130, "fields": {"title": "Activity", "created": "2020-07-09T19:01:26.019Z", "updated": "2020-07-09T19:01:26.019Z", "workshop": 162, "text": "<p>The key to the environmental scan is to see what a wider community is already up to. How does your project fit into the ongoing work of others in your field? What about in a related field that addresses a similar question from another perspective? Is someone already working on a similar question? </p><p>\n1. Brainstorm where you might go to look for digital projects in your field that use emerging or new forms of technology. Try to list 3 places you might look to see how others in your field are adapting their methods to use new digital tools. </p><p>\n2. What technologies/methods do most people use in your field, if any, for capturing, storing, exploring/analyzing, or displaying their data? Why do they tend to use it? Is there a reason why you want to use the same technologies as your colleagues? What are the benefits of doing things differently? </p><p>\n3. Does your project fill a need or stake new methodological ground? How do you know? </p><p>\n4. If there aren't any technologies that do <em>exactly</em> what you were hoping for, has anyone else run into this problem? How did they solve it? Will you need to create a new tools or make significant changes to an existing one to accomplish your goal? </p><p>\n5. Once you have gathered information about what is \"out there,\" what are the limits of what you are willing to change about your own project in response? How will you know if you have stretched beyond the core objectives of your own research project?</p>", "order": 3}}, {"model": "lesson.lesson", "pk": 1131, "fields": {"title": "Resource Assessment", "created": "2020-07-09T19:01:26.028Z", "updated": "2020-07-09T19:01:26.028Z", "workshop": 162, "text": "<p>The next step in our process is figuring out what resources you have available to you and what you still need in order to accomplish your project's objectives. </p>\n<h2>Types of Resources</h2>\n<ol>\n<li>data</li>\n<li>technology</li>\n<li>human</li>\n<li>institution</li>\n<li>financial</li>\n</ol>\n<h2>Data, Digital Assets, Collections</h2>\n<p>Do you have the dataset you need to do your project? Finding, cleaning, storing, managing changes in, and sharing your data is an often overlooked but <em>extremely</em> important part of designing your project. Successfully finding a good dataset means that you should keep in mind: Is the dataset the appropriate size and complexity to help address your project's goals? Finding, using, or creating a good dataset is a core part of your project's long-term success. </p>\n<h2>Activity:</h2>\n<p><strong> What data resources do you have at your disposal? What do you still need? What steps do you need to take during the course of your project in order to work with the dataset now that you have a general sense of what the data needs to look like if you are working with either textual or numeric data? </strong></p>\n<h3>No?</h3>\n<ul>\n<li>If not, do you know where to go to find it? </li>\n<li>Is it digitized? </li>\n<li>Do you need to create it yourself?</li>\n<li>Is it under copyright? </li>\n<li>Is it free to your institution? over the web? </li>\n<li>Who could you talk to about finding, accessing, or digitizing what you need?</li>\n</ul>\n<h3>Yes?</h3>\n<ul>\n<li>What format does your data need to be in so that you can begin working with it?</li>\n<li>Is the data in a format that you can use? </li>\n<li>If not, how will you get it into that format? How long will it take? Are you unsure? </li>\n<li>What is the biggest challenge that your dataset presents? </li>\n<li>What should go well? </li>\n<li>What makes your dataset interesting? </li>\n<li>Do you plan to make your dataset open? If yes, how will you do that? (GitHub?)</li>\n<li>If you do not plan to make your dataset open, where will you store it? Will you make it available to fellow researchers upon request? How will you communicate that? </li>\n<li>If you do not know what format your data needs to be in, whom will you ask for more information? </li>\n<li>Will your efforts and cleaning and preparing data be useful to anyone else? Would you be willing to share your methods? How would you do so? </li>\n<li>Will your data be standardized so that it can be combined with other datasets? What standards will you use? </li>\n<li>How will you fill gaps? </li>\n<li>How long will it take for you to be able to answer all of these questions? (You are unlikely to be able to do it all today.)</li>\n</ul>\n<h2>Technology</h2>\n<ul>\n<li>Name all the types of technology that you will need to go from \"raw data\" to \"final project\"? If you don't know the name of the technology, you can just describe it. (<strong>Example</strong>: First I will scrape texts from poetry websites like poets.org from the internet using a python library called Beautiful Soup. Next, I will clean my data using Python, explore the data in NLTK to look for coocurrances of the words \"painting\" and \"sea.\" I will store my results using GitHub, and visualize the results using the D3.js libraries. I will use these visualizations to write the second half of my article. When the project is done, I will deposit the dataset into the Academic Works repository.\" )</li>\n<li>Do you need a server or other cloud computing environment? </li>\n<li>Do you have someone who can work on the public-facing presence of the project (design skills)?</li>\n<li>Where will you host your project?</li>\n<li>How much time do you think will need to be dedicated to tech support for the project? </li>\n<li>Do you need mobile devices? 3D printers? other hardware or software? </li>\n<li>Will you choose open source platforms or proprietary ones? </li>\n</ul>\n<h4>Example:</h4>\n<p>Have: basic knowledge of git and python and some nltk</p><p>\nNeed: I need a more powerful computer, to learn how to install and use Beautiful Soup, and to get help cleaning the data. I will also need to learn about the D3.js library. </p>\n<h2>Human resources</h2>\n<p>Looking back at the Audiences worksheet, review which of your audiences were invested in your work. Who can you draw on for support? Consider the various roles that might be necessary for the project. Who will fill those roles? \n* design\n* maintenance and support\n* coding/programming\n* outreach / documentation\n* project management</p>\n<h3>Locally:</h3>\n<ul>\n<li>Have you met anyone at this week's institute who is working on a similar kind of project that uses similar methods? </li>\n<li>Are there other colleagues in your program who are interested in using similar technologies or methods? </li>\n<li>Is there a digital scholarship librarian at your college? Have you signed up for the GC Digital Fellows email list? </li>\n<li>Who is going to manage the work of the project? Is it you? What if the project grows? </li>\n<li>Do you need to bring someone on board who has a more extensive digital skill set? Other content knowledge? Describe what that person would do on the project. </li>\n</ul>\n<h3>Remote:</h3>\n<ul>\n<li>Is there an online research community that you could connect to such as an online forum? blog? research center? </li>\n<li>If you have presented at a conference or are part of a scholarly society or other group, do they have a listserv with people who are interested in the same technologies or research questions? </li>\n<li>Do people in your field use Twitter or another social network platform to communicate? Could you create a hashtag for people who share similar research interests and/or technology needs? </li>\n</ul>\n<h2>Institution</h2>\n<ul>\n<li>What resources are available to you through your institution? </li>\n<li>What services or support might be available through the GC Digital Initiatives, the Teaching &amp; Learning Center, the New Media Lab, or the Futures Initiative? If you are not at the GC, does your insitution have a digital research support network? </li>\n<li>Have you joined the CUNY Academic Commons? </li>\n<li>Have you applied for internal funding? Where would you look? </li>\n<li>Are there resources at your institution for hosting, data sharing, and/or preservation?</li>\n</ul>", "order": 4}}, {"model": "lesson.lesson", "pk": 1132, "fields": {"title": "Outreach", "created": "2020-07-09T19:01:26.032Z", "updated": "2020-07-09T19:01:26.032Z", "workshop": 162, "text": "<p>Outreach can take many different forms, from presenting your research at conferences and through peer-reviewed scholarly publications, but also through blog posts, Twitter conversations, forums, and/or press releases. The key to a good outreach plan is to being earlier than you think is necessary, and give your work a public presence (via Tumblr, Twitter, website, etc). You can use your outreach contacts to ask for feedback and input as well as share challenges and difficult decisions. \n* Will you create a website for your project? \n* How will you share your work? \n* Will you publish in a traditional paper or in a less-traditional format? \n* Whom will you reach out to get the word out about your work? \n* Is there someone at your college who can help you to publicize your accomplishments? \n* Will you have a logo? Twitter account? Tumblr page? Why or why not? \n* Can you draw on  your colleagues to help get the word out about your work? \n* What information could you share about your project at its earliest stages? \n* Does your project have a title? </p>\n<h2>Activity:</h2>\n<ol>\n<li>Is there a project that you know of that seems to have garnered lots of attention either from a broader public or from your own field? What made that project stand out? How did they share their work? </li>\n<li>Consider the early, middle, and final stages of your project. What kinds of outreach activities could you fit in at each stage? What audiences would you try to reach? Would they change over time?</li>\n</ol>", "order": 5}}, {"model": "lesson.lesson", "pk": 1133, "fields": {"title": "Sustainability and Data Management", "created": "2020-07-09T19:01:26.038Z", "updated": "2020-07-09T19:01:26.038Z", "workshop": 162, "text": "<p>You will need to come up with a plan for how you are going to manage the \"data\" created by your project. Data management plans, now required by most funders, will ask for you to list all the types of data and metadata created over the duration of the project and then account for the various manners by which you will account for various versions, make the datasets available openly (if possible) and share your data. </p><p>\nSustainability plans require detailing what format files will be in and accounting for how those files and your data will continue to be accessible to you and/or to your audience or a general public long after the project's completion. </p><p>\nLibrarians are your allies in developing a sound data management and sustainability plan. </p>\n<h2>Activity</h2>\n<p>Very quickly, try to think of all the different types of data your project will involve. \n* Where will you store your data? \n* Is your software open source? \n* What is the likelihood that your files will remain usable? \n* How will you keep track of your data files? \n* Where will the data live after the project is over?</p>", "order": 6}}, {"model": "lesson.lesson", "pk": 1134, "fields": {"title": "Effective Partnerships", "created": "2020-07-09T19:01:26.043Z", "updated": "2020-07-09T19:01:26.043Z", "workshop": 162, "text": "<p>After brainstorming your project ideas and assessing your available resources, it is time to scope out potential partners to help fill in gaps and formalize relationships. </p><p>\nplease keep in mind that each project is different. This outline offers suggestions and lessons learned from successful and less successful collaborations. while each project is unique in the way responsibilities are shared, perhaps one universal attribute of successful partnerships is mutual respect. The most successful collaborations are characterized by a demonstrated respect for each partners's time, work, space, staff, or policies in words and actions. </p>\n<h2>Identify what you need:</h2>\n<p>Once you know where you need help, start thinking about who you know who might have those skills, areas of expertise, resources, and interest. \n* Partnerships should be selected on the basis of specific strengths. \n* If you don't know someone who fits the bill, can someone you know introduce you to someone you would like to know? What are some ways of finding someone with skills you don't have if you don't know anyone with those skills? </p>\n<h2>Find Collaborators:</h2>\n<ul>\n<li>Attending conferences and unconferences can be the best way to meet potential collaborators who share similar goals and passions. Informal gatherings are often the best place to chat with folks: \u201cBirds of a Feather\u201d dinners, or affinity group luncheons.</li>\n<li>Talk to a grant program office about your project; they may have some great recommendations.</li>\n<li>Circulate some ideas on your professional social networks to scope out potential partners.</li>\n</ul>\n<h2>Identify a good fit:</h2>\n<ul>\n<li>Talk with a potential collaborator. Introduce yourself by email and schedule a phone call. It\u2019s very important to speak or meet face-to-face with potential collaborators before formalizing partnerships.</li>\n<li>Good partners share in the project\u2019s vision and are committed to the project\u2019s success.</li>\n<li>Good partners respect one another and appreciate what each one brings to the project.</li>\n</ul>\n<h2>Formalize partnerships:</h2>\n<ul>\n<li>Clearly state expectations of work in a written document or contract.</li>\n<li>Make sure each partner understands exactly what their contributions will be, when those contributions are due, and who else is responsible for other pieces.</li>\n<li>Be sure both/all parties are in agreement on issues such as:<ul>\n<li>who takes notes during meetings,</li>\n<li>who manages the budget,</li>\n<li>who is the \u201cdecider\u201d on major project decisions.</li>\n</ul>\n</li>\n<li>Determine who the primary contact for inter-institutional communications will be.</li>\n<li>Designate staff titles and responsibilities, including a description of job responsibilities over the life-cycle of the project.</li>\n</ul>\n<h2>Communicate Effectively:</h2>\n<ul>\n<li>Early on, establish communication norms: including regular meeting times, means for meeting (conference calls, Skype, Hangouts, et al), and best ways to communicating in between meetings (ie, email only), and collect all preferred contact information and publish it somewhere accessible to the entire team.</li>\n<li>Use project management software for organizing project tasks, deadline, deliverable requirements that makes all of this information easily accessible and visible to project collaborators.</li>\n<li>If working with geographically-dispersed collaborators, be sure to schedule face-to-face meetings at a reasonable, yet, regular interval.</li>\n<li>Generally, treat all project team members with respect and engage in common courtesies.</li>\n</ul>\n<h2>Stay flexible:</h2>\n<ul>\n<li>No project is able to anticipate all problems or challenges before they occur, but simply acknowledging that challenges may arise, and allowing time and budget for those challenges is helpful.</li>\n</ul>\n<h2>Bad Marriages:</h2>\n<ul>\n<li>Not all collaborations work out as planned, even with the best of intentions.</li>\n<li>If there is a major breakdown in communications, or if relationships deteriorate, you may need to break apart partnerships.</li>\n</ul>\n<h2>Asking for letters of support</h2>\n<p>When preparing a proposal, you will need mentors, collaborators, or other interested parties to write a strong letter of support for your project that will help your proposal stand out to the reviewers. Some funders want letters from all project participants. </p><p>\nIt is important to respect people\u2019s time when asking them for a letter by showing that you\u2019ve done your research and that you have some grant materials to share with them. Good letters demonstrate some knowledge of the project and recognition of its impact if funded. </p><p>\nFollow these steps when asking for a support letter and for specific types of assistance during the life of the grant, and you should receive a good letter in return.\n* One month before grant deadline, begin brainstorming candidates for letters of support and note which collaborators are required to submit letters of commitment and support. \n* Start asking supporters at least two weeks in advance of grant deadline, because they will also have deadlines and other work competing for their work hours. You may find some folks are on leave at the time you inquire, be sure to have back-ups on your list. \n* Email potential supporters, collaborators:</p><p>\n    * State why, specifically, you are asking Person A for support;</p><p>\n    * Be specific about what you are asking Person A to do over the scope of the grant, if anything, such as participate in 3 meetings, 2 phone calls over 18 months; or  agree to review the project and provide feedback one month before official launch; </p><p>\n    * Provide any information about compensation, especially when asking someone to participate (ie, there will be a modest honorarium to recognize the time you give to this project of $xxx);</p><p>\n    8 Tell supporters what exactly you need to complete the grant application, in what format, and by what date (ie, a 2-page CV in PDF and letter of support on letterhead by next Friday).\n* Attach materials that will be helpful for them when writing the letter.</p><p>\n    * Provide a short project summary that includes the project goals, deliverables, and work plan from the grant proposal draft;</p><p>\n    * Include a starter letter containing sample text that references that person\u2019s or institution\u2019s role and why they are supporting the project.</p>", "order": 7}}, {"model": "lesson.lesson", "pk": 1135, "fields": {"title": "Finding Funding", "created": "2020-07-09T19:01:26.047Z", "updated": "2020-07-09T19:01:26.047Z", "workshop": 162, "text": "<p>Now that you have started to form:\n* a more refined project idea;\n* a wider awareness of the ecosystem of existing projects in your field;\n* a sense of the national, local, or institutional demand for your project;\n* and a clearer sense of the resources at your disposal</p><p>\n... the next step is to find an appropriate funding source. Below you will find some suggestions as to where to begin the search for funding. As you look for possible funders, below are some guidelines for the process:</p><p>\n1. Check federal, state, and local grant-making agencies, and local foundations for possibility of grants.</p><p>\n    * Federal agencies list all of their available grants on http://grants.gov.</p><p>\n    * States also have opportunities for grants, such as state humanities councils. </p><p>\n    * Private foundations are also possible areas to look. The following may prove useful: </p><p>\n        * The Foundation Center: [http://foundationcenter.org] (http://foundationcenter.org)</p><p>\n        * A Directory of State and Local Foundations: </p><p>\n        [http://foundationcenter.org/getstarted/topical/sl_dir.html] (http://foundationcenter.org/getstarted/topical/sl_dir.html)</p><p>\n        * The Council on Foundations Community Foundations List</p><p>\n     http://www.conf.org/whoweserve/community/resources/index.cfm?navitemNumber=15626#locator</p><p>\n     * The USDA offers a valuable Guide to Funding Resources [https://www.nal.usda.gov/ric/guide-to-funding-resources] (https://www.nal.usda.gov/ric/guide-to-funding-resources)</p><p>\n2. Check your institution\u2019s eligibility for a potential grants before beginning the application process. Eligibility requirements and restrictions are often found in grant guidelines.</p><p>\n3. Review the types of projects this program funds, and consider how your project fits with the agency or foundation\u2019s mission and strategic goals.</p><p>\n4. Review a potential grant program\u2019s deadlines and requirements (including proposal requirements and format for submission).</p><p>\n5. Identify funding levels/maxes, and keep them close at hand as you develop your budget.</p><p>\n6. Jenny Furlong Director of the Office of Career Services will be here tomorrow, and she is an excellent resource for those interested in external fellowships. </p>\n<h2>Activity</h2>\n<p>Find one or two grant opportunites in your subject area. Consider also looking for fellowship opportunities. </p>\n<h2>Short Project Proposal (2-3 pages)</h2>\n<p>What follows is a template for writing a short project proposal that, once developed, will position you to move forward with building partnerships with other institutions or for pursuing funding opportunities. Though this template does not directly reflect a specific grant narrative format, the short project proposal includes important project-development steps that can later form the basis for a wide variety of grant narratives.</p>\n<h2>Project Details</h2>\n<ul>\n<li>Title:</li>\n<li>Lead Applicant/PI:</li>\n<li>Funding Opportunity:</li>\n<li>Proposal Deadline</li>\n</ul>\n<h2>Abstract</h2>\n<p>150 word summary of project: (1 short paragraph)</p>\n<h2>The Need</h2>\n<p>Statement of the conditions that make the project necessary and beneficial for your key audiences (2-3 paragraphs).</p>\n<h2>Impact and Intended Results</h2>\n<p>A brief explanation that combines your environmental scan and your research goals. Why is what you are doing necessary and different in your field\u2014and maybe to more than just scholars in your field. (4-5 paragraphs)</p>\n<h2>The Plan</h2>\n<p>Rough outline and project calendar that includes project design and evaluation, and possibly a communications plan, depending on the grant with major deliverables (bullet-pointed list of phases and duration):\n* Phase 1 (month/year - month/year):\n* Phase 2 (month/year - month/year):\n* Phase 3 (month/year - month/year):</p>\n<h2>Project Resources: Personnel and Management</h2>\n<p>Description of the why the cooperating institutions and key personnel are well-suited to undertake this work (list of experience and responsibilities of each staff member, and institutional description).</p>\n<h2>Sustainability</h2>\n<p>If applicable, describe how this project will live beyond the grant period. Will it continue to be accessible? How so? A data management plan might need to be specified here.</p>", "order": 8}}, {"model": "lesson.lesson", "pk": 1136, "fields": {"title": "Presentation Template", "created": "2020-07-09T19:01:26.050Z", "updated": "2020-07-09T19:01:26.050Z", "workshop": 162, "text": "<p><strong>Name:</strong> </p><p>\n<strong>Program:</strong> </p><p>\n<strong>Project title:</strong></p><p>\n<strong>2 Sentence abstract:</strong></p><p>\n<strong>What resources do you have now?</strong></p><p>\n<strong>What have you learned this week that will help you?</strong></p><p>\n<strong>What additional support will you need as you take your next steps?</strong></p>", "order": 9}}, {"model": "lesson.lesson", "pk": 1137, "fields": {"title": "Presentation", "created": "2020-07-09T19:01:26.058Z", "updated": "2020-07-09T19:01:26.058Z", "workshop": 162, "text": "<h2><strong>Name:</strong> Lisa Rhody</h2>\n<h2><strong>Program:</strong> Digital Initiatives</h2>\n<h3><strong>Project title:</strong> Projects made Easy!</h3>\n<p><strong>2 Sentence abstract</strong></p><p>\nMy project is going to make every installation seamless. It will make all of your Python dreams come true, your databases tidy, and your Git Hub happy. </p><p>\n<strong>What resources do you have now?</strong></p><p>\n<strong>What have you learned this week that will help you?</strong></p><p>\n<strong>What additional support will you need as you take your next steps?</strong></p>\n<h2>Activity</h2>\n<ul>\n<li>Delete my information above and use this as a template for your own presentation. </li>\n<li>You may want to consult this <a href=\"https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#lists\">Markdown Cheat Sheet by Adam Pritchard</a> to help you format your slide better. </li>\n</ul>\n<h2>When you are done:</h2>\n<ul>\n<li>save this file with a new name (\"yourlastname.md\")</li>\n<li>add this file to your repository </li>\n</ul>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">git</span> <span class=\"k\">add</span> <span class=\"n\">yourlastname</span><span class=\"p\">.</span><span class=\"n\">md</span>\n</code></pre></div>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">git</span> <span class=\"k\">commit</span> <span class=\"o\">-</span><span class=\"n\">m</span> <span class=\"ss\">\"my presentation file\"</span>\n</code></pre></div>\n<ul>\n<li>If you have used an image (and you can) don't forget to add your image to the image folder and then add it to your repo: </li>\n</ul>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">git</span> <span class=\"k\">add</span> <span class=\"n\">images</span><span class=\"o\">/</span><span class=\"n\">myfile</span><span class=\"p\">.</span><span class=\"n\">jpg</span>\n</code></pre></div>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">git</span> <span class=\"k\">commit</span> <span class=\"o\">-</span><span class=\"n\">m</span> <span class=\"ss\">\"adding an image file\"</span>\n</code></pre></div>", "order": 10}}, {"model": "frontmatter.learningobjective", "pk": 954, "fields": {"frontmatter": 154, "label": "To identify the purpose of and need for your project"}}, {"model": "frontmatter.learningobjective", "pk": 955, "fields": {"frontmatter": 154, "label": "To place your project within the context of other existing work"}}, {"model": "frontmatter.learningobjective", "pk": 956, "fields": {"frontmatter": 154, "label": "To account for the resources already at your disposal and those you may need"}}, {"model": "frontmatter.learningobjective", "pk": 957, "fields": {"frontmatter": 154, "label": "To locate partners, collaborators, and audiences"}}, {"model": "frontmatter.learningobjective", "pk": 958, "fields": {"frontmatter": 154, "label": "To develop a work plan "}}, {"model": "frontmatter.learningobjective", "pk": 959, "fields": {"frontmatter": 154, "label": "To communicate your plan"}}, {"model": "frontmatter.learningobjective", "pk": 960, "fields": {"frontmatter": 154, "label": "To evaluate your progress"}}, {"model": "frontmatter.learningobjective", "pk": 961, "fields": {"frontmatter": 154, "label": "To describe how the work and data will be managed and sustained"}}, {"model": "frontmatter.learningobjective", "pk": 962, "fields": {"frontmatter": 154, "label": "To draft an initial project proposal"}}, {"model": "frontmatter.contributor", "pk": 485, "fields": {"first_name": "Lisa Marie", "last_name": "Rhody", "role": "Current author", "url": null}}, {"model": "frontmatter.contributor", "pk": 486, "fields": {"first_name": "Lisa Marie", "last_name": "Rhody", "role": "Original author", "url": null}}, {"model": "frontmatter.contributor", "pk": 487, "fields": {"first_name": "none", "last_name": "", "role": "Contributing authors", "url": null}}, {"model": "frontmatter.contributor", "pk": 488, "fields": {"first_name": "", "last_name": "", "role": "Reviewers", "url": null}}, {"model": "library.reading", "pk": 758, "fields": {"title": "\u201cEdition, Project, Database, Archive, Thematic Research Collection: What\u2019s in a Name?\u201d", "url": "<http://www.digitalhumanities.org/dhq/vol/3/3/000053/000053.html>", "annotation": "Price, Kenneth M. [\u201cEdition, Project, Database, Archive, Thematic Research Collection: What\u2019s in a Name?\u201d](<http://www.digitalhumanities.org/dhq/vol/3/3/000053/000053.html>) Digital Humanities Quarterly, vol. 003, no. 3, Sept. 2009.", "zotero_item": null}}, {"model": "library.reading", "pk": 759, "fields": {"title": "\u201cGetting Started in Digital Humanities Journal of Digital Humanities.\u201d", "url": "http://journalofdigitalhumanities.org/1-1/getting-started-in-digital-humanities-by-lisa-spiro", "annotation": "Spiro, Lisa. [\u201cGetting Started in Digital Humanities Journal of Digital Humanities.\u201d](http://journalofdigitalhumanities.org/1-1/getting-started-in-digital-humanities-by-lisa-spiro/) Journal of Digital Humanities, vol. 1, no. 1, Winter 2011, http://journalofdigitalhumanities.org/1-1/getting-started-in-digital-humanities-by-lisa-spiro/.", "zotero_item": null}}, {"model": "library.project", "pk": 380, "fields": {"title": "Provost's Digital Innovation Grants site", "url": "http://cuny.is/digitalgrants", "annotation": "The Graduate Center's [Provost's Digital Innovation Grants site](http://cuny.is/digitalgrants) features  proposal narratives from successfully funded projects by doctoral students. Look under \"Funded Projects\" for a list of digital project descriptions, narratives, and white papers for each academic year. ", "zotero_item": null}}, {"model": "library.project", "pk": 381, "fields": {"title": "Digital Humanities Advancement Grant Program", "url": "https://www.neh.gov/grants/odh/digital-humanities-advancement-grants", "annotation": "The NEH Office of Digital Humanities posts sample application narratives with its call for applications. You can find these at the bottom of the grant program announcements, for example, on the [Digital Humanities Advancement Grant Program](https://www.neh.gov/grants/odh/digital-humanities-advancement-grants) page.", "zotero_item": null}}, {"model": "library.tutorial", "pk": 370, "fields": {"label": "DevDH.org \u2013 Development for the Digital Humanities", "url": "http://devdh.org", "annotation": "Appleford, Simon, and Jennifer Guiliano. DevDH.org, 2013. http://devdh.org.\n  In this online resource guide, Guiliano and Appleford share recorded lectures as podcasts, slides, and handouts to help scholars develop strong projects. Topics include \"Designing Your First Project,\" \"Building Budgets,\" \"Manage Your Project,\" and more. ", "zotero_item": null}}, {"model": "library.tutorial", "pk": 371, "fields": {"label": "PM4DH: Project Management for the Digital Humanities", "url": "https://scholarblogs.emory.edu/pm4dh", "annotation": "[PM4DH: Project Management for the Digital Humanities](https://scholarblogs.emory.edu/pm4dh/), Emory Center for Digital Scholarship, Accessed 13 June 2020. https://scholarblogs.emory.edu/pm4dh/\n    The staff of the Emory Center for Digital Scholarship has produced a helpful website that includes information on project proposals accompanied with information about project management. Considering how you might manage all aspects of the project's lifecycle will help to inform your own proposal. Consider reviewing a resource such as this one early on in your process.", "zotero_item": null}}, {"model": "library.reading", "pk": 760, "fields": {"title": "CUNY Digital Humanities Resource Guide List of Sample Projects", "url": "https://wiki.commons.gc.cuny.edu/Sample_Projects", "annotation": "[CUNY Digital Humanities Resource Guide List of Sample Projects](https://wiki.commons.gc.cuny.edu/Sample_Projects/)", "zotero_item": null}}, {"model": "library.reading", "pk": 761, "fields": {"title": "How to get a digital humanities project off the ground | HASTAC", "url": "https://www.hastac.org/blogs/paigecm/2014/06/06/how-get-digital-humanities-project-ground", "annotation": "Morgan, Paige. \u201cHow to Get a Digital Humanities Project off the Ground.\u201d HASTAC, 6 June 2014. https://www.hastac.org/blogs/paigecm/2014/06/06/how-get-digital-humanities-project-ground.", "zotero_item": null}}, {"model": "library.reading", "pk": 762, "fields": {"title": "\"Designing a DH Project.\"", "url": "https://repository.asu.edu/items/50759#embed", "annotation": "Grumbach, Elizabeth. [\"Designing a DH Project.\"](https://repository.asu.edu/items/50759#embed) Institute for Humanities Research Faculty Development Workshop Series, 26 September 2018. https://repository.asu.edu/items/50759", "zotero_item": null}}, {"model": "library.reading", "pk": 763, "fields": {"title": "Podcast, Johanna Drucker: \"Designing Digital Humanities\" - MIT Comparative Media Studies/Writing", "url": "https://cmsw.mit.edu/podcast-johanna-drucker-design", "annotation": "Whitacre, Andrew. Designing Digital Humanities. cmsw.mit.edu, https://cmsw.mit.edu/podcast-johanna-drucker-design/. Accessed 16 June 2020.", "zotero_item": null}}, {"model": "website.page", "pk": 4, "fields": {"name": "Workshops", "slug": "workshops", "text": "<p class=\"lead\">This is the workshop page.</p>", "template": "workshop/workshop-list.html"}}, {"model": "website.page", "pk": 5, "fields": {"name": "About", "slug": "about", "text": "<p class=\"lead\">This is the about page.</p>", "template": "website/page.html"}}, {"model": "website.page", "pk": 6, "fields": {"name": "Library", "slug": "library", "text": "<p class=\"lead\">This is the library page.</p>", "template": "library/all-library-items.html"}}]